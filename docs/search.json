[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"course introduce scientists practitioners interested applying statistical approaches daily routine using R working environment. Participants introduced R R Studio learning perform common statistical analyses. short introduction R principles, focus questions addressed using common statistical analyses, descriptive statistics statistical inference.","code":""},{"path":"index.html","id":"learning-outcomes","chapter":"Overview","heading":"0.1 Learning outcomes","text":"Understand read, interpret write scripts R.Understand read, interpret write scripts R.Learn statistical tools address common questions research activities.Learn statistical tools address common questions research activities.introduction efficient, readable reproducible analysesAn introduction efficient, readable reproducible analysesBeing comfortable using R performing descriptive inferential statistics.comfortable using R performing descriptive inferential statistics.","code":""},{"path":"index.html","id":"packages","chapter":"Overview","heading":"0.2 Packages","text":"","code":"\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(performance)\nlibrary(see)\nlibrary(lmerTest)\nlibrary(patchwork)\nlibrary(broom.mixed)\nlibrary(ggeffects)\nlibrary(DHARMa)\nlibrary(sjPlot)\nlibrary(MuMIn)\nlibrary(emmeans)\nlibrary(report)\nlibrary(MuMIn)"},{"path":"loading-data.html","id":"loading-data","chapter":"1 Loading data","heading":"1 Loading data","text":"workshop work loading data. curated cleaned dataset can work generating insights data.biologist used asking questions gathering data. also important learn aspects research process. includes responsible data management (understanding data files & spreadsheet organisation, keeping data safe) data analysis.chapter look structure data files, read R. also continue develop reproducible scripts. means writing scripts well organised easy read, also making sure scripts complete capable reproducing analysis start finish.Transparency reproducibility key values scientific research, analyse data reproducible way means others can understand check work. also means important person can benefit work, ! return analysis even short break, thanking earlier self worked clear reproducible way, can pick right left .","code":""},{"path":"loading-data.html","id":"meet-the-penguins","chapter":"1 Loading data","heading":"1.1 Meet the Penguins","text":"data, taken palmerpenguins (Horst et al. (2022)) package originally published Gorman et al. (2014). course work real data shared researchers.palmer penguins data contains size measurements, clutch observations, blood isotope ratios three penguin species observed three islands Palmer Archipelago, Antarctica study period three years.data collected 2007 - 2009 Dr. Kristen Gorman Palmer Station Long Term Ecological Research Program, part US Long Term Ecological Research Network. data imported directly Environmental Data Initiative (EDI) Data Portal, available use CC0 license (“Rights Reserved”) accordance Palmer Station Data Policy. gratefully acknowledge Palmer Station LTER US LTER Network. Special thanks Marty Downs (Director, LTER Network Office) help regarding data license & use. intrepid package co-author, Dr. Gorman, action collecting penguin data:map study site","code":""},{"path":"loading-data.html","id":"activity-1-organising-our-workspace","chapter":"1 Loading data","heading":"1.2 Activity 1: Organising our workspace","text":"can begin working data, need set-.Go RStudio Cloud open Penguins R projectGo RStudio Cloud open Penguins R projectCreate following folders using + New Folder button Files tab\ndata\noutputs\nscripts\nCreate following folders using + New Folder button Files tabdataoutputsscripts\nR case-sensitive type everything EXACTLY printed \nseparate subfolders within project helps keep things tidy, means harder lose things, lets easily tell R exactly go retrieve data.next step workflow well organised project space. RStudio Cloud lot hard work , new data project can set Project space.define project series linked questions uses one (sometimes several) datasets. example coursework assignment particular module project, series linked experiments particular research project might project.Project contain several files, possibly organised sub-folders containing data, R scripts final outputs. might want keep information (wider reading) gathered relevant project.\nFigure 1.1: example typical R project set-\nWithin project notice already one file .Rproj. R project file, useful feature, interacts R tell working specific place computer (case cloud server dialed ). means R automatically treat location project file 'working directory' makes importing exporting easier1.\nimportant NEVER move .Rproj file, may\nprevent workspace opening properly.\n","code":""},{"path":"loading-data.html","id":"activity-2-access-our-data","chapter":"1 Loading data","heading":"1.3 Activity 2: Access our data","text":"Now project workspace, ready import data.Use link open page browser data openUse link open page browser data openRight-click Save download csv format computer (Make note file downloaded e.g. Downloads)Right-click Save download csv format computer (Make note file downloaded e.g. Downloads)Compare data looks \"raw\" format open data ExcelCompare data looks \"raw\" format open data ExcelAt first glance data might look quite strange messy. stored CSV comma-separated values file. CSV files plain text files can store large amounts data, can readily imported spreadsheet storage database.files simplest form database, coloured cells, formulae, text formatting. row row data, value row (previously separate columns) separated comma.file format helps us maintain ethos Keep Raw Data Raw -many cases, captured collected data may unique impossible reproduce, measurements lab field observations. reason, protected possible loss. Every time change made raw data file threatens integrity information.practice, means use data file data entry storage. data manipulation, cleaning analysis happens R, using transparent reproducible scripts.\navoid saving files Excel format nasty\nhabit formatting even losing data file gets large\nenough.\n\n[https://www.theguardian.com/politics/2020/oct/05/-excel-may--caused-loss--16000-covid-tests--england].\n\nneed add data csv file, can always open \nExcel-like program add information, remember save \noriginal csv format afterwards.\n\nFigure 1.2: Top image: Penguins data viewed Excel, Bottom image: Penguins data native csv format\nraw format, line CSV separated commas different values. open spreadsheet program like Excel automatically converts comma-separated values tables columns.\nprobably used working Excel (.xls .xlsx)\nfile formats, widely supported, CSV files, simple\ntext formats supported data interfaces. also \nproprietary (e.g. Excel format owned Microsoft), working\n.csv format data open accessible.\n","code":""},{"path":"loading-data.html","id":"activity-3-upload-our-data","chapter":"1 Loading data","heading":"1.4 Activity 3: Upload our data","text":"data now Downloads folder computerThe data now Downloads folder computerWe need upload data remote cloud-server (RStudio Cloud), select upload files server button Files tabWe need upload data remote cloud-server (RStudio Cloud), select upload files server button Files tabPut file data folder - make mistake select tickbox file, go cogs button choose option Move.Put file data folder - make mistake select tickbox file, go cogs button choose option Move.\nFigure 1.3: Highlighted buttons upload files, options\n","code":""},{"path":"loading-data.html","id":"activity-4-make-a-script","chapter":"1 Loading data","heading":"1.5 Activity 4: Make a script","text":"now create new R script file write instructions store comments manipulating data, developing tables figures. Use File > New Script menu item select R Script.Add following:load following add-package R script, just underneath comments. Tidyverse actually one package, bundle many different packages play well together - example includes ggplot2 used last session, call separatelyAdd following script:Save file inside scripts folder call 01_import_penguins_data.R\nClick document outline button (top right script pane). \nshow use \n\n#TITLES—-\n\nAllows us build series headers subheaders, \nuseful using longer scripts.\n","code":"\n#___________________________----\n# SET UP ----\n## An analysis of the bill dimensions of male and female Adelie, Gentoo and Chinstrap penguins ----\n\n### Data first published in  Gorman, KB, TD Williams, and WR Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLos One 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081. ----\n#__________________________----\n# PACKAGES ----\nlibrary(tidyverse) # tidy data packages\nlibrary(janitor) # cleans variable names\nlibrary(lubridate) # make sure dates are processed properly\n#__________________________----"},{"path":"loading-data.html","id":"activity-5-read-in-data","chapter":"1 Loading data","heading":"1.6 Activity 5: Read in data","text":"Now can read data. use function readr::read_csv() allows us read .csv files. also functions allow read .xlsx files formats, however course use .csv files.First, create object called penguins_data contains data penguins_raw.csv file.First, create object called penguins_data contains data penguins_raw.csv file.Add following script, check document outline:Add following script, check document outline:\nalso function called read.csv(). \ncareful use function instead read_csv() \ndifferent ways naming columns.\n","code":"\n# IMPORT DATA ----\npenguins <- read_csv (\"data/penguins_raw.csv\")\n\nhead(penguins) # check the data has loaded, prints first 10 rows of dataframe\n#__________________________----"},{"path":"loading-data.html","id":"filepaths","chapter":"1 Loading data","heading":"1.7 Filepaths","text":"example read_csv() function requires provide filepath (\"quotes\"), order tell R file wish read located example two components\"data/\" - specifies directory look file\"data/\" - specifies directory look file\"penguins_raw.csv\" - specifies name format file\"penguins_raw.csv\" - specifies name format file","code":""},{"path":"loading-data.html","id":"directories","chapter":"1 Loading data","heading":"1.7.0.1 Directories","text":"directory refers folder computer relationships folders. term “directory” considers relationship folder folders within around . Directories hierarchical means can exist within folders well folders exist within .\nidea directories files ? alone File\nFound\n\"parent\" directory folder contains subdirectory. example downloads folder directory, parent directory subdirectories files contained within .","code":""},{"path":"loading-data.html","id":"home-directory","chapter":"1 Loading data","heading":"1.7.0.2 Home directory","text":"home directory computer directory defined operating system. home directory primary directory user account computer. files default stored home directory.Windows, home directory typically C:\\Users\\-username.Windows, home directory typically C:\\Users\\-username.Mac Linux, home directory typically /home/-username.Mac Linux, home directory typically /home/-username.","code":""},{"path":"loading-data.html","id":"working-directory","chapter":"1 Loading data","heading":"1.7.0.3 Working directory","text":"working directory refers directory computer tool assumes starting place filepaths","code":""},{"path":"loading-data.html","id":"absolute-vs-relative-filepaths","chapter":"1 Loading data","heading":"1.7.1 Absolute vs Relative filepaths","text":"got working R?use programming language, specify filepaths order program find files read-output files.Absolute file path path contains entire path file directory starting Home directory ending file directory wish access e.g./home/-username/project/data/penguins_raw.csvThe main drawbacks using absolute file paths :share files, another user won’t directory structure , need recreate file pathsIf share files, another user won’t directory structure , need recreate file pathsif alter directory structure, ’ll need rewrite pathsif alter directory structure, ’ll need rewrite pathsan absolute file path likely longer relative path, backslashes need edited, scope error.absolute file path likely longer relative path, backslashes need edited, scope error.different computers can different path constructions, scripts use absolute filepaths reproducible.Relative filepath path relative working directory location computer.use RStudio Projects, wherever .Rproj file located set working directory. means .Rproj file located project folder relative path data :data/penguins_raw.csvThis filepath shorter means share project someone else script run without editing.\nusing RStudio Cloud, remember working \nLinux OS cloud server, different absolute\nfilepath - scripts project working right now\nwork using relative filepaths\n","code":""},{"path":"loading-data.html","id":"activity-5-check-your-script","chapter":"1 Loading data","heading":"1.8 Activity 5: Check your script","text":"","code":"\n#___________________________----\n# SET UP ----\n## An analysis of the bill dimensions of male and female Adelie, Gentoo and Chinstrap penguins ----\n\n### Data first published in  Gorman, KB, TD Williams, and WR Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLos One 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081. ----\n#__________________________----\n\n# PACKAGES ----\nlibrary(tidyverse) # tidy data packages\nlibrary(janitor) # cleans variable names\nlibrary(lubridate) # make sure dates are processed properly\n#__________________________----\n\n# IMPORT DATA ----\npenguins <- read_csv (\"data/penguins_raw.csv\")\n\nhead(penguins) # check the data has loaded, prints first 10 rows of dataframe\n#__________________________----"},{"path":"loading-data.html","id":"activity-7-test-yourself","chapter":"1 Loading data","heading":"1.9 Activity 7: Test yourself","text":"Question 1. order make R project reproducible filepath use?Absolute filepathRelative filepathQuestion 2. acceptable include raw datafile?Highlighting blocks cellsExcel formulaeA column observational notes fielda mix ddmmyy yymmdd date formatsQuestion 3. always first set functions script? ?()Question 4. reading data R useread_csv()read.csv()Question 5. format penguins data ?wide datalong dataEach column unique variable row unique observation data long (tidy) formatQuestion 6. working directory projects default set location ?data filesthe .Rproj fileyour R scriptQuestion 7. Using filepath \"data/penguins_raw.csv\" example ofan absolute filepatha relative filepathQuestion 8. operator need use wish assign output read_csv function R object (rather just print dataframe console)?","code":""},{"path":"data-wrangling-part-one.html","id":"data-wrangling-part-one","chapter":"2 Data wrangling part one","heading":"2 Data wrangling part one","text":"may surprise learn scientists actually spend far time cleaning preparing data spend actually analysing . means completing tasks cleaning bad values, changing structure dataframes, reducing data subset observations, producing data summaries.Many people seem operate assumption option data cleaning painstaking time-consuming cutting pasting data within spreadsheet program like Excel. witnessed students colleagues waste days, weeks, even months manually transforming data Excel, cutting, copying, pasting data. Fixing data hand terrible use time, error-prone reproducible. Additionally, age can easily collect massive datasets online, able organise, clean, prepare hand.short, thrive scientist learn key data wrangling skills. Although every dataset presents unique challenges, systematic principles follow make analyses easier, less error-prone, efficient, reproducible.chapter see data science skills allow efficiently get answers nearly question might want ask data. learning properly make computer hard boring work , can focus bigger issues.","code":""},{"path":"data-wrangling-part-one.html","id":"activity-1-change-column-names","chapter":"2 Data wrangling part one","heading":"2.1 Activity 1: Change column names","text":"going learn organise data using tidy format2. using tidyverse packages Wickham (2023). opinionated, highly effective method generating reproducible analyses wide-range data manipulation tools. Tidy data easy format computers read. also required data structure statistical tests work later.'tidy' refers specific structure lets us manipulate visualise data ease. tidy dataset variable one column row contains one observation. cell table/spreadsheet contains values. One observation might make tidy data quite long - generates lot rows data - might remember tidy data can referred long-format data (opposed wide data).know data R, know columns names imported. still know whether values imported correctly, whether captured rows.","code":""},{"path":"data-wrangling-part-one.html","id":"add-this-to-your-script","chapter":"2 Data wrangling part one","heading":"2.1.0.1 Add this to your script","text":"run colnames() get identities column dataframeStudy name: identifier year sets observations madeStudy name: identifier year sets observations madeRegion: area observation recordedRegion: area observation recordedIsland: specific island observation recordedIsland: specific island observation recordedStage: Denotes reproductive stage penguinStage: Denotes reproductive stage penguinIndividual ID: unique ID individualIndividual ID: unique ID individualClutch completion: study nest observed full clutch e.g. 2 eggsClutch completion: study nest observed full clutch e.g. 2 eggsDate egg: date study nest observed 1 eggDate egg: date study nest observed 1 eggCulmen length: length dorsal ridge bird's bill (mm)Culmen length: length dorsal ridge bird's bill (mm)Culmen depth: depth dorsal ridge bird's bill (mm)Culmen depth: depth dorsal ridge bird's bill (mm)Flipper Length: length bird's flipper (mm)Flipper Length: length bird's flipper (mm)Body Mass: Bird's mass (g)Body Mass: Bird's mass (g)Sex: Denotes sex birdSex: Denotes sex birdDelta 15N : ratio stable Nitrogen isotopes 15N:14N blood sampleDelta 15N : ratio stable Nitrogen isotopes 15N:14N blood sampleDelta 13C: ratio stable Carbon isotopes 13C:12C blood sampleDelta 13C: ratio stable Carbon isotopes 13C:12C blood sample","code":"\n# CHECK DATA----\n# check the data\ncolnames(penguins)\n#__________________________----"},{"path":"data-wrangling-part-one.html","id":"clean-column-names","chapter":"2 Data wrangling part one","heading":"2.1.0.2 Clean column names","text":"Often might want change names variables. might non-intuitive, long. data couple issues:names contain spacesSome names contain spacesSome names capitalised lettersSome names capitalised lettersSome names contain bracketsSome names contain bracketsThis dataframe like correct quickly. R case-sensitive also like spaces brackets variable names","code":"\n# CLEAN DATA ----\n\n# clean all variable names to snake_case using the clean_names function from the janitor package\n# note we are using assign <- to overwrite the old version of penguins with a version that has updated names\n# this changes the data in our R workspace but NOT the original csv file\n\npenguins <- janitor::clean_names(penguins) # clean the column names\n\ncolnames(penguins) # quickly check the new variable names##  [1] \"study_name\"        \"sample_number\"     \"species\"          \n##  [4] \"region\"            \"island\"            \"stage\"            \n##  [7] \"individual_id\"     \"clutch_completion\" \"date_egg\"         \n## [10] \"culmen_length_mm\"  \"culmen_depth_mm\"   \"flipper_length_mm\"\n## [13] \"body_mass_g\"       \"sex\"               \"delta_15_n_o_oo\"  \n## [16] \"delta_13_c_o_oo\"   \"comments\""},{"path":"data-wrangling-part-one.html","id":"rename-columns-manually","chapter":"2 Data wrangling part one","heading":"2.1.0.3 Rename columns (manually)","text":"clean_names function quickly converts variable names snake case. N C blood isotope ratio names still quite long though, clean dplyr::rename() \"new_name\" = \"old_name\".","code":"\n# shorten the variable names for N and C isotope blood samples\n\npenguins <- rename(penguins,\n         \"delta_15n\"=\"delta_15_n_o_oo\",  # use rename from the dplyr package\n         \"delta_13c\"=\"delta_13_c_o_oo\")"},{"path":"data-wrangling-part-one.html","id":"check-data","chapter":"2 Data wrangling part one","heading":"2.2 Check data","text":"","code":""},{"path":"data-wrangling-part-one.html","id":"glimpse-check-data-format","chapter":"2 Data wrangling part one","heading":"2.2.0.1 glimpse: check data format","text":"run glimpse() get several lines output. number observations \"rows\", number variables \"columns\". Check csv file - . next lines see variable names type data.can see dataset 345 rows (including headers) 17 variables\nalso provides information type data column<chr> - means character text data<chr> - means character text data<dbl> - means numerical data<dbl> - means numerical data","code":"\nglimpse(penguins)"},{"path":"data-wrangling-part-one.html","id":"rename-text-values","chapter":"2 Data wrangling part one","heading":"2.2.0.2 Rename text values","text":"Sometimes may want rename values variables order make shorthand easier follow. changing values columns, column names.\nchecked code block worked? Inspect new\ntibble check variables renamed wanted.\n","code":"\n# use mutate and case_when for a statement that conditionally changes the names of the values in a variable\npenguins <- penguins %>% \n  mutate(species = case_when(species == \"Adelie Penguin (Pygoscelis adeliae)\" ~ \"Adelie\",\n                             species == \"Gentoo penguin (Pygoscelis papua)\" ~ \"Gentoo\",\n                             species == \"Chinstrap penguin (Pygoscelis antarctica)\" ~ \"Chinstrap\"))"},{"path":"data-wrangling-part-one.html","id":"dplyr-verbs","chapter":"2 Data wrangling part one","heading":"2.3 dplyr verbs","text":"section introduced commonly used data wrangling functions, come dplyr package (part tidyverse). functions likely become familiar .","code":""},{"path":"data-wrangling-part-one.html","id":"select","chapter":"2 Data wrangling part one","heading":"2.3.1 Select","text":"wanted create dataset includes certain variables, can use select() function dplyr package.example might wish create simplified dataset contains species, sex, flipper_length_mm body_mass_g.Run code select columnsAlternatively tell R columns want e.g.Note select() change original penguins tibble. spits new tibble directly console.save new tibble, stored. want keep , must create new object.run new code, see anything console, see new object appear Environment pane.","code":"\n# DPLYR VERBS ----\n\nselect(.data = penguins, # the data object\n       species, sex, flipper_length_mm, body_mass_g) # the variables you want to select\nselect(.data = penguins,\n       -study_name, -sample_number)\nnew_penguins <- select(.data = penguins, \n       species, sex, flipper_length_mm, body_mass_g)"},{"path":"data-wrangling-part-one.html","id":"filter","chapter":"2 Data wrangling part one","heading":"2.3.2 Filter","text":"previously used select() select certain variables, now use filter() select certain rows observations. example Adelie penguins.can equivalence operator ==Filter quite complicate function, uses several differe operators assess way apply filter.\nTable 2.1: Boolean expressions\nwanted select Penguin species except Adelies, use 'equals'.asYou can include multiple expressions within filter() pull rows evaluate TRUE conditions.example code pull observations Adelie penguins flipper length measured greater 190mm.","code":"\nfilter(.data = new_penguins, species == \"Adelie Penguin (Pygoscelis adeliae)\")\nfilter(.data = new_penguins, species != \"Adelie\")\nfilter(.data = new_penguins, species %in% c(\"Chinstrap\", \"Gentoo\"))\nfilter(.data = new_penguins, species == \"Adelie\", flipper_length_mm > 190)"},{"path":"data-wrangling-part-one.html","id":"arrange","chapter":"2 Data wrangling part one","heading":"2.3.3 Arrange","text":"function arrange() sorts rows table according columns supplied. exampleThe data now arranged alphabetical order sex. observations female penguins listed males.can also reverse desc()can also sort one column, think code ?","code":"\narrange(.data = new_penguins, sex)\narrange(.data = new_penguins, desc(sex))\narrange(.data = new_penguins,\n        sex,\n        desc(species),\n        desc(flipper_length_mm))"},{"path":"data-wrangling-part-one.html","id":"mutate","chapter":"2 Data wrangling part one","heading":"2.3.4 Mutate","text":"Sometimes need create new variable exist dataset. example might want figure flipper length factoring body mass.create new variables use function mutate().Note , want save new column must save object. mutating new column attaching new_penguins data oject.","code":"\nnew_penguins <- mutate(.data = new_penguins,\n                       body_mass_kg = body_mass_g/1000)"},{"path":"data-wrangling-part-one.html","id":"pipes","chapter":"2 Data wrangling part one","heading":"2.4 Pipes","text":"Pipes look like : %>% Pipes allow send output one function straight another function. Specifically, send result function %>% first argument function %>%. usual, easier show, rather tell look example.reason function called pipe 'pipes' data next function. wrote code previously, first argument function dataset wanted work . use pipes automatically take data previous line code need specify .Take penguins data \nSelect species, sex flipper length columns \nFilter keep observations labelled sex equals male \nArrange data HIGHEST LOWEST flipper lengths.\nR version 4 onwards now “native pipe”\n|>\n\ndoesn’t require tidyverse magrittr package \npackages load use.\n\ncoursebook chosen continue use \ntidyverse pipe %>% time \nlikely much familiar tutorials, website usages.\nnative pipe also behaves “slightly” differently, \ncause confusion.\n\nwant read operational differences, \nsite good job explaining\n","code":"\n# this example uses brackets to nest and order functions\narrange(.data = filter(.data = select(.data = penguins, species, sex, flipper_length_mm), sex == \"MALE\"), desc(flipper_length_mm))\n# this example uses sequential R objects to make the code more readable\nobject_1 <- select(.data = penguins, species, sex, flipper_length_mm)\nobject_2 <- filter(.data = object_1, sex == \"MALE\")\narrange(object_2, desc(flipper_length_mm))\n# this example is human readable without intermediate objects\npenguins %>% \n  select(species, sex, flipper_length_mm) %>% \n  filter(sex == \"MALE\") %>% \n  arrange(desc(flipper_length_mm))"},{"path":"data-wrangling-part-one.html","id":"a-few-more-handy-functions","chapter":"2 Data wrangling part one","heading":"2.5 A few more handy functions","text":"","code":""},{"path":"data-wrangling-part-one.html","id":"check-for-duplication","chapter":"2 Data wrangling part one","heading":"2.5.1 Check for duplication","text":"easy inputting data make mistakes, copy something twice example, someone lot copy-pasting assemble spreadsheet (yikes!). can check pretty quicklyGreat!","code":"\n# check for duplicate rows in the data\npenguins %>% \n  duplicated() %>% # produces a list of TRUE/FALSE statements for duplicated or not\n  sum() # sums all the TRUE statements[1] 0"},{"path":"data-wrangling-part-one.html","id":"summarise","chapter":"2 Data wrangling part one","heading":"2.5.2 Summarise","text":"can also explore data obvious typos checking implausibly small large values, simple use summarise function.minimum weight penguins 2.7kg, max 6.3kg - outrageous. min come 27g might suspicious. use summarise calculate metrics future.\nfirst data insight, difference smallest adult penguin \ndataset nearly half size largest penguin.\n","code":"\n# use summarise to make calculations\npenguins %>% \n  summarise(min=min(body_mass_g, na.rm=TRUE), \n            max=max(body_mass_g, na.rm=TRUE))"},{"path":"data-wrangling-part-one.html","id":"group-by","chapter":"2 Data wrangling part one","heading":"2.5.3 Group By","text":"Many data analysis tasks can approached using “split-apply-combine” paradigm: split data groups, apply analysis group, combine results. dplyr makes easy group_by() function. summarise example able find max-min body mass values penguins dataset. wanted break grouping species penguin. group_by() comes .Now know little data, max weight Gentoo penguins much larger two species. fact, minimum weight Gentoo penguin far max weight two species.","code":"\npenguins %>% \n  group_by(species) %>%  # subsequent functions are perform \"by group\"\n  summarise(min=min(body_mass_g, na.rm=TRUE), \n            max=max(body_mass_g, na.rm=TRUE))"},{"path":"data-wrangling-part-one.html","id":"distinct","chapter":"2 Data wrangling part one","heading":"2.5.4 Distinct","text":"can also look typos asking R produce distinct values variable. useful categorical data, expect distinct categoriesHere someone mistyped e.g. 'FMALE' obvious. thing (probably changed names) species.","code":"\npenguins %>% \n  distinct(sex)"},{"path":"data-wrangling-part-one.html","id":"missing-values-na","chapter":"2 Data wrangling part one","heading":"2.5.5 Missing values: NA","text":"multiple ways check missing values dataBut tell us , fortunately function summary easily","code":"\n# Get a sum of how many observations are missing in our dataframe\npenguins %>% \n  is.na() %>% \n  sum()"},{"path":"data-wrangling-part-one.html","id":"summary","chapter":"2 Data wrangling part one","heading":"2.6 Summary","text":"provides quick breakdown max min numeric variables, well list many missing observations one. can see appear two missing observations measurements body mass, bill lengths, flipper lengths several blood measures. know sure without inspecting data , likely two birds missing multiple measurements, several measured blood drawn.leave NA's alone now, useful know many .now got clean & tidy dataset, handful first insights data.","code":"\n# produce a summary of our data\nsummary(penguins)\n#__________________________----"},{"path":"data-wrangling-part-one.html","id":"finished","chapter":"2 Data wrangling part one","heading":"2.7 Finished","text":"lot work! remember remember functions, remember chapter data wrangling future. Also bookmark RStudio Cheatsheets Page.Finally, make sure saved changes made script 💾 & make sure workspace set save objects environment sessions.want script record work progress, confused cluttered R Environment.","code":""},{"path":"data-wrangling-part-one.html","id":"activity-reorganise-this-script","chapter":"2 Data wrangling part one","heading":"2.8 Activity: Reorganise this script","text":"Using link take text copy/paste new R script save YYYY_MM_DD_workshop_4_jumbled_script.RAll correct lines code, comments document markers present, correct order. Can unscramble produce sensible output clear document outline?want check answers (just completely stuck) click ","code":""},{"path":"data-wrangling-part-two.html","id":"data-wrangling-part-two","chapter":"3 Data wrangling part two","heading":"3 Data wrangling part two","text":"","code":""},{"path":"data-wrangling-part-two.html","id":"load-your-workspace","chapter":"3 Data wrangling part two","heading":"3.1 Load your workspace","text":"workspace ready work Palmer penguins data. Load workspace now.Think basic checks start work today.","code":""},{"path":"data-wrangling-part-two.html","id":"checklist","chapter":"3 Data wrangling part two","heading":"3.1.1 Checklist","text":"objects already Environment pane? , use rm(list=ls())objects already Environment pane? , use rm(list=ls())Re-run script last time line 1 last lineRe-run script last time line 1 last lineCheck warning error messagesCheck warning error messagesAdd code today's session script goAdd code today's session script go","code":""},{"path":"data-wrangling-part-two.html","id":"more-summary-tools","chapter":"3 Data wrangling part two","heading":"3.2 More summary tools","text":"often want make calculations aobut groups observations, mean median. often interested comparing responses among groups. example, previously found number distinct penguins entire dataset.\nAdd new lines code script try . Comment\n# add short descriptions achieving \nNow consider groups subsets observations, find number penguins species sex.progress, learning use data wrangling tools. also gaining insights data.Question many female Adelie penguins dataset?Question many Gentoo penguins sex recorded?using summarise group_by lot! powerful functions:group_by adds grouping information data object, subsequent calculations happen group-specific basis.group_by adds grouping information data object, subsequent calculations happen group-specific basis.summarise data aggregation function thart calculates summaries one variables, separately groups defined group_bysummarise data aggregation function thart calculates summaries one variables, separately groups defined group_by","code":"\npenguins %>% \n  summarise(n_distinct(individual_id))\npenguins %>% \n  group_by(species, sex) %>% \n  summarise(n_distinct(individual_id))"},{"path":"data-wrangling-part-two.html","id":"summarise-1","chapter":"3 Data wrangling part two","heading":"3.2.1 summarise()","text":"summarise() whole list useful functions producing descriptive statisticsmin max calculate minimum maximum values numeric vectormin max calculate minimum maximum values numeric vectormean median calculate averages numeric vectormean median calculate averages numeric vectorsd var calculate standard deviation variance numeric vectorsd var calculate standard deviation variance numeric vectorUsing summarise can calculate mean flipper bill lengths penguins:\nNote - provide informative names left side \n=\n\nperforming calculations summarise important set\nna.rm = TRUE, removes missing values \ncalculation\n\nhappens try produce calculations include\nNA? e.g NA + 4 NA * 5\ncan use several functions summarise. means can string several calculations together single step, generate insights data.190 unique IDs 344 total observations appear roughly twice many observations unique individuals. sex ratio roughly even (48% female) average flipper length 201 mm.","code":"\npenguins %>% \n  summarise(\n    mean_flipper_length = mean(flipper_length_mm, na.rm=TRUE),\n     mean_culmen_length = mean(culmen_length_mm, na.rm=TRUE))\npenguins %>% \n  summarise(n=n(), # number of rows of data\n            num_penguins = n_distinct(individual_id), # number of unique individuals\n            mean_flipper_length = mean(flipper_length_mm, na.rm=TRUE), # mean flipper length\n            prop_female = sum(sex == \"FEMALE\", na.rm=TRUE) / n()) # proportion of observations that are coded as female"},{"path":"data-wrangling-part-two.html","id":"summarize-across-columns","chapter":"3 Data wrangling part two","heading":"3.2.1.1 Summarize across columns","text":"across two arguments, .cols .fns..cols argument lets select columns wish apply functions toThe .cols argument lets select columns wish apply functions toThe .fns argument applies required function selected columns..fns argument applies required function selected columns.example calculates means & numeric variables dataset.example slightly complicated way running n_distinct summarise. .cols() looks column contains word \"penguin\" runs n_distinct()command ","code":"\n# Across ----\n# The mean of ALL numeric columns in the data, where(is.numeric == TRUE) hunts for numeric columns\n\npenguins %>% \n  summarise(across(.cols = where(is.numeric), \n                   .fns = ~ mean(., na.rm=TRUE)))\n# number of distinct penguins, as only one column contains the word penguin\n# the argument contains looks for columns that match a character expression\n\npenguins %>% \n  summarise(across(.cols = contains(\"individual\"), \n                   .fns = ~n_distinct(.)))"},{"path":"data-wrangling-part-two.html","id":"group_by-revisited","chapter":"3 Data wrangling part two","heading":"3.2.2 group_by revisited","text":"group_by function provides ability separate summary functions according subgroups wish make. real magic happens pair summarise mutate.example, grouping individual penguin ids, summarising n - can see many times penguin monitored course study.\nRemember actions group_by “invisible”.\nSubsequent functions applied “grouped ” manner - \ndataframe looks unchanged.\n","code":"\npenguin_stats <- penguins %>% \n  group_by(individual_id) %>% \n  summarise(num=n())"},{"path":"data-wrangling-part-two.html","id":"more-than-one-grouping-variable","chapter":"3 Data wrangling part two","heading":"3.2.2.1 More than one grouping variable","text":"need calculate one variable time?\nproblem can submit several arguments:can calculate mean flipper length penguins six combinationsNow first row summary table shows us mean flipper length (mm) female Adelie penguins. eight rows total, six unique combinations two rows sex penguins recorded(NA)","code":"\npenguins_grouped <- penguins %>% \n  group_by(sex, species)\npenguins_grouped %>% \nsummarise(mean_flipper = mean(flipper_length_mm, na.rm=TRUE))"},{"path":"data-wrangling-part-two.html","id":"using-group_by-with-mutate","chapter":"3 Data wrangling part two","heading":"3.2.2.2 using group_by with mutate","text":"far used group_by summarise function, always case.\nmutate used group_by, calculations occur 'group'. example:calculating group centered mean, new variable contains difference observation mean whichever group observation .","code":"\n# Using mutate and group_by ----\ncentered_penguins <- penguins %>% \n  group_by(sex, species) %>% \n  mutate(flipper_centered = flipper_length_mm-mean(flipper_length_mm, na.rm=TRUE))\n\ncentered_penguins %>% \n  select(flipper_centered)\n# Each row now returns a value for EACH penguin of how much greater/lesser than the group average (sex and species) its flipper is. "},{"path":"data-wrangling-part-two.html","id":"remove-group_by","chapter":"3 Data wrangling part two","heading":"3.2.2.3 remove group_by","text":"occasion may need remove grouping information dataset. often required string pipes together, need work using grouping structure, revert back whole dataset againLook grouped dataframe, can see information groups top data:Look output - can see information groups now removed data.","code":"# A tibble: 344 x 10\n# Groups:   sex, species [8]\n   species island culmen_length_mm culmen_depth_mm flipper_length_~ body_mass_g\n   <chr>   <chr>           <dbl>         <dbl>            <dbl>       <dbl>\n 1 Adelie  Torge~           39.1          18.7              181        3750\n 2 Adelie  Torge~           39.5          17.4              186        3800\n 3 Adelie  Torge~           40.3          18                195        3250\n# Run this command will remove the groups - but this is only saved if assigned BACK to an object\n\ncentered_penguins <- centered_penguins %>% \n  ungroup()\n\ncentered_penguins"},{"path":"data-wrangling-part-two.html","id":"working-with-dates","chapter":"3 Data wrangling part two","heading":"3.3 Working with dates","text":"Working dates can tricky, treating date strictly numeric problematic, account number days months number months year.Additionally lot different ways write date:13-10-201913-10-201910-13-201910-13-201913-10-1913-10-1913th Oct 201913th Oct 20192019-10-132019-10-13This variability makes difficult tell software read information, luckily can use functions lubridate package.\nget warning dates parsed, \nmight find date inconsistently entered \ndataset.\n\nPay attention warning error messages\nDepending interpret date ordering file, can use ymd(), ydm(), mdy(), dmy()Question appropriate function use date_egg variable?use mutate function dplyr create new variable called date_egg_proper based output converting characters date_egg date format. original variable left intact, specified \"new\" variable also called date_egg overwritten original variable.established date data, able perform calculations. date range across data collected.","code":"\npenguins <- penguins %>%\n  mutate(date_egg_proper = lubridate::dmy(date_egg))\npenguins %>% \n  summarise(min_date=min(date_egg_proper),\n            max_date=max(date_egg_proper))"},{"path":"data-wrangling-part-two.html","id":"calculations-with-dates","chapter":"3 Data wrangling part two","heading":"3.3.0.1 Calculations with dates","text":"many times penguin measured, across total time period?Cool can also convert intervals days weeks, months years dweeks(1), dmonths(1), dyears(1).cool functions, check RStudio cheat sheet information. Date type data common datasets, learning work useful skill.","code":"\npenguins %>% \n  group_by(individual_id) %>% \n  summarise(first_observation=min(date_egg_proper), \n            last_observation=max(date_egg_proper), \n            study_duration = last_observation-first_observation, \n            n=n())\npenguins %>% \n  group_by(individual_id) %>% \n  summarise(first_observation=min(date_egg_proper), \n            last_observation=max(date_egg_proper), \n            study_duration_years = (last_observation-first_observation)/lubridate::dyears(1), \n            n=n()) %>% \n    arrange(desc(study_duration_years))"},{"path":"data-wrangling-part-two.html","id":"factors","chapter":"3 Data wrangling part two","heading":"3.4 Factors","text":"R, factors class data allow ordered categories fixed set acceptable values.Typically, convert column character numeric class factor want set intrinsic order values (“levels”) can displayed non-alphabetically plots tables, use linear model analyses (later).Another common use factors standardise legends plots fluctuate certain values temporarily absent data.make barplot, order values x axis typically alphabetical order character dataTo convert character numeric column class factor, can use function forcats package. convert class factor also perform allow certain ordering levels - example using forcats::fct_relevel() lets manually specify level order.\nfunction as_factor() simply converts class without capabilities.base R function factor() converts column factor allows manually specify order levels, character vector levels = argument.use mutate() fct_relevel() convert column flipper_range class character class factor.Now call plot, can see x axis categories match intrinsic order specified factor levels.\nFactors also important build linear models bit\nlater. reference intercept categorical predictor variable\nread <chr> set R first\none ordered alphabetically. may always \nappropriate choice, changing ordered\n<fct> can manually set intercept.\n","code":"\npenguins <- penguins %>% \n  mutate(flipper_range = case_when(flipper_length_mm <= 190 ~ \"small\",\n                                   flipper_length_mm >190 & flipper_length_mm < 213 ~ \"medium\",\n                                   flipper_length_mm >= 213 ~ \"large\"))\npenguins %>% \n  ggplot(aes(x = flipper_range))+\n  geom_bar()\npenguins <- penguins %>% \n  mutate(flipper_range = fct_relevel(flipper_range))\nlevels(penguins$flipper_range)## [1] \"large\"  \"medium\" \"small\"\n# Correct the code in your script with this version\npenguins <- penguins %>% \n  mutate(flipper_range = fct_relevel(flipper_range, \"small\", \"medium\", \"large\"))\npenguins %>% \n  ggplot(aes(x = flipper_range))+\n  geom_bar()"},{"path":"data-wrangling-part-two.html","id":"finished-1","chapter":"3 Data wrangling part two","heading":"3.5 Finished","text":"Make sure saved script 💾 given filename \"01_import_penguins_data.R\" \"scripts\" folder.Make sure saved script 💾 given filename \"01_import_penguins_data.R\" \"scripts\" folder.workspace look like ?workspace look like ?\nFigure 3.1: neat project layout\n\nFigure 3.2: scripts file subdirectory\n","code":""},{"path":"data-wrangling-part-two.html","id":"activity-test-yourself","chapter":"3 Data wrangling part two","heading":"3.6 Activity: Test yourself","text":"Question 1. order subset data rows use function select()filter()group_by()Question 2. order subset data columns use function select()filter()group_by()Question 3. order make new column use function group_by()select()mutate()arrange()Question 4. operator use send output line code next line? Question 5. outcome following line code?penguins dataframe object reduced include Adelie penguins now onA new filtered dataframe Adelie penguins printed consoleUnless output series functions \"assigned\" object using <- saved, results immediately printed. code modified order create new filtered object penguins_filteredQuestion 5. main point data \"pipe\"?code runs fasterThe code easier readQuestion 6. naming convention outputted function `janitor::clean_names() \nsnake_casecamelCaseSCREAMING_SNAKE_CASEkebab-caseQuestion 7. package provides useful functions manipulating character strings?stringrggplot2lubridateforcatsQuestion 8. package provides useful functions manipulating dates?stringrggplot2lubridateforcatsQuestion 9. specify character variable factor, ordering default ?numericalalphabeticalorder dataframe","code":"\npenguins %>% \n  filter(species == \"Adelie\")\npenguins_filtered <- penguins %>% \n  filter(species == \"Adelie\")"},{"path":"data-visualisation-with-ggplot2.html","id":"data-visualisation-with-ggplot2","chapter":"4 Data visualisation with ggplot2","heading":"4 Data visualisation with ggplot2","text":"","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"intro-to-grammar","chapter":"4 Data visualisation with ggplot2","heading":"4.1 Intro to grammar","text":"ggplot2 package widely used valued simple, consistent approach making data visuals.'grammar graphics' relates different components plot function like different parts linguistic grammar. example, plots require axes, x y axes form one part ‘language’ plot. Similarly, plots data represented axes, often points, lines bars. visual way data represented forms another component grammar graphics. Furthermore, colour, shape size points lines can used encode additional information plot. information usually clarified key, legend, can also considered part ‘grammar’.philosophy ggplot much better explained package author, Hadley Wickham (Wickham et al. (2023)). now, just need aware ggplots constructed specifying different components want display, based underlying information data frame.\nFigure 4.1: example can produce ggplot\n","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"before-we-start","chapter":"4 Data visualisation with ggplot2","heading":"4.1.0.1 Before we start","text":"workspace ready work Palmer penguins data. Load workspace now.Think basic checks start work today.","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"checklist-1","chapter":"4 Data visualisation with ggplot2","heading":"4.1.0.2 Checklist","text":"\nToday going make NEW R script project space\npreviously working. part organising \nworkspace analysis workflow well documented easy \nfollow\nOpen new R script - moving data wrangling data visualisationOpen new R script - moving data wrangling data visualisationSave file scripts folder call 02_visualisation_penguins.RSave file scripts folder call 02_visualisation_penguins.RAdd following script run :Add following script run :find Environment fills objects script 1\nsource() function handy way allowing\ndifferent scripts different parts R project, \nallow access objects built elsewhere. way building \nanalysis stages.\n\ncommand work remembered save name\nscript exactly put script inside subfolder\ncalled scripts.\n\nproject look like one ?\n\nFigure 4.2: neat project layout\n\nFigure 4.3: sucessfully saved 02_visualisation_penguins.R visible \n","code":"\n# LOAD R OBJECTS AND FUNCTIONS ----\nsource(\"scripts/01_import_penguins_data.R\")\n# import tidied penguins data and functions\n#__________________________----"},{"path":"data-visualisation-with-ggplot2.html","id":"what-if-source-isnt-working","chapter":"4 Data visualisation with ggplot2","heading":"4.1.0.3 What if source isn't working?","text":"source working, figure project set-can complete worksheet put following commands top script instead source(\"scripts/01_import_penguins_data.R\")","code":"\n#___________________________----\n# SET UP ----\n## An analysis of the bill dimensions of male and female Adelie, Gentoo and Chinstrap penguins ----\n\n### Data first published in  Gorman, KB, TD Williams, and WR Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLos One 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081. ----\n#__________________________----\n\n# PACKAGES ----\nlibrary(tidyverse) # tidy data packages\nlibrary(janitor) # cleans variable names\nlibrary(lubridate) # make sure dates are processed properly\n#__________________________----\n\n# IMPORT DATA ----\npenguins <- read_csv (\"data/penguins_raw.csv\")\n\npenguins <- janitor::clean_names(penguins) # clean variable names\n#__________________________----"},{"path":"data-visualisation-with-ggplot2.html","id":"building-a-plot","chapter":"4 Data visualisation with ggplot2","heading":"4.2 Building a plot","text":"start building plot going use penguin data working previously. First must specify data frame contains relevant data plot. can two ways:‘sending penguins data set ggplot function’:specifying dataframe within ggplot() functionThe output identical\nRunning command produce empty grey panel. \nneed specify different columns data frame\nrepresented plot.\n","code":"\n# Building a ggplot step by step ----\n## Render a plot background ----\npenguins %>% \n  ggplot()\nggplot(data = penguins)"},{"path":"data-visualisation-with-ggplot2.html","id":"aesthetics---aes","chapter":"4 Data visualisation with ggplot2","heading":"4.2.1 Aesthetics - aes()","text":"can call different columns data dataset based column names. Column names given ‘aesthetic’ elements ggplot function, wrapped aes() function.want scatter plot, point x y coordinate. want x axis represent flipper length ( x = flipper_length_mm ), y axis represent body mass ( y = body_mass_g ).give specifications separated comma. Quotes required giving variables within aes().\ninterested quotes aren’t required can read non-standard\nevaluation.\nfar grid lines x y axis. ggplot() knows variables required plot, thus scale, information display data points.","code":"\n## Set axes ----\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g))"},{"path":"data-visualisation-with-ggplot2.html","id":"geometric-representations---geom","chapter":"4 Data visualisation with ggplot2","heading":"4.3 Geometric representations - geom()","text":"Given want scatter plot, need specify geometric representation data point form, using geom_point(). many geometric object types.\nFigure 4.4: geom shapes\nadding layer (hence + sign) points plot. can think similar e.g. Adobe Photoshop uses layers images can reordered modified individually. add plots layer layer order geoms may important final aesthetic design.ggplot, layer added plot according position code. first show full breakdown components layer. layer requires information ondataaestheticsgeometric typeany summary datapositionThis quite complicate way write new layers - usual see simpler compact approachNow scatter plot! row (except two rows missing data) penguins data set now x coordinate, y coordinate, designated geometric representation (point).can see smaller penguins tend smaller flipper lengths.","code":"\n## Add a geom ----\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g))+\n  layer(                # layer inherits data and aesthetic arguments from previous\n    geom=\"point\",       # draw point objects\n    stat=\"identity\",    # each individual data point gets a geom (no summaries)\n    position=position_identity()) # data points are not moved in any way e.g. we could specify jitter or dodge if we want to avoid busy overlapping data\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g))+\n  geom_point() # geom_point function will always draw points, and unless specified otherwise the arguments for position and stat are both \"identity\"."},{"path":"data-visualisation-with-ggplot2.html","id":"and","chapter":"4 Data visualisation with ggplot2","heading":"4.3.1 %>% and +","text":"ggplot2, early component tidyverse package, written pipe introduced. + sign ggplot2 functions similar way pipe functions tidyverse: allowing code written left right.","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"colour","chapter":"4 Data visualisation with ggplot2","heading":"4.3.2 Colour","text":"colors lines points can set directly using colour=\"red\", replacing “red” color name. colors filled objects, like bars, can set using fill=\"red\".However current plot informative colour used convey information species penguin.order achieve need use aes() , make colour conditional upon variable., aes() function containing relevant column name, given within geom_point() function.\ncommon mistake get confused use (use)\naes()\n\nspecifying fixed aesthetic e.g. red everything \ngo inside aes() instead specify e.g. colour = “red” \nshape =21.\n\nwish modify aethetic according variable \ndata go inside aes()\ne.g. aes(colour = species)\n\nmay (may ) noticed grammar ggplot (\ntidyverse general) accepts British/Americanization \nspelling!!!\ndata visualisations can start gain insights data quickly, can see Gentoo penguins tend larger longer flippers\nAdd carriage returns (new lines) %>% + symbols.\n\ncases, R blind white space new lines, \nsimply make code readable, allow us add readable\ncomments.\n","code":"\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g))+\n  geom_point(colour=\"red\")\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g))+\n  geom_point(aes(colour=species))"},{"path":"data-visualisation-with-ggplot2.html","id":"more-layers","chapter":"4 Data visualisation with ggplot2","heading":"4.3.3 More layers","text":"can see relationship body size flipper length. want model relationship trend line? can add another ‘layer’ plot, using different geometric representation data. case trend line, fact summary data rather representation point.geom_smooth() function draws trend line data. default behaviour draw local regression line (curve) points, however can hard interpret. want add straight line based linear model (‘lm’) relationship x y.first encounter linear models course, learn lot later .example may notice assigning colour variable (species) geometric layers. means option simplify code. Aesthetics set \"top layer\" ggplot() inherited subsequent layers.\nNote - trend line blocking certain points, \n‘top layer’ plot. geom layers appear early \ncommand drawn first, can obscured geom layers \ncome .\n\nhappens switch order geom_point() \ngeom_smooth() functions ? notice trend\nline?\n","code":"\n## Add a second geom ----\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g))+\n  geom_point(aes(colour=species))+\n  geom_smooth(method=\"lm\",    #add another layer of data representation.\n              se=FALSE,\n              aes(colour=species)) # note layers inherit information from the top ggplot() function but not previous layers - if we want separate lines per species we need to either specify this again *or* move the color aesthetic to the top layer. \npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g,\n             colour=species))+ ### now colour is set here it will be inherited by ALL layers\n  geom_point()+\n  geom_smooth(method=\"lm\",    #add another layer of data representation.\n              se=FALSE)"},{"path":"data-visualisation-with-ggplot2.html","id":"co-ordinate-space","chapter":"4 Data visualisation with ggplot2","heading":"4.3.4 Co-ordinate space","text":"ggplot automatically pick scale axis, type coordinate space. plots Cartesian (linear X vs linear Y) coordinate space.plot, let’s say want x y origin set 0. can add xlim() ylim() functions, define limits axes:, can control coordinate space using coord() functions. Say want flip x y axes, add coord_flip():","code":"\n## Set axis limits ----\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g,\n             colour=species))+ \n  geom_point()+\n  geom_smooth(method=\"lm\",    \n              se=FALSE)+\n  xlim(0,240) + ylim(0,7000)\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g,\n             colour=species))+ \n  geom_point()+\n  geom_smooth(method=\"lm\",    \n              se=FALSE)+\n  xlim(0,240) + ylim(0,7000)+\n  coord_flip()"},{"path":"data-visualisation-with-ggplot2.html","id":"labels","chapter":"4 Data visualisation with ggplot2","heading":"4.4 Labels","text":"default, axis labels column names gave aesthetics aes(). can change axis labels using xlab() ylab() functions. Given column names often short can cryptic, functionality particularly important effectively communicating results.","code":"\n## Custom labels ----\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g,\n             colour=species))+ \n  geom_point()+\n  geom_smooth(method=\"lm\",    \n              se=FALSE)+\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\")"},{"path":"data-visualisation-with-ggplot2.html","id":"titles-and-subtitles","chapter":"4 Data visualisation with ggplot2","heading":"4.4.0.1 Titles and subtitles","text":"","code":"\n## Add titles ----\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g,\n             colour=species))+ \n  geom_point()+\n  geom_smooth(method=\"lm\",    \n              se=FALSE)+\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       title= \"Penguin Size, Palmer Station LTER\",\n       subtitle= \"Flipper length and body mass for three penguin species\")"},{"path":"data-visualisation-with-ggplot2.html","id":"themes","chapter":"4 Data visualisation with ggplot2","heading":"4.5 Themes","text":"Finally, overall appearance plot can modified using theme() functions. default theme grey background.\nmay prefer theme_classic(), theme_minimal() even theme_void(). Try .\nlot customisation available theme()\nfunction. look making custom themes later\nlessons\n\ncan also try installing running even wider range \npre-built themes install R package ggthemes.\n\nFirst need run \ninstall.packages(\"ggthemes\") command. Remember one\ntimes command written script \ntyped directly console. ’s ’s rude send\nsomeone script install packages computer - think \nlibrary() polite request instead!\n\naccess range themes available type\nhelp(ggthemes) follow documentation find \ncan .\n","code":"\n## Custom themes ----\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g,\n             colour=species))+ \n  geom_point()+\n  geom_smooth(method=\"lm\",    \n              se=FALSE)+\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       title= \"Penguin Size, Palmer Station LTER\",\n       subtitle= \"Flipper length and body mass for three penguin species\")+\n  theme_void()"},{"path":"data-visualisation-with-ggplot2.html","id":"more-geom-shapes","chapter":"4 Data visualisation with ggplot2","heading":"4.6 More geom shapes","text":"","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"jitter","chapter":"4 Data visualisation with ggplot2","heading":"4.6.1 Jitter","text":"geom_jitter() command adds random scatter points can reduce -plotting. Compare two plots:","code":"\n## geom point\n\nggplot(data = penguins, aes(x = species, y = culmen_length_mm)) +\n  geom_point(aes(color = species),\n              alpha = 0.7, \n              show.legend = FALSE) \n\n## More geoms ----\nggplot(data = penguins, aes(x = species, y = culmen_length_mm)) +\n  geom_jitter(aes(color = species),\n              width = 0.1, # specifies the width, change this to change the range of scatter\n              alpha = 0.7, # specifies the amount of transparency in the points\n              show.legend = FALSE) # don't leave a legend in a plot, if it doesn't add value"},{"path":"data-visualisation-with-ggplot2.html","id":"boxplots","chapter":"4 Data visualisation with ggplot2","heading":"4.6.2 Boxplots","text":"Box plots, ‘box & whisker plots’ another essential tool data analysis. Box plots summarize distribution set values displaying minimum maximum values, median (.e. middle-ranked value), range middle 50% values (inter-quartile range).\nwhisker line extending IQR box define Q3 + (1.5 x IQR), Q1 - (1.5 x IQR) respectively. can watch short video learn box plots .create box plot data use (prizes ) geom_boxplot()\nNote specifying colour variables using aes()\ngeometric shapes support internal colour “fill” external\ncolour “colour”. Try changing aes fill colour code ,\nnote happens.\npoints indicate outlier values [.e., greater Q3 + (1.5 x IQR)].can overlay boxplot scatter plot entire dataset, fully communicate raw summary data. reduce width jitter points slightly.\nexample switched using show.legend=FALSE inside\ngeom layer using theme(legend.position=“none”). ? \nexample reducing redundant code. specify\nshow.legend=FALSE every geom layer plot, theme\nfunction applies every layer. Save code, save time, reduce\nerrors!\n","code":"\nggplot(data = penguins, aes(x = species, y = culmen_length_mm)) +\n  geom_boxplot(aes(fill = species),\n              alpha = 0.7, \n              width = 0.5, # change width of boxplot\n              show.legend = FALSE)\nggplot(data = penguins, aes(x = species, y = culmen_length_mm)) +\n  geom_boxplot(aes(fill = species), # note fill is \"inside\" colour and colour is \"edges\" - try it for yourself\n              alpha = 0.2, # fainter boxes so the points \"pop\"\n              width = 0.5, # change width of boxplot\n              outlier.shape=NA)+\n  geom_jitter(aes(colour = species),\n                width=0.2)+\n  theme(legend.position = \"none\")"},{"path":"data-visualisation-with-ggplot2.html","id":"density-and-histogram","chapter":"4 Data visualisation with ggplot2","heading":"4.6.3 Density and histogram","text":"Compare following two sets code:first might struggle see/understand difference two charts. shapes roughly .first block code produced frequency histogram, bar represents actual number observations made within 'bin', second block code shows 'relative density' within bin. density histogram area curve sub-group sum 1. allows us compare distributions shapes sub-groups different sizes. example far fewer Adelie penguins dataset, density histogram occupy area graph two species.","code":"\npenguins %>% \n    ggplot(aes(x=culmen_length_mm, fill=species),\n           position = \"identity\")+\n    geom_histogram(bins=50)\npenguins %>% \n    ggplot(aes(x=culmen_length_mm, fill=species))+\n    geom_histogram(bins=50, \n                   aes(y=..density..),\n                   position = \"identity\")"},{"path":"data-visualisation-with-ggplot2.html","id":"more-colours","chapter":"4 Data visualisation with ggplot2","heading":"4.7 More Colours","text":"two main differences comes colors ggplot2. arguments, color fill, can specified single color \nassigned variables.already seen tutorial, variables inside aesthetics encoded variables outside properties unrelated variables.","code":"\npenguins %>% \n    ggplot(aes(x=culmen_length_mm))+\n    geom_histogram(bins=50, \n                   aes(y=..density..,\n                       fill=species), \n                   position = \"identity\",\n                   colour=\"black\")"},{"path":"data-visualisation-with-ggplot2.html","id":"choosing-and-using-colour-palettes","chapter":"4 Data visualisation with ggplot2","heading":"4.7.0.1 Choosing and using colour palettes","text":"can specify colours want assign variables number different ways.ggplot2, colors assigned variables modified via scale_color_* scale_fill_* functions. order use color data, importantly need know dealing categorical continuous variable. color palette chosen depending type variable:sequential diverging color palettes used continuous variablessequential diverging color palettes used continuous variablesqualitative color palettes (unordered) categorical variables:qualitative color palettes (unordered) categorical variables:can pick sets colours assign categorical variable. number specified colours match number categories. can use wide number preset colour names can use hexadecimals.can also use range inbuilt colour palettes:\ncan explore schemes available command\nRColorBrewer::display.brewer.()\nalso many, many extensions provide additional colour palettes. favourite packages include ggsci wesanderson","code":"\n## Custom colours ----\n\npenguin_colours <- c(\"darkolivegreen4\", \"darkorchid3\", \"goldenrod1\")\n\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g))+\n  geom_point(aes(colour=species))+\n  scale_color_manual(values=penguin_colours)+\n  theme_minimal()\npenguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g))+\n  geom_point(aes(colour=species))+\n  scale_color_brewer(palette=\"Set1\")+\n  theme_minimal()"},{"path":"data-visualisation-with-ggplot2.html","id":"accessibility","chapter":"4 Data visualisation with ggplot2","heading":"4.8 Accessibility","text":"","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"colour-blindness","chapter":"4 Data visualisation with ggplot2","heading":"4.8.0.1 Colour blindness","text":"easy get carried away colour palettes, remember times figures must accessible. One way check accessible figures use colour blindness checker colorBlindness","code":"\n## Check accessibility ----\n\nlibrary(colorBlindness)\ncolorBlindness::cvdPlot() # will automatically run on the last plot you made"},{"path":"data-visualisation-with-ggplot2.html","id":"guides-to-visual-accessibility","chapter":"4 Data visualisation with ggplot2","heading":"4.8.0.2 Guides to visual accessibility","text":"Using colours tell categories apart can useful, can see example , choose carefully. aesthetics can access geoms include shape, size - can combine complimentary ways enhance accessibility plots. hierarchy \"interpretability\" different types data","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"multiple-plots","chapter":"4 Data visualisation with ggplot2","heading":"4.9 Multiple plots","text":"","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"facets","chapter":"4 Data visualisation with ggplot2","heading":"4.9.0.1 Facets","text":"Adding combinations different aesthetics allows layer information onto 2D plot, sometimes though things just become busy. point becomes difficult see trends differences plot want break single plot sub-plots; called ‘faceting’. Facets commonly used much data display clearly single plot. revisit faceting , however now, let’s try facet plot according sex.use tilde symbol ‘~’ indicate column name form facet.","code":"\n## Facetting ----\npenguins %>% \n  drop_na(sex) %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = body_mass_g,\n             colour=species))+ \n  geom_point()+\n  geom_smooth(method=\"lm\",    \n              se=FALSE)+\n  facet_wrap(~sex)"},{"path":"data-visualisation-with-ggplot2.html","id":"patchwork","chapter":"4 Data visualisation with ggplot2","heading":"4.9.0.2 Patchwork","text":"many times might want combine separate figures multi-panel plots. Probably easiest way patchwork package (Pedersen (2022)).","code":"\n## Patchwork ----\nlibrary(patchwork)\n\np1 <- penguins %>% \n  ggplot(aes(x=flipper_length_mm, \n             y = culmen_length_mm))+\n  geom_point(aes(colour=species))+\n  scale_color_manual(values=penguin_colours)+\n  theme_minimal()\n\np2 <- penguins %>% \n  ggplot(aes(x=culmen_depth_mm, \n             y = culmen_length_mm))+\n  geom_point(aes(colour=species))+\n  scale_color_manual(values=penguin_colours)+\n  theme_minimal()\n\np3 <- penguins %>%     \n  group_by(sex,species) %>% \n    summarise(n=n()) %>% \n     drop_na(sex) %>% \n     ggplot(aes(x=species, y=n)) + \n  geom_col(aes(fill=sex), \n               width=0.8,\n               position=position_dodge(width=0.9), \n               alpha=0.6)+\n     scale_fill_manual(values=c(\"darkorange1\", \"azure4\"))+\n     theme_classic()\n\n (p1+p2)/p3+\n  plot_layout(guides = \"collect\") "},{"path":"data-visualisation-with-ggplot2.html","id":"activity-replicate-this-figure","chapter":"4 Data visualisation with ggplot2","heading":"4.10 Activity: Replicate this figure","text":"\nclose can get replicating figure ?\n\nMake NEW script assignment - replicate_figure.R\n\nMake sure use tips links end chapter, \ndone save file\n","code":"\npal <- c(\"#FF8C00\", \"#A034F0\", \"#159090\")\n\npenguins %>% \n  ggplot(aes(x = species,\n             y = body_mass_g,\n             fill = species,\n             colour = species))+\n  geom_violin(alpha = 0.2)+\n  geom_boxplot(width = 0.2,\n               alpha = 0.6)+\n  scale_fill_manual(values = pal)+\n  scale_colour_manual(values = pal)+\n  theme_classic()+\n  theme(legend.position = \"none\")+\n    labs(\n    x = \"\",\n    y = \"Body mass (g)\",\n    title = \"Body mass of brush-tailed penguins\",\n    subtitle = \"Box and violin plot of body mass by species\")"},{"path":"data-visualisation-with-ggplot2.html","id":"saving","chapter":"4 Data visualisation with ggplot2","heading":"4.11 Saving","text":"One easiest ways save figure made ggsave() function. default save last plot made screen.specify output path figures folder, provide file name. decided call plot plot (imaginative!) want save .PNG image file. can also specify resolution (dpi 300 good enough computer screens).\ngot far still time try one \nfollowing:\n\n\nMaking another type figure using penguins dataset, use \nreading use inspiration.\n\n\nMaking another type figure using penguins dataset, use \nreading use inspiration.\n\n\nUse data\n\n\nUse data\n","code":"\n# OUTPUT FIGURE TO FILE\n\nggsave(\"outputs/YYYYMMDD_ggplot_workshop_final_plot.png\", dpi=300)"},{"path":"data-visualisation-with-ggplot2.html","id":"quitting","chapter":"4 Data visualisation with ggplot2","heading":"4.11.0.1 Quitting","text":"\nMake sure saved script! Remember Download image\nfile RStudio Cloud onto computer.\n\nrun SessionInfo() end script gather\npackages versions using. useful \ncite R versions \npackages writing reports later.\n","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"finished-2","chapter":"4 Data visualisation with ggplot2","heading":"4.12 Finished","text":"Make sure saved scripts 💾 \"scripts\" folder.","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"what-we-learned","chapter":"4 Data visualisation with ggplot2","heading":"4.12.0.1 What we learned","text":"learnedThe anatomy ggplotsThe anatomy ggplotsHow add geoms different layersHow add geoms different layersHow use colour, colour palettes, facets, labels themesHow use colour, colour palettes, facets, labels themesPutting together multiple figuresPutting together multiple figuresHow save export imagesHow save export images","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"further-reading-guides-and-tips-on-data-visualisation","chapter":"4 Data visualisation with ggplot2","heading":"4.13 Further Reading, Guides and tips on data visualisation","text":"R Cheat SheetsR Cheat SheetsFundamentals Data Visualization: book tells everything need know presenting figures accessbility clarityFundamentals Data Visualization: book tells everything need know presenting figures accessbility clarityBeautiful Plotting R: incredibly handy ggplot guide build improve figuresBeautiful Plotting R: incredibly handy ggplot guide build improve figuresThe ggplot2 book: original Hadley Wickham book ggplot2The ggplot2 book: original Hadley Wickham book ggplot2","code":""},{"path":"data-insights-part-one.html","id":"data-insights-part-one","chapter":"5 Data Insights part one","heading":"5 Data Insights part one","text":"last chapters concentrating generating insights data using visualisations descriptive statistics. easiest way use questions tools guide investigation. ask question, question focuses attention specific part dataset helps decide graphs, models, transformations make.exercise propose task generate insights body mass penguins, order answer questionHow body mass associated bill length depth penguins?order answer question properly first understand different variables might relate .Distribution data typesCentral tendencyRelationship variablesConfounding variablesThis inevitably leads variety questions. new question ask expose new aspect data.","code":""},{"path":"data-insights-part-one.html","id":"data-wrangling","chapter":"5 Data Insights part one","heading":"5.0.0.1 Data wrangling","text":"Importantly already generated understanding variables contained within dataset data wrangling steps. Including:number variablesThe number variablesThe data format variableThe data format variableChecked missing dataChecked missing dataChecked typos, duplications data errorsChecked typos, duplications data errorsCleaned column factor namesCleaned column factor names\nimportant lose site questions \nasking\n\nalso play close attention data, remind \nfrequently many variables \nnames?\n\nmany rows/observations ?\n\nPay close attention outputs, errors warnings R\nconsole.\n","code":""},{"path":"data-insights-part-one.html","id":"variable-types","chapter":"5 Data Insights part one","heading":"5.1 Variable types","text":"quick refresher:","code":""},{"path":"data-insights-part-one.html","id":"numerical","chapter":"5 Data Insights part one","heading":"5.1.1 Numerical","text":"already familiar concepts numerical categorical data. Numeric variables values describe measure quantity. also known quantitative data. can subdivide numerical data :Continuous numeric variables. observations can take value within range numbers. Examples might include body mass (g), age, temperature flipper length (mm).\ntheory values can numbers, within dataset likely bounded (set within minimum/maximum observed measurable values), accuracy may precise measurement protocol allows.tibble represented header <dbl>.Discrete numeric variables Observations numeric restricted whole values e.g. 1,2,3,4,5 etc. also known integers. Discrete variables include number individuals population, number eggs laid etc. Anything make sense describe fractions e.g. penguin lay 2 half eggs. Counting!tibble represented header <int>.","code":""},{"path":"data-insights-part-one.html","id":"categorical","chapter":"5 Data Insights part one","heading":"5.1.2 Categorical","text":"Values describe characteristic data 'type' 'category'. Categorical variables mutually exclusive - one observation able fall two categories - exhaustive - data fit category (NA - recorded). Categorical variables qualitative, often represented non-numeric values words. bad idea represent categorical variables numbers (R treat correctly). Categorical variables can defined :Nominal variables Observations can take values logically ordered. Examples include Species Sex Penguins data.\ntibble represented header <chr>.Nominal variables Observations can take values logically ordered. Examples include Species Sex Penguins data.\ntibble represented header <chr>.Ordinal variables Observations can take values can logically ordered ranked. Examples include - activity levels (sedentary, moderately active, active); size classes (small, medium, large).Ordinal variables Observations can take values can logically ordered ranked. Examples include - activity levels (sedentary, moderately active, active); size classes (small, medium, large).coded manually see Factors, represented tibble header <fct>important order Ordinal variables logical order value plotting data visuals tables. Nominal variables flexible ordered whatever pattern works best data ( default plot alphabetically, perhaps order according values another numeric variable).","code":""},{"path":"data-insights-part-one.html","id":"quick-view-of-variables","chapter":"5 Data Insights part one","heading":"5.1.2.1 Quick view of variables","text":"take look variables, functions give quick snapshot overview.can see bill length contains numbers, many fractions, 0.1mm. comparison body mass appear discrete number variables. make body mass integer? underlying quantity (bodyweight) clearly continuous, clearly possible penguin weigh 3330.7g might look like integer way measured. illustrates importance understanding type variable working - just looking values enough.hand, choose measure record data can change way presented dataset. researchers decided simply record small, medium large classes bodyweight, dealing ordinal categorical variables (factors). distinctions can become less clear start deal multiple classes ordinal categories - example researchers measuring body mass nearest 10g. might reasonable treat integers...","code":"\nglimpse(penguins)\nsummary(penguins)"},{"path":"data-insights-part-one.html","id":"categorical-variables","chapter":"5 Data Insights part one","heading":"5.2 Categorical variables","text":"","code":""},{"path":"data-insights-part-one.html","id":"frequency","chapter":"5 Data Insights part one","heading":"5.2.0.1 Frequency","text":"might useful us make quick data summaries , like relative frequencySo 44% sample made observations Adelie penguins. comes making summaries categorical data, best can , can make observations common categorical observations, relative proportions.chart ok - can make anything better?go stacked bar approachThis graph OK great, height section bar represents relative proportions species dataset, type chart becomes increasingly difficult read categories included. Colours become increasingly samey,difficult read y-axis category starts stops, subtraction work values.best graph probably first one made - minor tweak can rapidly improve .example figure might use report paper. cleaned theme, added simple colour, made sure labels clear descriptive, ordered categories ascending frequency order, included simple text percentages aid readability.","code":"\npenguins %>% \n  group_by(species) %>% \n  summarise(n = n())\nprob_obs_species <- penguins %>% \n  group_by(species) %>% \n  summarise(n = n()) %>% \n  mutate(prob_obs = n/sum(n))\n\nprob_obs_species\npenguins %>% \n  ggplot()+\n  geom_bar(aes(x=species))\npenguins %>% \n  ggplot(aes(x=\"\",\n             fill=species))+ \n  # specify fill = species to ensure colours are defined by species\n  geom_bar(position=\"fill\")+ \n  # specify fill forces geom_bar to calculate percentages\n  scale_y_continuous(labels=scales::percent)+ \n  #use scales package to turn y axis into percentages easily\n  labs(x=\"\",\n       y=\"\")+\n  theme_minimal()\npenguins %>% \n  mutate(species=factor(species, levels=c(\"Adelie\",\n                                          \"Gentoo\",\n                                          \"Chinstrap\"))) %>% \n  # set as factor and provide levels\n  ggplot()+\n  geom_bar(aes(x=species),\n           fill=\"steelblue\",\n           width=0.8)+\n  labs(x=\"Species\",\n       y = \"Number of observations\")+\n  geom_text(data=prob_obs_species,\n            aes(y=(n+10),\n                x=species,\n                label=scales::percent(prob_obs)))+\n  coord_flip()+\n  theme_minimal()"},{"path":"data-insights-part-one.html","id":"two-categorical-variables","chapter":"5 Data Insights part one","heading":"5.2.0.2 Two categorical variables","text":"Understanding frequency broken species sex might useful information .","code":"\npenguins %>% \n  group_by(species, sex) %>% \n  summarise(n = n()) %>% \n  mutate(prob_obs = n/sum(n))"},{"path":"data-insights-part-one.html","id":"continuous-variables","chapter":"5 Data Insights part one","heading":"5.3 Continuous variables","text":"","code":""},{"path":"data-insights-part-one.html","id":"visualising-distributions","chapter":"5 Data Insights part one","heading":"5.3.0.1 Visualising distributions","text":"Variation tendency values variable change measurement measurement. can see variation easily real life; measure continuous variable twice, get two different results. true even measure quantities constant, like speed light. measurements include small amount error varies measurement measurement. Every variable pattern variation, can reveal interesting information. best way understand pattern visualise distribution variable’s values.script plot frequency distribution, specify x variable, intend plot histogram, y variable always count observations. ask data presented 10 equally sized bins data. case chopping x axis range 10 equal parts counting number observations fall within one.\nChange value specified bins argument observe \nfigure changes. usually good idea try one set\nbins order better insights data\nget data, combine data collected summary() function histogram hereWhich values common? < 3500g3500-4000g4000-4500g4500-5000g5000-5500g5500-6000g>6500gWhich values common? < 3500g3500-4000g4000-4500g4500-5000g5000-5500g5500-6000g>6500gWhich values rare? ? match expectations?\n< 3500g3500-4000g4000-4500g4500-5000g5000-5500g5500-6000g>6500gWhich values rare? ? match expectations?\n< 3500g3500-4000g4000-4500g4500-5000g5000-5500g5500-6000g>6500gCan see unusual patterns? YesNoCan see unusual patterns? YesNoHow many observations missing body mass information? many observations missing body mass information? Penguins weighing less 3kg 6kg rare.\ncommon weight appears just 4kg.","code":"\npenguins %>% \n  ggplot()+\n  geom_histogram(aes(x=body_mass_g),\n                 bins=10)"},{"path":"data-insights-part-one.html","id":"atypical-values","chapter":"5 Data Insights part one","heading":"5.3.0.2 Atypical values","text":"found atypical values point, decide exclude dataset (using filter()). stage strong reason believing mistake data entry, rather true outlier.","code":""},{"path":"data-insights-part-one.html","id":"central-tendency","chapter":"5 Data Insights part one","heading":"5.3.0.3 Central tendency","text":"Central tendency descriptive summary dataset single value reflects center data distribution. three widely used measures central tendency mean, median mode.mean defined sum values variable divided total number values. median middle value. N odd N even, average two middle values. mode frequently occurring observation data set, arguable least useful understanding biological datasets.can find mean median easily summarise function. mean usually best measure central tendency distribution symmetrical, mode best measure distribution asymmetrical/skewed.\nFigure 5.1: Red dashed line represents mean, Black dashed line median value\n","code":"\npenguin_body_mass_summary <- penguins %>% \n    summarise(mean_body_mass=mean(body_mass_g, na.rm=T), \n              sd = sd(body_mass_g, na.rm = T),\n              median_body_mass=median(body_mass_g, na.rm=T), \n              iqr = IQR(body_mass_g, na.rm = T))\n\npenguin_body_mass_summary\npenguins %>% \nggplot()+\n  geom_histogram(aes(x=body_mass_g),\n               alpha=0.8,\n               bins = 10,\n               fill=\"steelblue\",\n               colour=\"darkgrey\")+\n   geom_vline(data=penguin_body_mass_summary,\n             aes(xintercept=mean_body_mass),\n             colour=\"red\",\n             linetype=\"dashed\")+\n     geom_vline(data=penguin_body_mass_summary,\n             aes(xintercept=median_body_mass),\n             colour=\"black\",\n             linetype=\"dashed\")+\n  labs(x = \"Body mass (g)\",\n       y = \"Count\")+\n  theme_classic()"},{"path":"data-insights-part-one.html","id":"normal-distribution","chapter":"5 Data Insights part one","heading":"5.3.0.4 Normal distribution","text":"histogram can likely already tell whether normally distributed data.\nNormal distribution, also known “Gaussian distribution”, \nprobability distribution symmetric mean, showing \ndata near mean frequent occurrence data far \nmean. graphical form, normal distribution appears “bell\ncurve”.\ndata follows normal distribution, can predict spread data, likelihood observing datapoint given value mean standard deviation.can simulate normally distributed dataset look like sample size, mean standard deviation.","code":"\nnorm_mass <- rnorm(n = 344,\n      mean = 4201.754,\n      sd = 801.9545) %>% \n  as_tibble()\n\nnorm_mass %>% \n  as_tibble() %>% \n  ggplot()+\n  geom_histogram(aes(x = value),\n                 bins = 10)"},{"path":"data-insights-part-one.html","id":"qq-plot","chapter":"5 Data Insights part one","heading":"5.3.0.5 QQ-plot","text":"QQ plot classic way checking whether sample distribution another (theoretical distribution). look bit odd first, actually fairly easy understand, useful! qqplot distributes data y-axis, theoretical normal distribution x-axis. residuals follow normal distribution, meet produce perfect diagonal line across plot.Watch video see QQ plots explained\nFigure 5.2: Examples qqplots different deviations normal distribution\nexample can see residuals can explained normal distribution, except low end data.fit perfect, also terrible!know much deviation idealised distribution ok?qqPlot() function R package car provides 95% confidence interval margins help determine severely quantiles deviate idealised distribution.information qqPlot section distribution deviates clearly normal distribution <3500g3500-4000g4000-4500g5000-5500g>5500g","code":"\nggplot(penguins, aes(sample = body_mass_g))+\n  stat_qq() + \n  stat_qq_line()\npenguins %>% \n  pull(body_mass_g) %>% \n  car::qqPlot()## [1] 170 186"},{"path":"data-insights-part-one.html","id":"variation","chapter":"5 Data Insights part one","heading":"5.3.0.6 Variation","text":"Dispersion (spread data ) important component towards understanding numeric variable. measures central tendency used estimate central value dataset, measures dispersion important describing spread data.Two data sets can equal mean (, measure central tendency) vastly different variability.Important measures dispersion range, interquartile range, variance standard deviation.range defined difference highest lowest values dataset. disadvantage defining range measure dispersion take account values calculation.range defined difference highest lowest values dataset. disadvantage defining range measure dispersion take account values calculation.interquartile range defined difference third quartile denoted 𝑸_𝟑 lower quartile denoted 𝑸_𝟏 . 75% observations lie third quartile 25% observations lie first quartile.interquartile range defined difference third quartile denoted 𝑸_𝟑 lower quartile denoted 𝑸_𝟏 . 75% observations lie third quartile 25% observations lie first quartile.Variance defined sum squares deviations mean, divided total number observations. standard deviation positive square root variance. standard deviation preferred instead variance units original values.Variance defined sum squares deviations mean, divided total number observations. standard deviation positive square root variance. standard deviation preferred instead variance units original values.","code":""},{"path":"data-insights-part-one.html","id":"interquartile-range","chapter":"5 Data Insights part one","heading":"5.3.0.7 Interquartile range","text":"used IQR function summarise() find interquartile range body mass variable.IQR also useful applied summary plots 'box whisker plots'. can also calculate values IQR margins, add labels scales Wickham & Seidel (2022).can see IQR obtained subtracting body mass tht 75% quantile 25% quantile (4750-3550 = 1200).","code":"\npenguins %>%\n  summarise(q_body_mass = quantile(body_mass_g, c(0.25, 0.5, 0.75), na.rm=TRUE),\n            quantile = scales::percent(c(0.25, 0.5, 0.75))) # scales package allows easy converting from data values to perceptual properties"},{"path":"data-insights-part-one.html","id":"standard-deviation","chapter":"5 Data Insights part one","heading":"5.3.0.8 Standard deviation","text":"standard deviation (\\(s\\)) measure dispersed data relation mean. Low standard deviation means data clustered around mean, high standard deviation indicates data spread . makes sense use mean good measure central tendency.\n\\(\\sigma\\) = known population\nstandard deviation\n\n\\(s\\) = sample standard\ndeviation\n","code":"penguins %>% \n  summarise(mean = mean(body_mass_g),\n            sd = sd(body_mass_g),\n            n = n())"},{"path":"data-insights-part-one.html","id":"visualising-dispersion","chapter":"5 Data Insights part one","heading":"5.3.0.9 Visualising dispersion","text":"\nFigure 5.3: Visualising dispersion different figures\nnow several compact representations body_mass_g including histogram, boxplot summary calculations. can generate summaries numeric variables. tables graphs provide detail need understand central tendency dispersion numeric variables.","code":"\ncolour_fill <- \"darkorange\"\ncolour_line <- \"steelblue\"\nlims <- c(0,7000)\n\nbody_weight_plot <- function(){\n  \n  penguins %>% \n  ggplot(aes(x=\"\",\n             y= body_mass_g))+\n  labs(x= \" \",\n       y = \"Mass (g)\")+\n  scale_y_continuous(limits = lims)+\n    theme_minimal()\n}\n\nplot_1 <- body_weight_plot()+\n  geom_jitter(fill = colour_fill,\n               colour = colour_line,\n               width = 0.2,\n              shape = 21)\n\nplot_2 <- body_weight_plot()+\n  geom_boxplot(fill = colour_fill,\n               colour = colour_line,\n               width = 0.4)\n\nplot_3 <- penguin_body_mass_summary %>% \n  ggplot(aes(x = \" \",\n             y = mean_body_mass))+\n  geom_bar(stat = \"identity\",\n           fill = colour_fill,\n           colour = colour_line,\n               width = 0.2)+\n  geom_errorbar(data = penguin_body_mass_summary,\n                aes(ymin = mean_body_mass - sd,\n                    ymax = mean_body_mass + sd),\n                colour = colour_line,\n                width = 0.1)+\n  labs(x = \" \",\n       y = \"Body mass (g)\")+\n  scale_y_continuous(limits = lims)+\n  theme_minimal()\n\n\nplot_1 + plot_2 + plot_3 "},{"path":"data-insights-part-one.html","id":"drop_na","chapter":"5 Data Insights part one","heading":"5.3.0.10 drop_na","text":"first met NA back Chapter 4 hopefully noticed, either previous chapters, missing values NA can really mess calculations. different ways can deal missing data:drop_na() everything start. runs risk lose lot data every row, NA column removeddrop_na() everything start. runs risk lose lot data every row, NA column removeddrop_na() particular variable. fine, approach cautiously - way write data new object e.g. penguins <- penguins %>% drop_na(body_mass_g) removed data forever - perhaps want drop rows specific calculation - might contain useful information variables.drop_na() particular variable. fine, approach cautiously - way write data new object e.g. penguins <- penguins %>% drop_na(body_mass_g) removed data forever - perhaps want drop rows specific calculation - might contain useful information variables.drop_na() specific task - cautious approach need aware another phenomena. data missing random? might need investigate missing values dataset. Data truly missing random can removed dataset without introducing bias. However, bad weather conditions meant researchers get particular island measure one set penguins data missing random treated caution. island contained one particular species penguin, might mean complete data two three penguin species. nothing can incomplete data aware data missing random influence distributions.drop_na() specific task - cautious approach need aware another phenomena. data missing random? might need investigate missing values dataset. Data truly missing random can removed dataset without introducing bias. However, bad weather conditions meant researchers get particular island measure one set penguins data missing random treated caution. island contained one particular species penguin, might mean complete data two three penguin species. nothing can incomplete data aware data missing random influence distributions.","code":""},{"path":"data-insights-part-one.html","id":"categorical-and-continuous-variables","chapter":"5 Data Insights part one","heading":"5.4 Categorical and continuous variables","text":"’s common want explore distribution continuous variable broken categorical variable.\nFigure 5.4: Species sex likely affect body mass\nreasonable think perhaps either species sex might affect morphology beaks directly - might affect body mass (direct relationship mass beak length, also indirect relationship sex species).best simplest place start exploring possible relationships producing simple figures.start looking distribution body mass species.","code":""},{"path":"data-insights-part-one.html","id":"activity-1-produce-a-plot-which-allows-you-to-look-at-the-distribution-of-penguin-body-mass-observations-by-species","chapter":"5 Data Insights part one","heading":"5.5 Activity 1: Produce a plot which allows you to look at the distribution of penguin body mass observations by species","text":"reasonable think perhaps either species sex might affect body mass, can visualise number different ways. last method, density histogram, looks little crowded now, use excellent ggridges package help ","code":"\njitter_plot <- penguins %>% \n    ggplot(aes(x = species,\n               y = body_mass_g))+\n    geom_jitter(shape = 21,\n                fill = colour_fill,\n                colour = colour_line,\n                width = 0.2)+\n  coord_flip()\n\nbox_plot <- penguins %>% \n    ggplot(aes(x = species,\n               y = body_mass_g))+\n    geom_boxplot(fill = colour_fill,\n                colour = colour_line,\n                width = 0.2)+\n  coord_flip()\n\nhistogram_plot <- penguins %>% \n    ggplot(aes(fill = species))+\n    geom_histogram(aes(x = body_mass_g,\n                       y = ..density..),\n                   position = \"identity\",\n                   alpha = 0.6,\n                colour = colour_line)\n\njitter_plot/box_plot/histogram_plot"},{"path":"data-insights-part-one.html","id":"ggridges","chapter":"5 Data Insights part one","heading":"5.6 GGridges","text":"package ggridges (Wilke (2022)) provides excellent extra geoms supplement ggplot. One useful features allow different groups mapped y axis, histograms easily viewed.Q. species data distribution appears normally distributed?Gentoo YesNoGentoo YesNoChinstrap YesNoChinstrap YesNoAdelie YesNoAdelie YesNoWhile Gentoo density plot appears show two peaks, qqplot indicates deviate might expect normal distribution. still investigate whether \"two populations\" .","code":"\nlibrary(ggridges)\nggplot(penguins, aes(x = body_mass_g, y = species)) + \n  ggridges::geom_density_ridges(fill = colour_fill,\n                colour = colour_line,\n                alpha = 0.8)\npenguins %>% \n  group_split(species) %>% \n  map(~ pull(.x, body_mass_g) \n      %>% car::qqPlot())## [[1]]\n## [1] 110 102\n## \n## [[2]]\n## [1] 38 39\n## \n## [[3]]\n## [1] 18 41\npenguins %>% drop_na %>% ggplot(aes(x = body_mass_g, y = species)) + \n    geom_density_ridges(aes(fill = sex),\n                        colour = colour_line,\n                        alpha = 0.8,\n                        bandwidth = 175)\n# try playing with the bandwidth argument - this behaves similar to binning which you should be familiar with from using geom_histogram"},{"path":"data-insights-part-one.html","id":"activity-2-test-yourself","chapter":"5 Data Insights part one","heading":"5.7 Activity 2: Test yourself","text":"Question 1. Write insights made data relationships observed. Compare ones . agree ? miss ? observations make list .revealing really interesting insights shape distribution body sizes penguin populations now.example:Gentoo penguins appear show strong sexual dimorphism almost males larger females (little overlap density curves).Gentoo penguins appear show strong sexual dimorphism almost males larger females (little overlap density curves).Gentoo males females average larger two penguin speciesGentoo males females average larger two penguin speciesGentoo females two distinct peaks body mass.Gentoo females two distinct peaks body mass.Chinstrap penguins also show evidence sexual dimorphism, though greater overlap.Chinstrap penguins also show evidence sexual dimorphism, though greater overlap.Adelie penguins larger males females average, wide spread male body mass, (possibly two groups?)Adelie penguins larger males females average, wide spread male body mass, (possibly two groups?)Note able understand data better, spending time making data visuals. descriptive data statistics (mean, median) measures variance (range, IQR, sd) important. substitutes spending time thinking data making exploratory analyses.Question 2. Using summarise can quickly calculate \\(s\\) can replicate hand dplyr functions? - total \\(s\\) (category).ResidualsResidualsSquared residualsSquared residualsSum squaresSum squaresVariance = SS/dfVariance = SS/df\\(s=\\sqrt{Variance}\\)\\(s=\\sqrt{Variance}\\)","code":"\nmean <- penguins %>% \n    summarise(mean = mean(body_mass_g, na.rm = T))\n\npenguins %>% \n    mutate(residuals = (body_mass_g - pull(mean)),\n           sqrd_resid = residuals^2) %>% \n    drop_na(sqrd_resid) %>% \n    summarise(sum_squares = sum(sqrd_resid),\n              variance = sum_squares/(n=n())-1,\n              sd = sqrt(variance))"},{"path":"data-insights-part-two.html","id":"data-insights-part-two","chapter":"6 Data insights part two","heading":"6 Data insights part two","text":"previous chapter looked individual variables, understanding different types data. made numeric graphical summaries distributions features within variable. week continue work space, extend understanding include relationships variables.Understanding relationship two variables often basis scientific questions. might include comparing variables type (numeric numeric) different types (numeric categorical). chapter see can use descriptive statistics visuals explore associations","code":""},{"path":"data-insights-part-two.html","id":"associations-between-numerical-variables","chapter":"6 Data insights part two","heading":"6.1 Associations between numerical variables","text":"","code":""},{"path":"data-insights-part-two.html","id":"correlations","chapter":"6 Data insights part two","heading":"6.1.0.1 Correlations","text":"common measure association two numerical variables correlation coefficient. correlation metric numerical measure strength associationThere several measures correlation including:Pearson's correlation coefficient : good describing linear associationsPearson's correlation coefficient : good describing linear associationsSpearman's rank correlation coefficient: rank ordered correlation - good assumptions Pearson's correlation met.Spearman's rank correlation coefficient: rank ordered correlation - good assumptions Pearson's correlation met.Pearson's correlation coefficient r designed measure strength linear (straight line) association. Pearson's takes value -1 1.value 0 means linear association variablesA value 0 means linear association variablesA value 1 means perfect positive association variablesA value 1 means perfect positive association variablesA value -1 means perfect negative association variablesA value -1 means perfect negative association variablesA perfect association one can predict value one variable complete accuracy, just knowing value variable.can use cor function R calculate Pearson's correlation coefficient.tells us two features association. sign magnitude. coefficient negative, bill length increases, bill depth decreases. value -0.22 indicates 22% variation bill length can explained changes bill depth (vice-versa), suggesting variables closely related.\nFigure 6.1: Different relationships two numeric variables. number represents Pearson's correlation coefficient association\nPearson's coefficient designed summarise strength linear relationship, can misleading relationship linear e.g. curved humped. always good idea plot relationship first (see ).Pearson's coefficient designed summarise strength linear relationship, can misleading relationship linear e.g. curved humped. always good idea plot relationship first (see ).Even relationship linear, tell us anything steepness association (see ). tells us often change one variable can predict change value change.Even relationship linear, tell us anything steepness association (see ). tells us often change one variable can predict change value change.can difficult understand first, carefully consider figure .first row shows differing levels strength association. drew perfect straight line two variables, closely data points fit around line.first row shows differing levels strength association. drew perfect straight line two variables, closely data points fit around line.second row shows series perfect linear relationships. can accurately predict value one variable just knowing value variable, steepness relationship example different. important means perfect association can still small effect.second row shows series perfect linear relationships. can accurately predict value one variable just knowing value variable, steepness relationship example different. important means perfect association can still small effect.third row shows series associations clearly relationship two variables, also linear inappropriate Pearson's correlation.third row shows series associations clearly relationship two variables, also linear inappropriate Pearson's correlation.","code":"\nlibrary(rstatix)\n\npenguins %>% \n  cor_test(culmen_length_mm, culmen_depth_mm)"},{"path":"data-insights-part-two.html","id":"non-linear-correlations","chapter":"6 Data insights part two","heading":"6.1.0.2 Non-linear correlations","text":"relationship variables non-linear? Instead using Pearson's correlation coefficient can calculate something called rank correlation.Instead working raw values two variables can use rank ordering instead. idea pretty simple start lowest vaule variable order '1', assign labels '2', '3' etc. ascend rank order. can see way applied manually function dense_rank dplyr :Measures rank correlation just comparison rank orders two variables, value -1 1 just like Pearsons's. already know Pearson's correlation coefficient, expect relationship negative. come surprise highest rank order values bill_length_mm appear associated lower rank order values bill_depth_mm.calculate Spearman's \\(\\rho\\) 'rho' pretty easy, can use cor functions , time specify hidden argument method=\"spearman\".can see example Pearson's r Spearman's \\(\\rho\\) basically identical.","code":"\npenguins %>% select(culmen_length_mm, \n                    culmen_depth_mm) %>% \n  drop_na() %>% \n  mutate(rank_length=dense_rank((culmen_length_mm)), \n         rank_depth=dense_rank((culmen_depth_mm))) %>% \n  head()\npenguins %>% \n  cor_test(culmen_length_mm, culmen_depth_mm, method=\"spearman\")"},{"path":"data-insights-part-two.html","id":"graphical-summaries-between-numeric-variables","chapter":"6 Data insights part two","heading":"6.1.0.3 Graphical summaries between numeric variables","text":"Correlation coefficients quick simple way attach metric level association two variables. limited however single number can never capture every aspect relationship. visualise data.already covered scatter plots ggplot2() extensively previous chapters, just cover different ways present nature relationship\nFigure 6.2: scatter plot bill depth bill length mm\n**Note - Remember number different options available constructing plot including changing alpha produce transparency plots lying top , colours (shapes) separate subgroups ways present third numerical variables setting aes(size=body_mass_g).\nFigure 6.3: Using patchwork can easily arrange extra plots fit marginals - boxplots, histograms density plots\nefforts allow us capture details spread distribution variables relate . figure provides us insights intoThe central tendency variableThe central tendency variableThe spread data variableThe spread data variableThe correlation two variablesThe correlation two variables","code":"\nlength_depth_scatterplot <- ggplot(penguins, aes(x= culmen_length_mm, \n                     y= culmen_depth_mm)) +\n    geom_point()\n\nlength_depth_scatterplot\nlibrary(patchwork) # package calls should be placed at the TOP of your script\n\nbill_depth_marginal <- penguins %>% \n  ggplot()+\n  geom_density(aes(x=culmen_depth_mm), fill=\"darkgrey\")+\n  theme_void()+\n  coord_flip() # this graph needs to be rotated\n\nbill_length_marginal <- penguins %>% \n  ggplot()+\n  geom_density(aes(x=culmen_length_mm), fill=\"darkgrey\")+\n  theme_void()\n\nlayout <- \"\nAA#\nBBC\nBBC\"\n# layout is easiest to organise using a text distribution, where ABC equal the three plots in order, and the grid is how much space they take up. We could easily make the main plot bigger and marginals smaller with\n\n# layout <- \"\n# AAA#\n# BBBC\n# BBBC\"\n# BBBC\n\nbill_length_marginal+length_depth_scatterplot+bill_depth_marginal+ # order of plots is important\n  plot_layout(design=layout) # uses the layout argument defined above to arrange the size and position of plots"},{"path":"data-insights-part-two.html","id":"associations-between-categorical-variables","chapter":"6 Data insights part two","heading":"6.2 Associations between categorical variables","text":"Exploring associations different categorical variables quite simple previous numeric-numeric examples. Generally speaking interested whether different combinations categories uniformally distributed show evidence clustering leading - -represented combinations.\nsimplest way investigate use group_by summarise used previously.**Note - remember group_by() applies functions comes group-specific pattern.tell us, 168 observations made Island Biscoe, three times many Gentoo penguin observations made Adelie penguins (remeber observations made, individual penguins). account penguin ID see around twice many Gentoo penguins recorded. can see Chinstrap penguins recorded Biscoe. Conversely can see Gentoo penguins observed Biscoe.\nisland Dream two populations Adelie Chinstrap penguins roughly equal size, island Torgensen appears population comprised Adelie penguins.also use bar chart ggplot represent count data.fine, looks bit odd, bars expand fill available space category axis. Luckily advanced version postion_dodge argument.alternative approach look 'relative proportions' population overall dataset. Using methods used previously looking single variables. add aesthetic tweaks improve look.\nFigure 6.4: dodged barplot showing numbers relative proportions data observations recorded penguin species location\n","code":"\nisland_species_summary <- penguins %>% \n  group_by(island, species) %>% \n  summarise(n=n(),\n            n_distinct=n_distinct(individual_id)) %>% \n  ungroup() %>% # needed to remove group calculations\n  mutate(freq=n/sum(n)) # then calculates percentage of each group across WHOLE dataset\n\nisland_species_summary\npenguins%>% \n  ggplot(aes(x=island, fill=species))+\n  geom_bar(position=position_dodge())+\n  coord_flip()\npenguins%>% \n  ggplot(aes(x=island, fill=species))+\n  geom_bar(position=position_dodge2(preserve=\"single\"))+ \n  #keeps bars to appropriate widths\n  coord_flip()\npenguins %>% \n  ggplot(aes(x=island, fill=species))+\n  geom_bar(position=position_dodge2(preserve=\"single\"))+ \n  #keeps bars to appropriate widths\n    labs(x=\"Island\",\n       y = \"Number of observations\")+\n  geom_text(data=island_species_summary, # use the data from the summarise object\n            aes(x=island,\n                y= n+10, # offset text to be slightly to the right of bar\n                group=species, # need species group to separate text\n                label=scales::percent(freq) # automatically add %\n                ),\n            position=position_dodge2(width=0.8))+ # set width of dodge\n  scale_fill_manual(values=c(\"cyan\",\n                            \"darkorange\",\n                            \"purple\"\n                            ))+\n  coord_flip()+\n  theme_minimal()+\n  theme(legend.position=\"bottom\") # put legend at the bottom of the graph"},{"path":"data-insights-part-two.html","id":"associations-between-categorical-numerical-variables","chapter":"6 Data insights part two","heading":"6.3 Associations between Categorical-numerical variables","text":"","code":"\npenguins %>% \n  ggplot(aes(x=species,\n             y=body_mass_g))+\n  geom_boxplot()+\n  labs(y=\"Body mass (g)\",\n         x= \"Species\")\npenguins %>% \n  ggplot(aes(x=body_mass_g,\n             fill=species))+\n  geom_histogram(alpha=0.6,\n         bins=30,\n         position=\"identity\")+\n  facet_wrap(~species,\n             ncol=1)"},{"path":"data-insights-part-two.html","id":"complexity","chapter":"6 Data insights part two","heading":"6.4 Complexity","text":"reasonable think perhaps either species sex might affect morphology beaks directly - might affect body mass (direct relationship mass beak length, also indirect relationship sex species).Failure account complex interactions can lead misleading insights data.","code":""},{"path":"data-insights-part-two.html","id":"simpsons-paradox","chapter":"6 Data insights part two","heading":"6.4.1 Simpson's Paradox","text":"Remember first correlated bill length bill depth found overall negative correlation -0.22. However, confounding variable accounted - species.another example carefully studying data - carefully considering variables likely affect studied controlled . entirely reasonable hypothesis different penguin species might different bill shapes might make overall trend misleading. can easily check effect categoricial variable two numeric variables assigning aesthetic colour.now clearly see striking reversal previous trend, fact within species penguin overall positive association bill length depth.prompt us re-evaluate correlation metrics:now see correlation values three species >0.22 - indicating associations much closer previously estimated.","code":"\ncolours <- c(\"cyan\",\n             \"darkorange\",\n             \"purple\")\n\nlength_depth_scatterplot_2 <- ggplot(penguins, aes(x= culmen_length_mm, \n                     y= culmen_depth_mm,\n                     colour=species)) +\n    geom_point()+\n  geom_smooth(method=\"lm\",\n              se=FALSE)+\n  scale_colour_manual(values=colours)+\n  theme_classic()+\n  theme(legend.position=\"none\")+\n    labs(x=\"Bill length (mm)\",\n         y=\"Bill depth (mm)\")\n\nlength_depth_scatterplot\nbill_depth_marginal_2 <- penguins %>% \n  ggplot()+\n  geom_density(aes(x=culmen_depth_mm,\n                   fill=species),\n               alpha=0.5)+\n  scale_fill_manual(values=colours)+\n  theme_void()+\n  coord_flip() # this graph needs to be rotated\n\nbill_length_marginal_2 <- penguins %>% \n  ggplot()+\n  geom_density(aes(x=culmen_length_mm,\n                   fill=species),\n               alpha=0.5)+\n  scale_fill_manual(values=colours)+\n  theme_void()+\n  theme(legend.position=\"none\")\n\nlayout2 <- \"\nAAA#\nBBBC\nBBBC\nBBBC\"\n\nbill_length_marginal_2+length_depth_scatterplot_2+bill_depth_marginal_2+ # order of plots is important\n  plot_layout(design=layout2) # uses the layout argument defined above to arrange the size and position of plots\npenguins %>% \n  group_by(species) %>% \n  cor_test(culmen_length_mm, culmen_depth_mm)"},{"path":"data-insights-part-two.html","id":"three-or-more-variables","chapter":"6 Data insights part two","heading":"6.4.2 Three or more variables","text":"example therefore, saw importance exploring relationships among two variables . Broadly speaking two ways top thisLayer extra aesthetic mapping onto ggplot - size, colour, shapeUse facets construct multipanel plots according values categorical variableIf want can also adopt approaches time:can see trends across different penguin sexes. Although comparing slopes lines, lengths lines amounts overlap can make insights \"sexually dimorphic\" different species e.g. terms beak morphology species show greater differences males females others?","code":"\npenguins %>% \n  drop_na(sex) %>% \nggplot(aes(x= culmen_length_mm, \n                     y= culmen_depth_mm,\n                     colour=sex)) + # colour aesthetic set to sex\n    geom_point(aes(shape = species))+\n  geom_smooth(aes(group = species),\n              method=\"lm\",\n              se=FALSE)+\n  scale_colour_manual(values=c(\"#1B9E77\", \"#D95F02\"))+ # pick two colour scheme\n  theme_classic()+\n  theme(legend.position=\"none\")+\n    labs(x=\"Bill length (mm)\",\n         y=\"Bill depth (mm)\")\npenguins %>% \n  drop_na(sex) %>% \nggplot(aes(x= culmen_length_mm, \n                     y= culmen_depth_mm,\n                     colour=sex)) + # colour aesthetic set to sex\n    geom_point()+\n  geom_smooth(method=\"lm\",\n              se=FALSE)+\n  scale_colour_manual(values=c(\"#1B9E77\", \"#D95F02\"))+ # pick two colour scheme\n  theme_classic()+\n  theme(legend.position=\"none\")+\n    labs(x=\"Bill length (mm)\",\n         y=\"Bill depth (mm)\")+\n  facet_wrap(~species, ncol=1) # specify plots are stacked split by species"},{"path":"data-insights-part-two.html","id":"summing-up","chapter":"6 Data insights part two","heading":"6.5 Summing up","text":"last data handling workshop. built towards able discover examine relationships differences among variables data. now skills handle many different types data, tidy , produce visuals generate insight communicate others.note caution required - easy spot identify patterns.spot trend, difference relationship, important recognise may enough evidence assign reason behind observation. scientists important develope hypotheses based knowledge understanding, can help (sometimes) avoiding spurious associations.Sometimes may see pattern data, likely occurred due random chance, rather result underlying process. formal statistical analysis, quantitatively assess evidence, assess probability study effect sizes can incredibly powerful. delve exciting topics next term.! Thank taking time get far. kind found difficult. done incredibly well.","code":""},{"path":"introduction-to-statistics.html","id":"introduction-to-statistics","chapter":"7 Introduction to statistics","heading":"7 Introduction to statistics","text":"term focusing statistics. actually quite lot descriptive statistics work last term. Every time summarised described data, calculating mean, median, standard deviation, frequency/count, distribution carrying descriptive statistics helped us understand data better.building develop skills inferential statistics. Inferential statistics allow us make generalisations - taking descriptive statistics data sample mean, using say something population parameter (.e. population mean).example might measure measure heights plants outcrossed inbred make summaries figures construct average difference height (descriptive). use produce estimates general effect outcrossing vs inbreeding plant heights (inferential).","code":""},{"path":"introduction-to-statistics.html","id":"darwins-maize-data","chapter":"7 Introduction to statistics","heading":"7.1 Darwin's maize data","text":"Loss genetic diversity important issue conservation species. Declines population size due exploitation, habitat fragmentation lead loss genetic diversity. Even populations restored viable numbers conservation efforts may suffer continued loss population fitness inbreeding depression.Charles Darwin even wrote book subject \"Effects Cross Self-Fertilisation Vegetable Kingdom\". describes produced seeds maize (Zea mays) fertilised pollen individual different plant. height seedlings produced measured proxy evolutionary fitness.Darwin wanted know whether inbreeding reduced fitness selfed plants - hypothesis. data going use today Darwin's original dataset.\ngo - make sure basic R project set \nscratch. know data file saved? got\nseparate subfolders set within project?\n\nset script put work - use write\ninstructions store comments. Use File > New Script menu item\nselect R Script.\n","code":"\nlibrary(tidyverse)\n\n\ndarwin <- read_csv(\"data/darwin.csv\")"},{"path":"introduction-to-statistics.html","id":"activity-1-carry-out-some-basic-exploratory-data-analysis","chapter":"7 Introduction to statistics","heading":"7.2 Activity 1: Carry out some basic exploratory data analysis","text":"first thing know now always start exploring data. want stretch , see can perform basic data check without prompts.","code":"\n# check the structure of the data\nglimpse(darwin)\n\n# check data is in a tidy format\nhead(darwin)\n\n# check variable names\ncolnames(darwin)\n\n\n# clean up column names\n\ndarwin <- janitor::clean_names(darwin)\n\n# check for duplication\ndarwin %>% \n  duplicated() %>% \n  sum()\n\n# check for typos - by looking at impossible values\ndarwin %>% \n  summarise(min=min(height, na.rm=TRUE), \n            max=max(height, na.rm=TRUE))\n\n# check for typos by looking at distinct characters/values\n\ndarwin %>% \n  distinct(pair)\n\ndarwin %>% \n  distinct(type)\n\n# missing values\ndarwin %>% \n  is.na() %>% \n  sum()\n\n# quick summary\n\nsummary(darwin)"},{"path":"introduction-to-statistics.html","id":"visualisation","chapter":"7 Introduction to statistics","heading":"7.2.0.1 Visualisation","text":"Now seems like good time first data visualisation.graph clearly shows average height 'crossed' plants greater 'selfed' plants. need investigate order determine whether signal (apparent differences mean values) greater level noise (variance within different groups).variance appears roughly similar two groups - though making graph can now clearly see crossed group, potential outlier value 12.","code":"\ndarwin %>% \n  ggplot(aes(x=type,\n         y=height))+\n  geom_point()\n# you could also substitute (or combine) other geoms including\n# geom_boxplot()\n# geom_violin()\n# geom_histogram()\n# Why not have a go and see what you can make?"},{"path":"introduction-to-statistics.html","id":"comparing-groups","chapter":"7 Introduction to statistics","heading":"7.2.0.2 Comparing groups","text":"seen previously can use various tidy functions determine mean standard deviations groups.\n(re)familiarise () calculate\nstandard deviation.\nSummary statistics like presented figures tables. normally reserve tables simple sets numbers, instance present .\nTable 7.1: Summary statistics crossed selfed maize plants\nDescriptive statistics careful data checking often skipped steps rush answer big questions. However, description essential part early phase analysis.","code":"\ndarwin %>% \n  group_by(type) %>% \n  summarise(mean=mean(height),\n            sd=sd(height))\n# make a new object\ndarwin_summary <-darwin %>% \n  group_by(type) %>% \n  summarise(mean=mean(height),\n            sd=sd(height))\n\n# make a summary plot\ndarwin_summary %>% \n  ggplot(aes(x=type,\n             y=mean))+\n  geom_pointrange(aes(ymin=mean-sd, ymax=mean+sd))+\n  theme_bw()\n# put this at top of script\nlibrary(kableExtra)\n\n# use kable extra functions to make a nice table (could be replaced with kable() if needed)\ndarwin_summary %>% \n    kbl(caption=\"Summary statistics of crossed and selfed maize plants\") %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = T, position = \"left\")"},{"path":"introduction-to-statistics.html","id":"estimation","chapter":"7 Introduction to statistics","heading":"7.3 Estimation","text":"section concentrated description. hypothesis Darwin aimed test whether 'inbreeding reduced fitness selfed plants'. use height plants proxy fitness explicitly address whether difference mean heights plants two groups.goal :Estimate mean heights plants two groupsEstimate mean heights plants two groupsEstimate mean difference heights two groupsEstimate mean difference heights two groupsQuantify confidence differencesQuantify confidence differences","code":""},{"path":"introduction-to-statistics.html","id":"differences-between-groups","chapter":"7 Introduction to statistics","heading":"7.3.0.1 Differences between groups","text":"Darwin's data used match pairs - pair shared one parent. pair 1 parent plant either 'selfed' 'crossed' produce offspring. powerful approach experimental design, means can look differences heights across 15 pairs plants - rather infer differences two randomly derived groups.order calculate differences height pair need data wrangling tidyr::pivot_wider() Chapter 2 calculations mutate.","code":""},{"path":"introduction-to-statistics.html","id":"activity-2-differences","chapter":"7 Introduction to statistics","heading":"7.4 Activity 2: Differences","text":"Create new column called difference height selfed plant pair subtracted crossed plant.now difference height pair, can use calculate mean difference heights paired plants, amount variance (standard deviation)\njust calculated average difference height\ngroups plants standard deviation \ndifference Moving forward working lot estimating \nconfidence differences groups\n","code":"\n# pivot data to wide format then subtract Selfed plant heights from Crossed plant heights\n\ndarwin_wide <- darwin %>% \n  pivot_wider(names_from = type, values_from = height) %>% \n  mutate(difference = Cross - Self)\ndifference_summary <- darwin_wide %>% \n  summarise(mean=mean(difference),\n            sd=sd(difference),\n            n=n())\n\ndifference_summary"},{"path":"introduction-to-statistics.html","id":"standard-error-of-the-difference","chapter":"7 Introduction to statistics","heading":"7.4.1 Standard error of the difference","text":"Remember standard deviation descriptive statistic - measures variance within dataset - e.g. closely datapoints fit mean. However inferential statistics interested confidence estimation mean. standard error comes .can think error standard deviation mean. mean calculated estimate based one sample data. expect sampled another 30 plants different sample mean. Standard error describes variability expect among sample means repeated experiment many times. can think measure confidence estimate true population mean.\\[\nSE = \\frac{s}{\\sqrt(n)}\n\\]sample size increases standard error reduce - reflecting increasing confidence estimate.can calculate standard error sample applying equation difference_summary object, can complete ?estimate mean really useful without accompanying measuring uncertainty like standard error, fact estimates averages differences always accompanied measure uncertainty.","code":"\ndifference_summary %>% \n  mutate(se= sd/sqrt(n))"},{"path":"introduction-to-statistics.html","id":"activity-3-communicate","chapter":"7 Introduction to statistics","heading":"7.5 Activity 3: Communicate","text":"information , present short sentence describing average different height?","code":""},{"path":"introduction-to-statistics.html","id":"uncertainty","chapter":"7 Introduction to statistics","heading":"7.6 Uncertainty","text":"mean standard error difference heights inbred crossed plants - work much confidence difference population means?Standard error measure uncertainty, larger standard error noise around data uncertainty . smaller standard error confidence can difference means real.Null hypothesis - difference mean height self vs crossed plantsNull hypothesis - difference mean height self vs crossed plantsAlternate hypothesis - inbreeding reduces fitness selfed plants, observed selfed plants average smaller crossed plantsAlternate hypothesis - inbreeding reduces fitness selfed plants, observed selfed plants average smaller crossed plants\nstatistical way thinking inferences terms \nconfidence around keeping rejecting null hypothesis. \n(alternate) hypothesis simply one contradicts null.\n\ndecide whether enough confidence difference \nreal (e.g. reject null hypothesis), ever 100%\ncertain isn’t false positive (also known Type error).\nlater\n","code":""},{"path":"introduction-to-statistics.html","id":"normal-distribution-1","chapter":"7 Introduction to statistics","heading":"7.6.0.1 Normal distribution","text":"remember, normal distribution bell-shaped curve. defined two parameters:meanthe meanthe standard deviationthe standard deviationThe mean determines centre/peak bell curve , standard deviation determines width bell (long tails ).Large standard deviations produce wide bell curves, short peaks. Small standard deviations produce narrow bell curves tall peaks.bell curve occurs frequently nature, circumstances can think continuous measure coming population e.g. human mass, penguin flipper lengths plant heights.probability distribution, area within curve sums whole population (e.g. probability curve contains every possible measurement 1). Known proportions curve lie within certain distances centre e.g. 67.8% observations values within one standard deviation mean. 95% observations values within two standard deviations mean. idealized normal distribution presented :convert information likely observe difference 2.62 inches plant heights 'true' difference crosses selfed plants zero?central limit theorem states population mean standard deviation, take sufficiently large random samples population, distribution sample means approximately normally distributed. Standard error measure variability around sample mean, assume can apply normal distribution ability estimate mean (revisit assumption later).now center bell curve estimate mean (2.62), just two thirds area curve ± 1.22 inches. 95% within ± 2 standard errors, 99.8% within ± 3 standard errors.Taking look figure can ask zero normal distribution? One way think , true difference plant groups zero, surprising estimated difference groups 2.62 inches?zero close center bell curve, observed mean surprising (null hypothesis true). However case middle bell. falls left-hand tail, > two standard deviations estimated mean.can describe two ways:estimate ran experiment 100 times >95 experiments estimate mean difference plant groups > 0.estimate ran experiment 100 times >95 experiments estimate mean difference plant groups > 0.also usually taken minimum threshold needed reject null hypothesis. can think probability estimating mean difference, true mean difference zero, p < 0.05.also usually taken minimum threshold needed reject null hypothesis. can think probability estimating mean difference, true mean difference zero, p < 0.05.probably used threshold null hypothesis rejection \\(\\alpha\\) = 0.05, lowest level confidence can pass statistical test. increase severity test minimum require \\(\\alpha\\) = 0.001 99% confidence, can see longer believe enough confidence reject null hypothesis (0 within 3 s.d. estimated mean).","code":"\n#Create a sequence of 100 equally spaced numbers between -4 and 4\nx <- seq(-4, 4, length=100)\n\n#create a vector of values that shows the height of the probability distribution\n#for each value in x\ny <- dnorm(x)\n\n#plot x and y as a scatterplot with connected lines (type = \"l\") and add\n#an x-axis with custom labels\nplot(x,y, type = \"l\", lwd = 2, axes = FALSE, xlab = \"\", ylab = \"\")\naxis(1, at = -3:3, labels = c(\"-3s\", \"-2s\", \"-1s\", \"mean\", \"1s\", \"2s\", \"3s\"))"},{"path":"introduction-to-statistics.html","id":"confidence-intervals","chapter":"7 Introduction to statistics","heading":"7.6.1 Confidence Intervals","text":"± 2 standard errors covers central 95% area normal curve, refer 95% confidence interval. can calculate confidence intervals level, commonly refer standard error (68% CI), 95% 99%.can work 95% confidence interval range estimated mean follows:common mistake state 95% confident 'true' mean lies within interval. technically refers fact kept running experiment intervals calculate capture true mean 95% experiments. really saying confident capture true mean 95% experiments.might write ?\nmaize plants cross pollinated taller \naverage self-pollinated plants, mean difference \nheight 2.62 [0.18, 5.06] inches (mean [95% CI]).\nNote possible generate multiple types average confidence interval, clearly state write . true presenting standard error (± S.E.) standard deviation (± S.D.) median interquartile range (median [± IQR]).good example simple clear write-clearly describes direction difference, amount error estimate.","code":"\nlowerCI <- 2.62-(2*1.22)\n\nupperCI <- 2.62+(2*1.22)\n\nlowerCI\nupperCI## [1] 0.18\n## [1] 5.06"},{"path":"introduction-to-statistics.html","id":"summary-1","chapter":"7 Introduction to statistics","heading":"7.7 Summary","text":"Statistics trying interpret whether signal (difference trend) stronger amount noise (variability). sample standard deviation strong choice estimating within dataset. standard deviation sampling distribution mean known standard error. standard error (mean) measure precision estimate mean. Thanks central limit theorem normal distribution can use variability estimate calculate confidence intervals, decide whether signal effect strong enough reject null hypothesis (effect difference).Next time start working linear models - approach allows us estimate means intervals sophisticated (automated) way.","code":""},{"path":"introduction-to-linear-models.html","id":"introduction-to-linear-models","chapter":"8 Introduction to Linear Models","heading":"8 Introduction to Linear Models","text":"last chapter conducted simple analysis Darwin's maize data using R. worked confidence intervals 'hand'. simple method allowed us learn analysis, estimates, standard error confidence. also slow, relied assumptions z-distribution assess true differences groups.now work much efficient way carry comparisons, use functions R let us perform linear model analysis.","code":""},{"path":"introduction-to-linear-models.html","id":"packages-1","chapter":"8 Introduction to Linear Models","heading":"8.0.1 Packages","text":"","code":"\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(emmeans)\nlibrary(performance)"},{"path":"introduction-to-linear-models.html","id":"a-linear-model-analysis-for-comparing-groups","chapter":"8 Introduction to Linear Models","heading":"8.1 A linear model analysis for comparing groups","text":"R general function lm() fitting linear models, part base R (require tidyverse packages). run different iterations linear model increasing complexity. often want fit several models data, common way work fit model assign named R object, can extract data need .example called model lsmodel0, short \"least-squares model 0\", linear-model uses technique called least-squares explore means later.\ncan pipe lm() function, use functions \n“outside” tidyverse family need put .\ndata go (usually first argument).\n\nlsmodel0 <- darwin %>% lm(height ~ 1, data= .)\nfirst argument lm() function formula (write full future) - specifies want analyse response variable (height) function explanatory variable using tilde symbol (~).simplest possible model ignores explanatory variables, instead 1 indicates just want estimate intercept. Without explanatory variables means formula just estimate overall mean height plants dataset.","code":"\nlsmodel0 <- lm(formula = height ~ 1, data = darwin)"},{"path":"introduction-to-linear-models.html","id":"summaries-for-models","chapter":"8 Introduction to Linear Models","heading":"8.2 Summaries for models","text":"made linear model, can investigate summary model using base R function summary(). also tidyverse option provided package broom(Robinson et al. (2023)).","code":""},{"path":"introduction-to-linear-models.html","id":"broom","chapter":"8 Introduction to Linear Models","heading":"8.2.0.1 Broom","text":"broom summarizes key information models tidy tibble()s. broom provides three verbs make convenient interact model objects:broom::tidy() summarizes information model componentsbroom::tidy() summarizes information model componentsbroom::glance() reports information entire modelbroom::glance() reports information entire modelbroom::augment() adds informations individual observations dataset can used model predictions onto new dataset.broom::augment() adds informations individual observations dataset can used model predictions onto new dataset.","code":""},{"path":"introduction-to-linear-models.html","id":"model-summary","chapter":"8 Introduction to Linear Models","heading":"8.2.0.2 Model summary","text":"output called table coefficients. 18.9 estimate model coefficient (case overall mean), together standard error. first row R model output always labelled 'Intercept' challenge usually workout represents. case can prove overall mean follows:simple model allows us understand lm() function .","code":"\nsummary(lsmodel0)## \n## Call:\n## lm(formula = height ~ 1, data = darwin)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.8833 -1.3521 -0.0083  2.4917  4.6167 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  18.8833     0.5808   32.52   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.181 on 29 degrees of freedom\nbroom::tidy(lsmodel0)\nmean(darwin$height)## [1] 18.88333"},{"path":"introduction-to-linear-models.html","id":"compare-means","chapter":"8 Introduction to Linear Models","heading":"8.2.0.3 Compare means","text":"really want linear model analyses difference average plant height function pollination type. can use lm() function fit linear model follows:Now model formula contains pollination type addition intercept.things notice :intercept value changed! represent now?intercept value changed! represent now?label second row 'typeSelf'label second row 'typeSelf'mean? Think see can figure clicking revealThe label second row 'typeSelf' produced combining variable type, one factors (Self). two levels type, stands reason intercept must represent typeCross. coefficient label intercept average height 15 maize plants crossed treatment.second row? common mistake think must refer height Self plants. However, true value negative. Instead actually refers difference mean height two groups. focuses question asking: difference height plants result cross pollination compared plants self pollinated?linear model indicates average height Crossed plants 20.2 inches, Selfed plants average 2.6 inches shorter.can confirm :take look fuller summary model, see else can determine\nFigure 1.3: Annotation summary function output\nuse information model superimpose calculated means onto plot.","code":"\nlsmodel1 <- lm(height ~ type, data=darwin)\n\n# note that the following is identical\n\n# lsmodel1 <- lm(height ~ 1 + type, data=darwin)\nbroom::tidy(lsmodel1)\ndarwin %>% \n  group_by(type) %>% \n  summarise(mean=mean(height))\nsummary(lsmodel1)## \n## Call:\n## lm(formula = height ~ type, data = darwin)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -8.1917 -1.0729  0.8042  1.9021  3.3083 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  20.1917     0.7592  26.596   <2e-16 ***\n## typeSelf     -2.6167     1.0737  -2.437   0.0214 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.94 on 28 degrees of freedom\n## Multiple R-squared:  0.175,  Adjusted R-squared:  0.1455 \n## F-statistic:  5.94 on 1 and 28 DF,  p-value: 0.02141\ndarwin %>% \n  ggplot(aes(x=type, \n             y=height,\n             colour=type))+\n  geom_jitter(alpha=0.5,\n              width=0.1)+\n  stat_summary(fun=mean,\n               size=1.2)+\n  theme_bw()"},{"path":"introduction-to-linear-models.html","id":"standard-error-of-the-difference-1","chapter":"8 Introduction to Linear Models","heading":"8.2.0.4 Standard error of the difference","text":"output model also gives standard errors values (estimates). first row intercept calculates mean standard error mean (SEM). second row gives mean difference column gives standard error difference two means (SED). already seen calculate SEM, SED?\\[\nSED = {\\sqrt \\frac{s_1^2}{n_1}}+\\frac{s_2^2}{n_2}\n\\]** Note - subscripts 1 2 indicate two groups (self cross).\nlinear model analysis doesn’t actually calculate individual\nvariances two groups. Instead uses ‘pooled’ variance\napproach. assumes variance roughly equal across groups,\nallows us take advantage larger sample size \n‘pooling’, generate accurate SED. However, assume\nvariance roughly equal two groups. fact last\nweek calculated standard deviation, saw \ncase. must check assumption model (\nlater).\n","code":""},{"path":"introduction-to-linear-models.html","id":"confidence-intervals-1","chapter":"8 Introduction to Linear Models","heading":"8.3 Confidence intervals","text":"follows layout table coefficients, output intercept row gives 95% CI height crossed plants second row gives 95% interval difference height crossed selfed plants. lower upper bounds 2.5% 97.5% t-distribution (later).difference height specifically interested. summary models generated p-values conspicuously ignored now. Instead going continue focus using confidence intervals answer question.","code":"\nconfint(lsmodel1)##                2.5 %     97.5 %\n## (Intercept) 18.63651 21.7468231\n## typeSelf    -4.81599 -0.4173433\nbroom::tidy(lsmodel1, conf.int=T)"},{"path":"introduction-to-linear-models.html","id":"answering-the-question","chapter":"8 Introduction to Linear Models","heading":"8.4 Answering the question","text":"Darwin's original hypothesis self-pollination reduce fitness (using height proxy ). null hypothesis effect pollination type, therefore difference average heights.\nmust ask experiment consistent null hypothesis can reject ? choose reject null hypothesis, level confidence can ?, can simply whether predicted value null hypothesis (difference zero) lies inside 95% CI difference mean.confidence intervals contain zero (difference), establish difference sample difference height (-2.62 inches) null prediction zero difference, given level variability (noise) data.case can see upper lower bounds confidence intervals cross zero. difference height consistent Darwin's alternate hypothesis inbreeding depression.GGally package handy ggcoef_model() function, produces graph estimated mean difference approx 95% CI. can see able reject null hypothesis 95% confidence level.\nSet confidence levels 99%, think difference \ntreatments still statistically significant 0.01?\nincrease level confidence (95% 99%, roughly 2 SE 3 SE), may find reject null hypothesis higher threshold confidence. Try altering conf.level argument see action.can also include argument tidy() function wish :","code":"\nGGally::ggcoef_model(lsmodel1,\n                     show_p_values=FALSE, \n                     conf.level=0.95)\n## \n\n#lsmodel1 %>% \n#  broom::tidy(conf.int = T) %>% \n#  filter(term != \"(Intercept)\") %>% \n#  ggplot(aes(x = estimate, y = term)) + \n#  geom_pointrange(aes(xmin = conf.low, xmax = conf.high))+\n#  coord_cartesian(xlim = c(-5,0))+ \n#  geom_vline(xintercept = 0, linetype = \"dashed\")\nbroom::tidy(lsmodel1, conf.int=T, conf.level=0.99)"},{"path":"introduction-to-linear-models.html","id":"getting-the-other-treatment-mean-and-standard-error","chapter":"8 Introduction to Linear Models","heading":"8.4.0.1 Getting the other treatment mean and standard error","text":"One limitation table coefficients output provide mean standard error treatment level (difference ). wish calculate \"\" mean SE can get R .relevelling, self treatment now taken intercept, get estimate mean standard error","code":"\ndarwin %>% \n  mutate(type=factor(type)) %>% \n  mutate(type=fct_relevel(type, c(\"Self\", \"Cross\"))) %>% \n  lm(height~type, data=.) %>% \n  broom::tidy()"},{"path":"introduction-to-linear-models.html","id":"emmeans","chapter":"8 Introduction to Linear Models","heading":"8.4.1 Emmeans","text":"also use package emmeans function emmeans() similar thingThe advantage emmeans provides mean, standard error 95% confidence interval estimates levels model (e.g. relevels model multiple times behind scenes).emmeans also gives us handy summary include data visuals combine raw data statistical inferences. standard ggplot() outputs can customised much want.Notice matter calculate estimated SE (therefore 95% CI) treatments . mentioned earlier variance pooled estimate, e.g. variance calculate separately group. difference see SE across treatments difference sample size groups.\nNotice Confidence intervals estimated means strongly\noverlap, difference two SEMs SED \ncalculated. overlapping error bars used infer\nsignificance.\npooled variance, assumption variance equal across groups, assumptions linear model checked. trust results assumptions model adequately met.","code":"\nmeans <- emmeans::emmeans(lsmodel1, specs = ~ type)\n\nmeans##  type  emmean    SE df lower.CL upper.CL\n##  Cross   20.2 0.759 28     18.6     21.7\n##  Self    17.6 0.759 28     16.0     19.1\n## \n## Confidence level used: 0.95\nmeans %>% \n  as_tibble() %>% \n  ggplot(aes(x=type, \n             y=emmean))+\n  geom_pointrange(aes(\n    ymin=lower.CL, \n    ymax=upper.CL))"},{"path":"introduction-to-linear-models.html","id":"assumption-checking","chapter":"8 Introduction to Linear Models","heading":"8.5 Assumption checking","text":"now main parts linear model analysis, results inferences. need check whether assumptions model adequately met know whether analysis can trusted.first part going check two assumptions:residual/unexplained variance data approximately normally distributed.residual/unexplained variance data approximately normally distributed.residual/unexplained variance approximately equal groupsthat residual/unexplained variance approximately equal groupsResiduals differences observed values fitted values produced model - case heights plants treatment means. assumption normal distribution applies linear model uses calculate standard errors (therefore confidence intervals). assumption equal variance applies pooled variance approach (e.g. two treatments 15 replicates - pooling variance across treatments sample size 30).Several functions exist check assumptions linear models, easiest way make graphs. can several ways, base R plot() function, using performance::check_model() function.","code":"\nperformance::check_model(lsmodel1)\nplot(lsmodel1)"},{"path":"introduction-to-linear-models.html","id":"normal-distribution-2","chapter":"8 Introduction to Linear Models","heading":"8.5.0.1 Normal distribution","text":"","code":"\nperformance::check_model(lsmodel1, check=c(\"normality\",\"qq\"))\nplot(lsmodel1, which=c(2,2))"},{"path":"introduction-to-linear-models.html","id":"what-is-a-quantile-quantile-qq-plot","chapter":"8 Introduction to Linear Models","heading":"8.5.0.2 What is a Quantile-Quantile (QQ) plot?","text":"QQ plot classic way checking whether sample distribution another (theoretical distribution). look bit odd first, actually fairly easy understand, useful! qqplot distributes data y-axis, theoretical normal distribution x-axis. residuals follow normal distribution, meet produce perfect diagonal line across plot.Watch video see QQ plots explained\nFigure 8.1: Examples qqplots different deviations normal distribution\nexample can see residuals can explained normal distribution, except extreme low end data. surprising, already identified potential outliers.fit perfect, also terrible!","code":""},{"path":"introduction-to-linear-models.html","id":"equal-variance","chapter":"8 Introduction to Linear Models","heading":"8.5.0.3 Equal variance","text":"order assess variances equal can plot residuals (variance) data fitted (predicted) values. residuals zero, mean error, data exactly matches estimates. reality, always residual error, long evenly distributed treatments ok.check_models plot provides call 'standardized residuals' divide residual error standard deviation.instance can see higher fitted values (Cross treatment) appears variable lower fitted values. , bad, perfect. probably influence least partially potential outliers.","code":"\nperformance::check_model(lsmodel1, check=\"homogeneity\")\nplot(lsmodel1, which=c(1,3))"},{"path":"introduction-to-linear-models.html","id":"outliers","chapter":"8 Introduction to Linear Models","heading":"8.5.0.4 Outliers","text":"talked lot potential effect outliers, can see potentially affecting estimates error/variance. However, also check much effect might model estimates (means). formal outlier tests useful.value data points measured called Cook's distance. measure much 'leverage' single data point exerting model, high, may outsized effect estimates. check_model() function gives contours indicate whether data points fall inside outside margins affecting fit, rough estimate acceptable Cook's Distance either \\(\\frac{4}{N}\\) \\(\\frac{4}{df}\\), unlikely calculate hand.","code":"\nperformance::check_model(lsmodel1, check=\"outliers\")\nplot(lsmodel1, which=c(4,4))"},{"path":"introduction-to-linear-models.html","id":"summary-2","chapter":"8 Introduction to Linear Models","heading":"8.6 Summary","text":"can determine analysis? model perfect, however reasonably good. However, addressed important part experimental design yet. fact plants paired, example basically carried Student's t-test, paired t-test. Later add pair another explanatory variable see affects model.remember linear model sets one factor level 'intercept' estimates mean, draws line first treatment second treatment, slope line difference means two treatments.difference means always accompanied standard error difference (SED), can used calculate 95% confidence interval. confidence interval contain intercept value, can reject null hypothesis 'effect'.Linear models make variety assumptions, including noise (residual differences) approximately normally distributed, roughly equal (homogenous) variance.","code":"\ndarwin %>% \n  ggplot(aes(x=type, \n             y=height))+\n   geom_jitter(width=0.1, \n              pch=21, \n              aes(fill=type))+\n  theme_classic()+\n  geom_segment(aes(x=1, xend=2, y=20.192, yend=20.192-2.617), linetype=\"dashed\")+\nstat_summary(fun.y=mean, geom=\"crossbar\", width=0.2)"},{"path":"testing.html","id":"testing","chapter":"9 Testing","heading":"9 Testing","text":"","code":""},{"path":"testing.html","id":"its-t-time","chapter":"9 Testing","heading":"9.1 It's t-time","text":"last chapter used linear models calculate estimates, estimates mean difference confidence intervals. can set confidence intervals whatever threshold choose - reporting without P sufficient estimation choose. interesting bit, reporting direction effect size relationship difference whatever confidence threshold want.P-values comparison, boring, -one actually cares P-values, forgiven thinking important thing statistics way often see presented. , inevitably, asked supply P-value many lab reports, dissertations (maybe future scientific papers). Luckily significance tests come parcelled coefficients linear models.","code":""},{"path":"testing.html","id":"students-t-test","chapter":"9 Testing","heading":"9.2 Student's t-test","text":"Student's t-test uses t-distribution, small-sample size version normal distribution, tails fatter degrees freedome small. two basic types t-test encountered linear models.one sample t-test: takes mean sample compares null hypothesis zeroThe two sample t-test compares difference means two samples null hypothesis difference means two populations.general equation calculating t :\\[\nt = \\frac{difference}{SE}\n\\]calculation t-value essentially counting standard errors, rough rule thumb estimate difference twice large standard error confidence interval 95%, three times large standard error confidence interval 99%.approximate becomes less robust smaller sample sizes, sample sizes large t-distribution roughly equal normal (z) distribution. However, sample sizes small t-distribution shorter wider distribution (need larger standard errors capture 95% confidence interval).potential source confusion discussing t two values must considered. critical t value must exceeded test significant (e.g. generates P value less predefined \\(\\alpha\\)). critical value t defined df. observed  value t, value returned statistical test, calculated \\(\\frac{difference}{SE}\\). \\(observed~t > critical~t\\) result can declared significantly different threshold \\(\\alpha\\).spending lot time t default value generated linear model outputs, instead assuming normal distriubtion using z. Recall z distribution critical value P = 0.05 \\(1.96 * SE\\) (roughly double Standard Error), actually look figure , can see even 8 df distribution t starting look pretty close z distribution. values critical t degree freedom 30 presented belowSo far used mixture base R summary() function broom package tibbles produced broom::tidy(). Summary common way investigate model result, specific type R object (e.g. dataframe tibble), tidying results dataframe like structure can useful.\nUsing either method can can see include t-tests coefficient, summary explicity calls t, tidy() refers generically 'statistic'linear model summary automatically apply test every row table, sometimes important apriori tests defined hypothesis, sometimes .example second row table test planned perform, tests null hypothesis comparing average observed difference plant heights cross self-pollinated plants, calculates average difference (estimate), amount uncertainty (std. error), calculates observed t value determines probability observing effect least size (sample size) null hypothesis true.However, first row also performs t-test, tests null hypothesis intercept (mean height cross-pollinated plants) zero. comparison intended make, likely test particularly useful.Anyway observed difference plant heights 2.62 inches ± 1.07, produces observed value t :value t model summary.","code":"lm(y ~ 1)\n\nlm (height~1)\nx <- seq(-4, 4, length=100)\nhx <- dnorm(x)\n\ndegf <- c(1, 3, 8, 30)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\", \"black\")\nlabels <- c(\"df=1\", \"df=3\", \"df=8\", \"df=30\", \"normal\")\n\nplot(x, hx, type=\"l\", lty=2, xlab=\"x value\",\n     ylab=\"Density\", main=\"Comparison of t Distributions\")\n\nfor (i in 1:4){\n    lines(x, dt(x,degf[i]), lwd=2, col=colors[i])\n}\n\nlegend(\"topright\", inset=.05, title=\"Distributions\",\n       labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)\nx <- seq(-4, 4, length=100)\nz_dist <- dnorm(x)\n\nvalues <- tibble(x,z_dist)\n\n# map_dfc combines values returned into a dataframe\nt <- map_dfc(degf, ~dt(x, .x))\ncolnames(t) <- degf\n\ncombined <- cbind(values,t)\n\ncombined %>% \n    pivot_longer(cols=!x, names_to=\"distribution\") %>% \n    mutate(distribution=factor(distribution, levels=c(\"z_dist\", \"1\", \"3\", \"8\", \"30\"))) %>%  \n  mutate(distribution=fct_recode(distribution, \"z distribution\" = \"z_dist\", \"df = 1\" = \"1\", \"df = 3\" = \"3\", \"df = 8\" = \"8\", \"df = 30\" = \"30\")) %>% \n  ggplot(aes(x=x, y=value, colour=distribution))+\n  geom_line(linetype=\"dashed\")+\n  theme_classic()\ndf <- c(1:30)\n\n# map_dbl forces returned values to be a single vector of numbers (rather than a list)\ncritical_t <- map_dbl(df, ~qt(p=0.05/2, df=.x, lower.tail=FALSE))\n\ntibble(df,critical_t) %>% \n  ggplot(aes(x=df, y=critical_t))+\n  geom_point()+\n  geom_line()+\n  geom_hline(aes(yintercept=1.96), linetype=\"dashed\", colour=\"red\")+\n  labs(x= \"Degrees of Freedom\",\n       y= expression(paste(\"Critical value of \", italic(\"t\"))))\nlsmodel1 <- lm(height ~ type, data = darwin)\nsummary(lsmodel1)## \n## Call:\n## lm(formula = height ~ type, data = darwin)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -8.1917 -1.0729  0.8042  1.9021  3.3083 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  20.1917     0.7592  26.596   <2e-16 ***\n## typeSelf     -2.6167     1.0737  -2.437   0.0214 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.94 on 28 degrees of freedom\n## Multiple R-squared:  0.175,  Adjusted R-squared:  0.1455 \n## F-statistic:  5.94 on 1 and 28 DF,  p-value: 0.02141\nbroom::tidy(lsmodel1)\ntidy_model1 <- broom::tidy(lsmodel1)\n\ntidy_model1[[2,2]] / tidy_model1[[2,3]]## [1] -2.437113"},{"path":"testing.html","id":"paired-t","chapter":"9 Testing","heading":"9.3 Paired t","text":"structure linear model far produced output standard two-sample Student's t-test. However, first calculated estimates hand - started making average paired differences height. generate equivalent paired t-test, simply add factor pairs linear model formula:Note made pair factor - pair 2 greater pair 1 - nott make sense treat number values.table coefficients suddenly looks lot complicated! now intercept height crossed plant pair 1:second row now compares mean heights Crossed Selfed plants pairThe second row now compares mean heights Crossed Selfed plants pairrows three 16 compare average difference pair (Crossed Selfed combined) pair 1rows three 16 compare average difference pair (Crossed Selfed combined) pair 1Again linear model computes every possible combination t-statistic P-value, however one care difference Cross Self-pollinated plant heights. ignore pair comparisons second row gives us paired t-test. 'difference height Cross Self-pollinated plants hold pairs constant.'completeness generate confidence intervals paired t-test.can see estimate mean difference identical 95% confidence intervals now slightly different. particular version actually increased level uncertainty including pair parameter.\nChoosing right model\n\nfuture sessions work model building \nsimplification, case good priori reason \ninclude pair initial model, simple tests can \nsee safe remove , doesn’t appear adding \nexplanation difference heights self \ncross-fertilised plants.\n","code":"\nlsmodel_darwin <- lm(height ~ type + factor(pair), data = darwin)\nsummary(lsmodel_darwin)## \n## Call:\n## lm(formula = height ~ type + factor(pair), data = darwin)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.4958 -0.9021  0.0000  0.9021  5.4958 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     21.7458     2.4364   8.925 3.75e-07 ***\n## typeSelf        -2.6167     1.2182  -2.148   0.0497 *  \n## factor(pair)2   -4.2500     3.3362  -1.274   0.2234    \n## factor(pair)3    0.0625     3.3362   0.019   0.9853    \n## factor(pair)4    0.5625     3.3362   0.169   0.8685    \n## factor(pair)5   -1.6875     3.3362  -0.506   0.6209    \n## factor(pair)6   -0.3750     3.3362  -0.112   0.9121    \n## factor(pair)7   -0.0625     3.3362  -0.019   0.9853    \n## factor(pair)8   -2.6250     3.3362  -0.787   0.4445    \n## factor(pair)9   -3.0625     3.3362  -0.918   0.3742    \n## factor(pair)10  -0.6250     3.3362  -0.187   0.8541    \n## factor(pair)11  -0.6875     3.3362  -0.206   0.8397    \n## factor(pair)12  -0.9375     3.3362  -0.281   0.7828    \n## factor(pair)13  -3.0000     3.3362  -0.899   0.3837    \n## factor(pair)14  -1.1875     3.3362  -0.356   0.7272    \n## factor(pair)15  -5.4375     3.3362  -1.630   0.1254    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.336 on 14 degrees of freedom\n## Multiple R-squared:  0.469,  Adjusted R-squared:  -0.09997 \n## F-statistic: 0.8243 on 15 and 14 DF,  p-value: 0.6434\ndarwin %>% \n  mutate(pair = as_factor(pair)) %>% \n  lm(height ~ type + pair, data = .) %>% \n  broom::tidy()\nlm(height ~ type + factor(pair), data = darwin) %>% \n  broom::tidy(., conf.int=T) %>% \n  slice(1:2) # just show first two rows\nm1 <- lm(height ~ type, data = darwin) %>% \n  broom::tidy(., conf.int=T) %>% \n  slice(2:2) %>% \n  mutate(model=\"unpaired\")\n\nm2 <- lm(height ~ type + factor(pair), data = darwin) %>% \n  broom::tidy(., conf.int=T) %>% \n  slice(2:2) %>% \n  mutate(model=\"paired\")\n\nrbind(m1,m2) %>% \n  ggplot(aes(model, estimate))+\n  geom_pointrange(aes(ymin=conf.high, ymax=conf.low))+\n  geom_hline(aes(yintercept=0), linetype=\"dashed\")+\n  theme_minimal()+\n  coord_flip()"},{"path":"testing.html","id":"effect-sizes","chapter":"9 Testing","heading":"9.4 Effect sizes","text":"discussed importance using confidence intervals talk effect sizes. 95% confidence intervals overlap intercept, indicates difference means significant \\(\\alpha\\) = 0.05. interestingly allows us talk 'amount difference' treatments, lower margin confidence intervals smallest/minimum effect size. response scale variables useful, can report example least 0.43 inch height difference self crossed fertilised plants \\(\\alpha\\) = 0.05.","code":""},{"path":"testing.html","id":"type-1-and-type-2-errors","chapter":"9 Testing","heading":"9.5 Type 1 and Type 2 errors","text":"repeatability results key part scientific method. Unfortunately often emphasis literature 'novel findings', means unusual/interesting results happen reach statistical significance may likely published. reality know set \\(\\alpha\\) = 0.05, run risk rejecting null hypothesis incorrectly 1 20 experiments (Type 1 error).Type 2 errors. Statistical tests provide probability making Type 1 error (rejecting null hypothesis incorrectly) form P. Type 2 errors? Keeping null hypothesis, rejecting ? finding effect.probability making Type 2 error known \\(1-\\beta\\), \\(\\beta\\) refers statistical 'power'. Working statistical power straightforward simple tests, becomes rapidly diffcult complexity analysis increases... important concept understand.side coin experimental power - strength experiment detect statistical effect one. Power expressed 1-\\(\\beta\\). want beta error typically less 20%. , want power 80%. 80% chance finding effect .\nexperiments/statistical analyses become statistically\nsignificant make sample size large enough. \nrespect shows misleading significant result can . \ninteresting result statistically significant, \neffect size tiny.\n","code":""},{"path":"testing.html","id":"repeatability","chapter":"9 Testing","heading":"9.6 Repeatability","text":"possible know single experiment whether made Type 1 Type 2 errors. However, time experiments eventually repeated literature builds allowing us synthesise evidence. try now & imagine scenario Darwin's experiment repeated another 20 times.example made loop assumes 'know' true mean difference crossed fertilised plants standard deviation 'population'(taken Darwin's experimental data ). loop creates 20 new sampling experiments, calculates estimated mean difference experiment","code":"\nset.seed(1234)\n\nmyList <- vector(\"list\", 20)\ny <- tibble()\n\nfor (i in 1:length(myList)) { \n\nx <-  rnorm(n=12, mean=2.6, sd=2.83)\ndata <- tibble(x)\ntemp <- lm(x~1, data=data) %>% \n  broom::tidy(conf.int=T) \ny <- rbind(y,temp)  \n\n}\n\ny$`experiment number` <- rep(1:20)\n\n# the new dataframe y contains the results of 20 new experiments"},{"path":"testing.html","id":"activity-1-experimental-repeatability","chapter":"9 Testing","heading":"9.7 Activity 1: Experimental Repeatability","text":"example nearly third experiments find statistically significant difference. less formal review research might tally P-values conclude inconsistent results literature.better way look estimates calculated confidence intervalsBy illustrating visually, clearer see results really inconsistent, negative effects inbreeding depression clear see experiments - simply observing effect sampling error.20 studies showed effect inbreeding depression, experiments identical levels uncertainty. can clearly see estimates intervals substantial improvement way report experiments, make comparisons across repeated studies valuable.","code":"\ny %>% \n  mutate(`p value < 0.05` = if_else(p.value > 0.049, \"non-significant\", \"significant\")) %>% \n  group_by(`p value < 0.05`) %>% \n  summarise(`number of experiments`=n())\ny %>% \n  ggplot(aes(x=`experiment number`, y=estimate))+\n  geom_pointrange(aes(ymin = conf.low, ymax=conf.high))+\n  labs(y = \"Estimated mean effect of outcrossing\")+\n  geom_hline(linetype=\"dashed\", yintercept=0.05)+\n  theme_minimal()"},{"path":"testing.html","id":"summary-3","chapter":"9 Testing","heading":"9.8 Summary","text":"chapter finally allowed us calculate P-values test statistical significance experiments using linear models. also compared linear model structures producing paired vs. unpaired t-test.However also learned appreciate potential issues around making Type 1 Type 2 errors, appreciation confidence intervals standardised effect sizes can used assess .single experiment never definitive, reliance reporting P-values uninformative can misleading. Instead reporting estimates confidence intervals allows us report levels uncertainty, provides results informative comparitive studies.","code":""},{"path":"regression.html","id":"regression","chapter":"10 Regression","heading":"10 Regression","text":"","code":""},{"path":"regression.html","id":"introduction-to-regression","chapter":"10 Regression","heading":"10.1 Introduction to Regression","text":"far used linear models analyses two 'categorical' explanatory variables e.g. t-tests. 'continuous' explanatory variable? need use regression analysis, luckily just another 'special case' linear model, can use lm() function already using, can interpret outputs way.","code":"\nlibrary(tidyverse)\nlibrary(rstatix)\nlibrary(performance)"},{"path":"regression.html","id":"linear-regression","chapter":"10 Regression","heading":"10.2 Linear regression","text":"Much like t-test generating linear model, regression analysis interpreting strength 'signal' (change mean values according explanatory variable), vs amount 'noise' (variance around mean).normally visualise regression analysis scatter plot, explanatory (predictor, independent) variable x-axis response (dependent) variable y-axis. Individual data points plotted, attempt draw straight-line relationship throught cloud data points. line 'mean', variability around mean captured calculated standard errors confidence intervals variance.equation linear regression model :\\[ y = + bx \\]\nmay also note basically identical equation straight fit line \\(y = mx +c\\).:y predicted value response variabley predicted value response variablea regression intercept (value y x = 0)regression intercept (value y x = 0)b slope regression lineb slope regression linex value explanatory variablex value explanatory variableThis formula explains mean, need include unexplained residual error term include measure uncertainty\\[ y = + bx + e \\]regression uses two values fit straight line. First need starting point, known regression intercept. categorical predictors mean value y one categories, regression mean value y x = 0. need gradient (value y changes value x changes). allows us draw regression line.linear model analysis estimates values intercept gradient order predict values y given values x.","code":""},{"path":"regression.html","id":"data","chapter":"10 Regression","heading":"10.3 Data","text":"going use example data Australian forestry industry, recording density hardness 36 samples wood different tree species. Wood density fundamental property relatively easy measure, timber hardness, quantified 'amount force required embed 0.444\" steel ball wood half diameter'.regression, can test biological hypothesis wood density can used predict timber hardness, use regression predict timber hardness new samples known density.Timber hardness quantified using 'Janka scale', data going use today comes originally R package SemiPar\nCheck data imported correctly make sure ‘tidy’ \nobvious errors missing data\n","code":"\njanka <- read_csv(\"data/janka.csv\")"},{"path":"regression.html","id":"activity-1-exploratory-analysis","chapter":"10 Regression","heading":"10.4 Activity 1: Exploratory Analysis","text":"Wood density timber hardness appear positively related, relationship appears fairly linear. can look simple strength association dens hardness using correlation","code":"\njanka %>% \n  ggplot(aes(x=dens, y=hardness))+\n  geom_point()"},{"path":"regression.html","id":"activity-2-correlation---generate-pearsons-r","chapter":"10 Regression","heading":"10.5 Activity 2: Correlation - Generate Pearson's R","text":"Can work code needed generate Pearson's R? - Try using google search, check code answer solution.Hint try rstatix package?Correlation coefficients range -1 1 perfectly negative perfectly positive linear relationships. relationship appears strongly positive. Correlation looks association two variables, want go - arguing wood density causes higher values timber hardness. order test hypothesis need go correlation use regression.","code":"\n# cor() does not have a data option so need to use the with() function\nwith(janka, cor(dens, hardness))## [1] 0.9743345\nlibrary(rstatix)\n\njanka %>% \n  cor_test(dens, hardness)"},{"path":"regression.html","id":"regression-in-r","chapter":"10 Regression","heading":"10.6 Regression in R","text":"can fit regression model exactly way fit linear model Darwin's maize data. difference predictor variable continuous rather categorical.\ncareful ordering variables :\n\n\nleft ‘tilde’ response variable,\n\n\nleft ‘tilde’ response variable,\n\n\nright predictor.\n\n\nright predictor.\n\nGet wrong way round reverse hypothesis.\nlinear model estimate 'line best fit' using method 'least squares' minimise error sums squares (average distance data points regression line).can add regression line ggplots easily function geom_smooth().Q. blue line represents regression line, shaded interval 95% confidence interval band. notice width interval band move along regression line?95% confidence interval band narrowest middle widest either end regression line. ?performing linear regression, two types uncertainty prediction.First prediction overall mean estimate (ie center fit). second uncertainly estimate calculating slope.combine uncertainties prediction spread high low estimates. away center data get (either direction), uncertainty slope becomes large noticeable factor, thus limits widen.","code":"\njanka_ls1 <- lm(hardness ~ dens, data = janka) \n# specify linear model method for line fitting\n\njanka %>% \n  ggplot(aes(x=dens, y=hardness))+\n  geom_point()+\n  geom_smooth(method=\"lm\")"},{"path":"regression.html","id":"summary-4","chapter":"10 Regression","heading":"10.6.0.1 Summary","text":"output look familiar , output produced analysis maize data. Including column coefficient estimates, standard error, t-statistic P-value. first row intercept, second row difference mean intercept caused explanantory variable.\nmany ways intercept makes intuitive sense regression\nmodel difference model. intercept describes value \ny (timber hardness) x (wood density) = 0. \nstandard error standard error calculated mean value. \nwrinkle value y impossible value -\ntimber hardness obviously negative value (anti-hardness???).\naffect fit line, just means regression\nline (infinite straight line) can move impossible value\nranges.\n\nOne way intercept can made valuable use \ntechnique known ‘centering’. subtracting average (mean) value\nx every data point, intercept (x \n0) can effectively right-shifted centre data.\n","code":"\nsummary(janka_ls1)## \n## Call:\n## lm(formula = hardness ~ dens, data = janka)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -338.40  -96.98  -15.71   92.71  625.06 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -1160.500    108.580  -10.69 2.07e-12 ***\n## dens           57.507      2.279   25.24  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 183.1 on 34 degrees of freedom\n## Multiple R-squared:  0.9493, Adjusted R-squared:  0.9478 \n## F-statistic:   637 on 1 and 34 DF,  p-value: < 2.2e-16\njanka_ls1 %>% \n  broom::tidy()"},{"path":"regression.html","id":"activity-3-mean-centered-regression","chapter":"10 Regression","heading":"10.7 Activity 3: Mean centered regression","text":"\nNote estimate row 2 - effect density timber\nhardness changed, intercept now\nrepresents estimated mean timber hardness mean wood density\ne.g. density \\(\\rho\\) = 45.73\naverage timber hardness janka scale 1469.\n","code":"\ndens_mean <- janka %>% \n  summarise(mean_dens=mean(dens))\n# 45.73333\n\njanka %>% \n  mutate(centered_dens = dens-pull(dens_mean)) %>% \n  lm(hardness ~ centered_dens, data = .) %>% \n  broom::tidy()"},{"path":"regression.html","id":"the-second-row","chapter":"10 Regression","heading":"10.7.0.1 the second row","text":"second row labelled 'dens'. Density explanatory variable, slope estimated . 57.5 value regression slope (standard error) - timber hardness predicted increase 57.5 janka scale every unit change density.According model summary, estimated change mean statistically significant - effect size sample size unlikely observe relationship null hypothesis (predict timber hardness wood density) true.","code":""},{"path":"regression.html","id":"confidence-intervals-2","chapter":"10 Regression","heading":"10.7.0.2 Confidence intervals","text":"Just like maize data, can produce upper lower bounds confidence intervals:52.962.157.52.28","code":"\nconfint(janka_ls1)##                   2.5 %     97.5 %\n## (Intercept) -1381.16001 -939.83940\n## dens           52.87614   62.13721\nbroom::tidy(janka_ls1, conf.int=T, conf.level=0.95)"},{"path":"regression.html","id":"effect-size","chapter":"10 Regression","heading":"10.7.0.3 Effect size","text":"regression model, can also produce standardised effect size. estimate 95% confidence intervals amount change observed, just like maize data can produce standardised measure strong relationship . value represented \\(R^2\\) : proportion variation data explained linear regression analysis.value \\(R^2\\) can found model summaries follows\nTable 10.1: R squared effect size\n","code":"\nsummary(janka_ls1)## \n## Call:\n## lm(formula = hardness ~ dens, data = janka)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -338.40  -96.98  -15.71   92.71  625.06 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -1160.500    108.580  -10.69 2.07e-12 ***\n## dens           57.507      2.279   25.24  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 183.1 on 34 degrees of freedom\n## Multiple R-squared:  0.9493, Adjusted R-squared:  0.9478 \n## F-statistic:   637 on 1 and 34 DF,  p-value: < 2.2e-16\njanka_ls1 %>% \n  broom::glance()"},{"path":"regression.html","id":"assumptions","chapter":"10 Regression","heading":"10.8 Assumptions","text":"Regression models make assumptions linear models - unexplained variation around regression line (residuals) approximately normally distributed, constant variance. can check way.Remember, residuals difference observed values fitted values predicted model. words vertical distance data point fitted value regression line. can take look another function broom package augment(). generates predicted value data point according regression, calculates residuals data point.plot , black fitted regression line red dashed lines representing residuals:can use augmented data really help us understand residual variance looks like, can used diagnose models. perfect model mean residual values = 0, incredibly unlikely ever occur. Instead like seethat 'normal distribution' residuals e.g. residuals close mean, fewer away rough z-distribution.'normal distribution' residuals e.g. residuals close mean, fewer away rough z-distribution.also want see homogeneity residuals e.g. bad model average error greater one end model . might mean uncertainty slope line large values small values vice versa.also want see homogeneity residuals e.g. bad model average error greater one end model . might mean uncertainty slope line large values small values vice versa.example functional, repetitive code - make function reduces amount code needed?example function want make:","code":"\npredict(janka_ls1)\n\nresid(janka_ls1)##         1         2         3         4         5         6         7         8 \n##  259.9152  265.6658  409.4325  472.6899  472.6899  507.1939  581.9525  719.9686 \n##         9        10        11        12        13        14        15        16 \n##  886.7379 1053.5073 1070.7593 1099.5126 1105.2633 1134.0166 1157.0193 1174.2713 \n##        17        18        19        20        21        22        23        24 \n## 1180.0220 1180.0220 1306.5366 1473.3060 1536.5633 1611.3220 1801.0940 1801.0940 \n##        25        26        27        28        29        30        31        32 \n## 1910.3567 2059.8741 2088.6274 2134.6328 2151.8848 2243.8954 2278.3994 2634.9408 \n##        33        34        35        36 \n## 2715.4502 2795.9595 2813.2115 2813.2115 \n##            1            2            3            4            5            6 \n##  224.0848370  161.3341695    3.5674826   44.3101404   76.3101404  140.8061355 \n##            7            8            9           10           11           12 \n##    5.0474583  -15.9685611   92.2620821 -139.5072748   -0.7592772  -79.5126146 \n##           13           14           15           16           17           18 \n##  104.7367180 -145.0166194    2.9807107 -164.2712918  -80.0219592  -50.0219592 \n##           19           20           21           22           23           24 \n##  -36.5366437 -293.3060005 -136.5633428  148.6779800  -91.0940467  208.9059533 \n##           25           26           27           28           29           30 \n##  -30.3567287  -79.8740831 -268.6274205 -114.6327603 -171.8847628   66.1045576 \n##           31           32           33           34           35           36 \n## -338.3994472  625.0591692  -15.4501754   94.0404799  -73.2115225  326.7884775\njanka_ls1 %>% \n  broom::augment() %>% \n  head()\naugmented_ls1 <- janka_ls1 %>% \n  broom::augment()\n\naugmented_ls1 %>% \n    ggplot(aes(x=dens, \n               y=.fitted))+\n    geom_line()+ \n  geom_point(aes(x=dens, \n                 y=hardness))+\n  geom_segment(aes(x=dens, \n                   xend=dens, \n                   y=.fitted, \n                   yend=hardness), \n               linetype=\"dashed\", colour=\"red\")\n# A line connecting all the data points in order \np1 <- augmented_ls1 %>% \n  ggplot(aes(x=dens, y=hardness))+\n  geom_line()+\n  ggtitle(\"Full Data\")\n\n# Plotting the fitted values against the independent e.g. our regression line\np2 <- augmented_ls1 %>% \n  ggplot(aes(x=dens, y=.fitted))+\n  geom_line()+\n  ggtitle(\"Linear trend\")\n\n# Plotting the residuals against the fitted values e.g. remaining variance\np3 <- augmented_ls1 %>% \n  ggplot(aes(x=.fitted, y=.resid))+\n  geom_hline(yintercept=0, colour=\"white\", size=5)+\n  geom_line()+\n  ggtitle(\"Remaining \\npattern\")\n\n\nlibrary(patchwork)\np1+p2+p3model_plot(data=augmented_ls1, \n            x=\"dens\", \n            y=\"hardness\", \n            title=\"Full data\")\nmodel_plot <- function(data=augmented_ls1, \n                       x=\"dens\", \n                       y=\"hardness\", \n                       title=\"Full data\"){\n  ggplot(aes(x=.data[[x]], \n             y=.data[[y]]), \n         data=data)+\n  geom_line()+\n    theme_bw()+\n      ggtitle(title)\n}\n\np1 <- model_plot()\np2 <- model_plot(y=\".fitted\", title=\"Linear prediction\")\np3 <- model_plot(y=\".resid\", title=\"Remaining pattern\")"},{"path":"regression.html","id":"normal-distribution-3","chapter":"10 Regression","heading":"10.8.0.1 Normal distribution","text":"can use model diagnostic plots used maize data.\ncan see mostly pretty good, just one two data points outside confidence intervals","code":"\nplot(janka_ls1, which=c(2,2))\nperformance::check_model(janka_ls1, check=c(\"normality\",\"qq\"))"},{"path":"regression.html","id":"equal-variance-1","chapter":"10 Regression","heading":"10.8.0.2 Equal variance","text":"can use model diagnostic plots used maize data.\nsee similar p3 plot constructed manually. plot constructed earlier 'raw' residuals function fitted values. plot produced now 'standardized residuals' - raw residual divided standard deviation.plots suggests residuals constant variance, broadly speaking amount variance y increases x increases. means less confidence predictions high values density. Later see can improve fit model","code":"\nplot(janka_ls1, which=c(1,3))\nperformance::check_model(janka_ls1, check=\"homogeneity\")"},{"path":"regression.html","id":"outliers-1","chapter":"10 Regression","heading":"10.8.0.3 Outliers","text":"can see just one potential outlier.positional order dataframe? Check data, make sense?","code":"\nplot(janka_ls1, which=c(4,5))\nperformance::check_model(janka_ls1, check=\"outliers\")"},{"path":"regression.html","id":"prediction","chapter":"10 Regression","heading":"10.9 Prediction","text":"Using coefficients intercept slope can make predictions new data.\nestimates intercept slope :Now imagine new wood samples density 65, can use equation linear regression predict timber hardness wood sample ?\\[ y = + bx \\]Rather work values manually, can also use coefficients model directlyBut time unlikely want work predicted values hand, instead can use functions like predict() broom::augment()","code":"\ncoef(janka_ls1)## (Intercept)        dens \n## -1160.49970    57.50667\n# a + bx\n\n-1160.49970 + 57.50667 * 65## [1] 2577.434\ncoef(janka_ls1)[1] + coef(janka_ls1)[2] * 65## (Intercept) \n##    2577.434\npredict(janka_ls1, newdata=list(dens=c(22,35,65)))##         1         2         3 \n##  104.6471  852.2339 2577.4342\nbroom::augment(janka_ls1, \n               newdata=tibble(dens=c(22,35,65)))"},{"path":"regression.html","id":"adding-confidence-intervals","chapter":"10 Regression","heading":"10.9.1 Adding confidence intervals","text":"","code":""},{"path":"regression.html","id":"standard-error","chapter":"10 Regression","heading":"10.9.1.1 Standard error","text":"","code":"\nbroom::augment(janka_ls1, newdata = tibble(dens=c(22,35,65)), se=TRUE)"},{"path":"regression.html","id":"confidence-intervals-3","chapter":"10 Regression","heading":"10.9.1.2 95% Confidence Intervals","text":"really like emmeans package - good producing quick predictions categorical data - can also continuous variables. default produce single mean-centered prediction. list can provided - produce confidence intervals standard.","code":"\nbroom::augment(janka_ls1, newdata=tibble(dens=c(22,35,65)), interval=\"confidence\")\nemmeans::emmeans(janka_ls1, \n                 specs = \"dens\", \n                 at = list(dens = c(22, 35, 65)))##  dens emmean   SE df lower.CL upper.CL\n##    22    105 62.1 34    -21.5      231\n##    35    852 39.1 34    772.8      932\n##    65   2577 53.5 34   2468.8     2686\n## \n## Confidence level used: 0.95"},{"path":"regression.html","id":"activity-4-prediction","chapter":"10 Regression","heading":"10.10 Activity 4: Prediction","text":"Hint - first make new R object contains predictions, work add two dataframes one plot.","code":"\npred_newdata <- broom::augment(janka_ls1, \n               newdata=tibble(dens=c(22,35,65)))\n\njanka %>% \n  ggplot(aes(x=dens, y=hardness))+\n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  geom_point(data=pred_newdata, aes(y=.fitted, x=dens), colour=\"red\")+\n  geom_label(data=pred_newdata, (aes(y=(.fitted+10), x=(dens+3), label=round(.fitted, digits=0))))+\n  theme_bw()+\n  labs(x=\"Density\", y=\"Timber Hardness\")+\n  scale_x_continuous(limits=c(20,80), expand=expansion(add=c(0,5)))"},{"path":"regression.html","id":"summary-5","chapter":"10 Regression","heading":"10.11 Summary","text":"Linear model analyses can extend beyond testing differences means categorical groupings test relationships continuous variables. known linear regression, relationship explanatory variable response variable modelled equation straight line. intercept value y x = 0, often useful, can use 'mean-centered' values wish make intercept intuitive.\nlinear models, regression assumes unexplained variability around regression line, normally distributed constant variance.regression fitted possible predict values y values x, uncertainty around predictions can captured confidence intervals.","code":""},{"path":"anova.html","id":"anova","chapter":"11 ANOVA","heading":"11 ANOVA","text":"","code":""},{"path":"anova.html","id":"analysis-of-variance-anova","chapter":"11 ANOVA","heading":"11.1 Analysis of Variance (ANOVA)","text":"far used linear models analyses two 'categorical' explanatory variables e.g. t-tests, regression two 'continuous' variables. However, designs become complicated, number comparisons can quickly become overwhelming, working estimates intervals alone can become harder.designs become elaborate number pairwise t-tests rapidly increases, therefore risk false positives (Type errors). therefore useful complementary approach first asks support difference different means diving multiple comparisons. approach called analysis variance (ANOVA), although tends associated categorical data, see following chapters ANOVA just another type linear model, approach can also extended included continuous variables.","code":""},{"path":"anova.html","id":"maize-data","chapter":"11 ANOVA","heading":"11.2 Maize data","text":"simple linear model maize data :structure fitting linear model, one explanatory variable also known one-way ANOVA. general strategy ANOVA quantify overall variability data set divide variability within groups. can also refer 'signal--noise' ratio, variability explained slope linear model, vs unexplained variance.variation explained fitted linear model, confident can detected real effect estimates mean differences. method fitting model called ordinary least squares.least squares model first quantifies total amount variation (total sum squares, SST) measuring difference individual data points reference point (usually 'grand' mean, see ).Next model quantifies fitted slope, aims produce slope produces least amount squared residuals data points (sum squares regression/ANOVA, SSR/).leaves residual unexplained variation (sum squares error, SSE).way model splitting overall variability (SST) signal (SSR) noise (SSE):\\[ SST = SSR + SSE \\]Remember:SST = sum squared differences data points grand meanSST = sum squared differences data points grand meanSSR = sum squared differences grand mean predicted position linear modelSSR = sum squared differences grand mean predicted position linear modelSSE = sum squared differences predicted position observed positionSSE = sum squared differences predicted position observed position\nLeast squares quantifies ) total least squares, B) treatment least\nsquares, C) error sum squares (SST, SSR, SSE). vertical lines\nmeasure distances, squared summed. SST \ncalculated measuring intercept, SSR calculated measuring\ndistance estimates intercept, SSE calculated \nmeasuring distance estimates\n","code":"lsmodel1 <- lm(height ~ type, data = darwin)"},{"path":"anova.html","id":"the-anova-table","chapter":"11 ANOVA","heading":"11.2.1 The ANOVA table","text":"want get ANOVA table linear model can use anova() function:Compare output get use summary() function. see F-value P-value df exactly . ANOVA table actually far less information , contains information biology (estimates difference, uncertainty standard errors etc.). amazing many times see tables reported, even though contain least interesting part analysis.can help us simplify effects two levels, need learn use appropriately.ANOVA table six columns two rows information.first column contains information source, signal/regression (first row) error/noise (second row).second column gives degrees freedom, first row number predictors(treatments) (k - 1, including intercept), second row residual degrees freedom (N - k).third column sum squares first row SSR, second row SSE (remember SST = SSR + SSE).fourth column mean sum squares MSR = SSR/(k-1) & MSE = SSE/(N-k). average amounts variability per treatment level.worth stopping briefly point remembering effectively pooled SSE treatments generate MSE. Linear models always use pooled variance approach, can give greater power detecting effects smaller sample sizes, reason need homogeneity variance assumption model.fifth column F statistic (signal--noise ratio). calculated dividing treatment variance residual error variance\\[ F =\\frac{SSR/(k-1)}{SSE/(N-k)} =  \\frac{MSR}{MSE} \\]\nexample F = 5.9, means estimated signal nearly six times larger estimated noise.can use F value sample size treatment levels calculate probability observing ratio signal noise null hypothesis effect treatment true.probability value can looked ANOVA table, calculated F distribution. F-test takes sample size account, probability assigned takes account degrees freedom signal noise.can see use R function pf() recreate exact P-value see ANOVA table.first three arguments F-value, degrees freedom signal, noise. last argument set two directional test.common see P-values reported supporting information (naked P-values). impossible interpret without knowing:test came fromwhat test came fromwhat observed value test waswhat observed value test wasthe degrees freedom.degrees freedom.good (conventional) way report result ANOVA :height cross-pollinated plants significantly taller height self-pollinated plants (F1,28 = 5.9, P = 0.02).know way reporting, emphasises statistical tests underlying biology. report tell us anything heights plants, estimated difference , measure uncertainty around .self pollinated maize plants measured average 17.6 [16-19.1] (mean[95% CI]) inches high, cross-pollinated plants mean height 20.2 [18.6-21.7] inches - difference 2.6 [-0.4-4.8] inches (one-way ANOVA: F1,28 = 5.9, P = 0.02).","code":"\nanova(lsmodel1)\npf(5.9395, 1, 28, lower.tail=FALSE)## [1] 0.02141466"},{"path":"anova.html","id":"two-way-anova","chapter":"11 ANOVA","heading":"11.3 Two-way ANOVA","text":"Two way ANOVA (might guess) includes two explanatory variables. Usually treatments interest, example stick including pair variable.can look table using anova() function againNote degrees freedom initially residuals, now 'used' new term complex model. SSE now used SSR explained pair term.\nalready able predict last time\nlooked model structure, pairing doesn’t really \nanything ( F< 1). pairing ‘nothing’ \nstill expect F ratio ~ 1. However, can see \nactually quite bit lower , implies ‘negative\nvariance component’ - actually increased relative\nproportion SSE compared SSR .\n\nmean squares regression smaller residuals \nimplies problem experimental design. result \nundersampling (pairs needed), however also \npairs sampled random.\n","code":"\nlsmodel2 <- lm(height ~ type + as.factor(pair), data = darwin)\nanova(lsmodel2)"},{"path":"anova.html","id":"summary-6","chapter":"11 ANOVA","heading":"11.4 Summary","text":"ANOVA tables can built linear model. tables partition variance signal(s) noise, can compared using F-test. complex analyses many pairwise comparisons performed, initial F-test can provide initial evidence whether differences , reducing risk overtesting false positives can generated.","code":""},{"path":"anova.html","id":"activity","chapter":"11 ANOVA","heading":"11.5 Activity","text":"Write short R based report (Rmd) using ANOVA test experimental hypothesis:Set new R project analysis - get talking GithubSet new R project analysis - get talking GithubImport, clean analyse dataImport, clean analyse dataTest experimental hypothesisTest experimental hypothesisProduce data visual summaryProduce data visual summaryProduce short write-upProduce short write-upNow time try put analysis skills action. instructions importing data setting new project answer question, temperature affect frogspawn development? chance practice:Q. frogspawn hatching time vary temperature?Imagine ran manipulative experiment.manipulative study one experimenter changes something experimental study system studies effect change.collected newly-layed frogspawn pond Italian Alps brought back lab, divided 60 water containers. 20 containers’ water temperature kept 13°C, 20 containers kept 18°C remaining 20 containers kept 25°C. high number replicates increases confidence expected difference groups due factor interested . , temperature.monitored water container recorded hatching times (days hatching eggs) spreadsheet (called frogs_messy_data.csv).response variable Hatching_time.response variable Hatching_time.explanatory variable Temperature, 3 levels: 13°C, 18°C 25°C.explanatory variable Temperature, 3 levels: 13°C, 18°C 25°C.want compare means 3 independent groups (13°C, 18°C 25°C temperature groups) one continuous response variable (Hatching time days) one categorical explanatory variable (Temperature). One-way ANOVA appropriate analysis!","code":""},{"path":"anova.html","id":"hypothesis","chapter":"11 ANOVA","heading":"11.5.1 Hypothesis","text":"Always make hypothesis prediction, delve data analysis.hypothesis tentative answer well-framed question, referring mechanistic explanation expected pattern. can verified via predictions, can tested making additional observations performing experiments.backed level knowledge study system.case, knowing frogspawn takes around 2-3 weeks hatch optimal temperatures (15-20°C), can hypothesize lower temperature, longer take frogspawn hatch. hypothesis can therefore : mean frogspawn hatching time vary temperature level. can predict given temperature range, highest temperature (25°C) hatching time reduced.setting R project, import tidy dataset.Hint: check data tidy format.multiple ways present data, equally valid, emphasising different concepts. example first figure uses boxplot data points illustrate differences. Remember median IQR essentially descriptive statistics, inferential.\nFigure 11.1: Frogspawn hatching times 13, 18 25 degrees Celsius. Boxplot displays median, hinges first third quartiles, whiskers extend hinge 1.5X interquartile range. Points represent individual frogspawns.\nWhereas next figure, uses emmeans() package produce estimate means confidence intervals lm() therefore produce inferential statistics, figure illustrates estimates model rather parameters sample.Neither method 'best'.\nFigure 5.4: Time hatching inversely related temperature frogspawn. Circles represent estimated mean hatching times 95% confidence intervals one-way ANOVA (F1,28 = 385.9, P < 0.001). Dashed lines indicate slope mean difference 13-18 degrees 13-25 degrees Celsius. Faded points represent individual data points.\nIncreasing temperatures clear effect reducing time taken frogspawn hatch (one-way ANOVA: F2,57 = 385.9, P < 0.001). 13\\(^\\circ\\) C mean time hatching 26.3 days [25.8-26.8 95% CI], reduced average 5.3 days [4.57 - 6.02] 18\\(^\\circ\\) C 10.1 days [9.37 - 10.82] 25\\(^\\circ\\) C.","code":"\n#___________________________----\n\n# SET UP ----\n## An analysis of the development time of frogspawn in response to water temperature ----\n#___________________________----\n\n# PACKAGES ----\nlibrary(tidyverse)\n#___________________________----\n\n# IMPORT DATA ----\nfrogs <- read_csv(\"data/frogs_messy_data.csv\")\n#___________________________----\n\n# TIDY DATA ----\nfrogs <- frogs %>% \n  rename(\"13\" = Temperature13,\n         \"18\" = Temperature18,\n         \"25\" = Temperature25,\n         frogspawn_id = `Frogspawn sample id`) %>% \n  pivot_longer(`13`:`25`, names_to=\"temperature\", values_to=\"days\") %>% \n  drop_na(days)\n#___________________________----\n# ANALYSIS ----\nlsmodel_frogs1 <- lm(days ~ temperature, data = frogs)\n\n# summary(lsmodel_frogs1)\n\n# anova(lsmodel_frogs1)\n\nbroom::tidy(lsmodel_frogs1, conf.int = T)\n#___________________________----\nplot(lsmodel_frogs1)\nperformance::check_model(lsmodel_frogs1,\n                         check = c(\"qq\", \"outliers\", \"homogeneity\"))"},{"path":"multivariate-linear-models.html","id":"multivariate-linear-models","chapter":"12 Multivariate linear models","heading":"12 Multivariate linear models","text":"far worked almost exclusively single explanatory variables either continuous (regression) categorical (t-test ANOVA). analyses one possible explanatory variable? Extra terms can easily incorporated linear models order test .","code":""},{"path":"multivariate-linear-models.html","id":"factorial-linear-models","chapter":"12 Multivariate linear models","heading":"12.1 Factorial linear models","text":"example data response ground plant biomass production grassland plots relation two resource addition treatments. addition Fertiliser soil addition Light grassland understorey. biomass limited nutrients soil, expect addition fertiliser increase production. biomass limited low levels light caused plant crowding expect addition light increase biomass.first check data look top rows, can see shows application non-application fertiliser light additions, four possible combinations present, known fully factorial design:control (F- L-, added light fertiliser)control (F- L-, added light fertiliser)fertiliser (F+ L-)fertiliser (F+ L-)light (F- L+)light (F- L+)addition (F+ L+)addition (F+ L+)Close inspection dataset shows data presented two ways:Column 1 shows status Fertiliser (F +/-) (two levels)Column 1 shows status Fertiliser (F +/-) (two levels)Column 2 shows status Light (L +/-) (two levels)Column 2 shows status Light (L +/-) (two levels)Column 3 shows whether fertiliser light applied (four levels, combination previous two columns)Column 3 shows whether fertiliser light applied (four levels, combination previous two columns)Column 4 shows biomassColumn 4 shows biomass","code":"\nbiomass <- read_csv(\"data/biomass.csv\")\n# check the structure of the data\nglimpse(biomass)\n\n# check data is in a tidy format\nhead(biomass)\n\n# check variable names\ncolnames(biomass)\n\n# check for duplication\nbiomass %>% \n  duplicated() %>% \n  sum()\n\n# check for typos - by looking at impossible values\nbiomass %>% \n  summarise(min=min(Biomass.m2, na.rm=TRUE), \n            max=max(Biomass.m2, na.rm=TRUE))\n\n# check for typos by looking at distinct characters/values\nbiomass %>% \n  distinct(Fert)\n\nbiomass %>% \n  distinct(Light)\n\nbiomass %>% \n  distinct(FL)\n\n# missing values\nbiomass %>% \n  is.na() %>% \n  sum()\n\n# quick summary\n\nsummary(biomass)## Rows: 64\n## Columns: 4\n## $ Fert       <chr> \"F-\", \"F-\", \"F-\", \"F-\", \"F-\", \"F-\", \"F-\", \"F-\", \"F-\", \"F-\",…\n## $ Light      <chr> \"L-\", \"L-\", \"L-\", \"L-\", \"L-\", \"L-\", \"L-\", \"L-\", \"L-\", \"L-\",…\n## $ FL         <chr> \"F-L-\", \"F-L-\", \"F-L-\", \"F-L-\", \"F-L-\", \"F-L-\", \"F-L-\", \"F-…\n## $ Biomass.m2 <dbl> 254.2, 202.0, 392.4, 455.3, 359.1, 386.5, 355.2, 323.1, 373…## [1] \"Fert\"       \"Light\"      \"FL\"         \"Biomass.m2\"\n## [1] 0## [1] 0\n##      Fert              Light                FL              Biomass.m2   \n##  Length:64          Length:64          Length:64          Min.   :152.3  \n##  Class :character   Class :character   Class :character   1st Qu.:370.1  \n##  Mode  :character   Mode  :character   Mode  :character   Median :425.9  \n##                                                           Mean   :441.6  \n##                                                           3rd Qu.:517.2  \n##                                                           Max.   :750.4"},{"path":"multivariate-linear-models.html","id":"data-summary","chapter":"12 Multivariate linear models","heading":"12.2 Data summary","text":"64 observations four variables, last column response variable (biomass) three columns two different ways indicating experimental design.use column 3 - treating design though one-way ANOVA (four levels)use column 3 - treating design though one-way ANOVA (four levels)use columns 1 & 2 - treating factorial designIf use columns 1 & 2 - treating factorial designWe look two different ways analyse data , pros cons two approaches.","code":""},{"path":"multivariate-linear-models.html","id":"one-way-anova","chapter":"12 Multivariate linear models","heading":"12.3 One-way ANOVA","text":"\nFigure 4.3: Boxplot individual biomass values (black points) treatment means (red diamonds)\nPlotting data superimposing mean values produces graph shown . Now confident difference sample means, can begin linear model quantify test null hypothesis.use FL column predictor equivalent four-level one way ANOVA.can see control treatment (F- L-) intercept mean 356g standard error 23g. Light treatment adds average 30g biomass (close standard error estimated mean difference - 32.72g). contrast addition fertiliser adds 93g biomass.wanted get precise confidence intervals use broom::tidy(ls_1, conf.int = T) confint(ls1).can visualise differences coefficient plot, see clearly adding light produce slight increase mean biomass samples 95% confidence interval includes zero, real confidence consistent/true effect.\nFigure 1.2: Effects light fertiliser treatments biomass relative untreated control (error bars = 95% CI)\nfourth level - combined fertiliser light treatments? interaction (combined effect) light fertiliser expect average biomass difference caused light average biomass difference caused fertiliser add together approximately value found combined treatment.words one treatment effect size , another treatment effect size B, ANOVA predicts combination effect size combined treatments + B.example can see clearly combining separate treatments add combined value. mean biomass combined treatment well additive prediction.mean biomass combined treatment well expect additive effects alone. suggests may positive interaction (light fertiliser treatments produce sum effect greater predicted looking individual effects).Using one-way ANOVA design able accurately estimate mean difference controlled group combined treatment group. really say anything concrete strength interaction effect e.g. true interaction? confident can effect? strength interaction effect compare main effects?contrast one-way ANOVA approach, factorial design lets us test compare additive effects interaction effects","code":"\nls_1 <- lm(Biomass.m2 ~ FL, data = biomass)\nsummary(ls_1)## \n## Call:\n## lm(formula = Biomass.m2 ~ FL, data = biomass)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -233.619  -42.842    1.356   67.961  175.381 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   355.79      23.14  15.376  < 2e-16 ***\n## FLF-L+         30.12      32.72   0.921  0.36095    \n## FLF+L-         93.69      32.72   2.863  0.00577 ** \n## FLF+L+        219.22      32.72   6.699 8.13e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 92.56 on 60 degrees of freedom\n## Multiple R-squared:  0.4686, Adjusted R-squared:  0.442 \n## F-statistic: 17.63 on 3 and 60 DF,  p-value: 2.528e-08\nGGally::ggcoef_model(ls_1,\n                      show_p_values=FALSE,\n                      signif_stars = FALSE,\n                     conf.level=0.95)\n# combine the average mean differences of the light effect and fertiliser effect\ncoef(ls_1)[2] + coef(ls_1)[3] \n\n# compare this to the average difference of the combined treatment\ncoef(ls_1)[4]##   FLF-L+ \n## 123.8187 \n##  FLF+L+ \n## 219.225"},{"path":"multivariate-linear-models.html","id":"testing-for-interactions","chapter":"12 Multivariate linear models","heading":"12.3.1 Testing for interactions","text":"\nFigure 1.3: Left illustration additive model, Right illustration model interaction effect.\ndata can best explained additive model, expect new terms shift intercept slope line remain . interaction effect changes relationship see different gradients.\nLook figure determine whether think evidence\ninteraction effect?\nproduce interactive model, need use separate factors fertiliser light rather single fl factor. make model, include main effects Light Fert, additive model explains sufficient variance enough, suspect interaction term add Light:Fert written follows:Notice estimates mean differences previous one-way ANOVA model. fourth line now indicates much effect two factors interacting changes mean. Also note standard error larger, less power accurately estimate interaction main effect.use factorial combination, last line table coefficients estimates size interaction effect around 95g. combining light fertilisation treatments produced biomass equivalent additive predictions, estimate interaction zero. Instead 95g expect additive effects alone. means order work estimated biomass treatment light fertiliser must sum additive effects Light+ Fert interaction effect Light:Fert.","code":"\nbiomass %>% ggplot(aes(x=Fert, y=Biomass.m2, colour = Light, fill = Light, group = Light))+\n    geom_jitter(width=0.1) +\n    stat_summary(\n        geom = \"point\",\n        fun = \"mean\",\n        size = 3,\n        shape = 23\n    )+stat_summary(\n        geom = \"line\",\n        fun = \"mean\",\n        size = 1, linetype = \"dashed\"\n    )\nls_2 <- lm(Biomass.m2 ~ Fert + # main effect\n             Light + # main effect\n             Fert:Light, # interaction term\n           data = biomass)\n\nsummary(ls_2)## \n## Call:\n## lm(formula = Biomass.m2 ~ Fert + Light + Fert:Light, data = biomass)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -233.619  -42.842    1.356   67.961  175.381 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)      355.79      23.14  15.376  < 2e-16 ***\n## FertF+            93.69      32.72   2.863  0.00577 ** \n## LightL+           30.13      32.72   0.921  0.36095    \n## FertF+:LightL+    95.41      46.28   2.062  0.04359 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 92.56 on 60 degrees of freedom\n## Multiple R-squared:  0.4686, Adjusted R-squared:  0.442 \n## F-statistic: 17.63 on 3 and 60 DF,  p-value: 2.528e-08\nGGally::ggcoef_model(ls_2,\n                      show_p_values=FALSE,\n                      signif_stars = FALSE,\n                     conf.level=0.95)"},{"path":"multivariate-linear-models.html","id":"model-estimates-and-confidence-intervals","chapter":"12 Multivariate linear models","heading":"12.3.1.1 Model estimates and confidence intervals","text":"compare one-way ANOVA model, must add single terms interaction term, add combined treatment first model:\nDon’t make mistake looking main effect light\ntreatment, reporting effect uncertainty\nintervals cross zero. main effect light gives average effect\nacross two fertiliser treatments. However, \ninteraction effect, know doesn’t tell whole story - whether\nlight effect depends whether fertiliser applied.\n\n, significant interaction term, must consider \nmain effects, regardless whether significant \n.\n","code":"\n# model 1\ncoef(ls_1)[4]\n\n# model 2\ncoef(ls_2)[2] + coef(ls_2)[3] + coef(ls_2)[4]##  FLF+L+ \n## 219.225 \n##  FertF+ \n## 219.225"},{"path":"multivariate-linear-models.html","id":"anova-tables","chapter":"12 Multivariate linear models","heading":"12.3.2 ANOVA tables","text":"made linear model interaction term, report whether interaction significant? report main effect terms interaction present?Start interactionIn previous chapters ran anova() function directly linear model. works well simple & 'balanced' designs (equal sample sizes level factor), can misleading 'unbalanced' designs models complex interactions.order report F statistic interaction effect, need carry F-test two models, one one without interaction effect. can use anova() function compare nested modelsThis F-test testing null hypothesis true interaction effect. significance test rejects null hypothesis (just). also provides Akaike information criterion (AIC), alternative method model selection (later).now write follows:interactive effect light fertiliser treatments (ANOVA F1,60 = 4.25, P = 0.044) combining treatments produced substantially biomass (95.4g [95% CI: 2.8 - 188]) expected additive effects alone (Fertiliser 93.7g [28.2 - 159.2], Light 30.1g [-35.3 - 95.6]).make mistake just reporting statistics, interesting bit size effect (estimate) uncertainty (confidence intervals).\nRemember check assumptions full model\nMain effectsA good thing drop1() function interactions model, stops . interaction effect significant, main effects must included, even significant . can make models test main effects less important already know interaction term provides main result.decide include reports main effect estimates confidence intervals come full model, need produce interaction free model produce accurate F-values (especially unbalanced designs, see ).\ncan use reduced models get F values, reports \nestimates come full model.\n","code":"\n# A simpler model with the interaction term removed\nls_3 <- lm(Biomass.m2 ~ Fert + Light, data = biomass)\n\n# Put the simpler model first\nanova(ls_3, ls_2,  test = \"F\")\n# we have to remove the interaction term before we can keep using drop1()\n\nls_4a <- lm(Biomass.m2 ~ Fert, data = biomass)\nls_4b <- lm(Biomass.m2 ~ Light, data = biomass)\n\nanova(ls_4a, ls_3, test = \"F\")\n\nanova(ls_4b, ls_3, test = \"F\")"},{"path":"multivariate-linear-models.html","id":"balancedunbalanced-designs","chapter":"12 Multivariate linear models","heading":"12.4 Balanced/Unbalanced designs","text":"\nunbalanced design run anova() function \nmodel, order variables included can \neffect e.g. \n\nlm(Fert + Light) give different anova table \n\nlm(Light + Fert)\nexamples , unlikely find much difference running anova() function test nested models anova() full model. designs 'balanced' (equal numbers level predictor). designs balanced order matters use anova() - sum squares calculated sequentially (order formula), get different results depending order assemble predictors model!","code":""},{"path":"multivariate-linear-models.html","id":"practice","chapter":"12 Multivariate linear models","heading":"12.4.0.1 Practice","text":"start making deliberately unbalanced dataset","code":"\n# make three vectors and combine them into a new tibble\n\nheight <- c(50,57,91,94,102,110,57,71,85,105,120)\nsize <- c(rep(\"small\", 2), rep(\"large\", 4), rep(\"small\", 3), rep(\"large\", 2))\ntreatment <- c(rep(\"Control\", 6), rep(\"Removal\", 5))\n\nunbalanced <- tibble(height, size, treatment)\n\nunbalanced"},{"path":"multivariate-linear-models.html","id":"activity-1-sums-of-squares","chapter":"12 Multivariate linear models","heading":"12.5 Activity 1: Sums of Squares","text":"nested function drops one term model, adds back drops new one matter order included.","code":"\nmodel_1 <- lm(height ~ treatment + size, data = unbalanced)\nanova(model_1)\n\nmodel_2 <- lm(height ~ size + treatment, data = unbalanced)\nanova(model_2)"},{"path":"multivariate-linear-models.html","id":"post-hoc","chapter":"12 Multivariate linear models","heading":"12.5.1 post-hoc","text":"example unnecessary spend time looking pairwise comparisons four possible levels, interesting finding report strength interaction effect. possible generate estimated means, produce pairwise comparisons emmeans() package","code":"\nemmeans::emmeans(ls_2, specs = pairwise ~ Light + Fert + Light:Fert) %>% \n  confint()\n# including the argument pairwise in front of the ~ prompts the post-hoc pairwise comparisons.\n # $emmeans contains the estimate mean values for each possible combination (with confidence intervals)\n # $ contrasts contains tukey test post hoc comparisons between levels## $emmeans\n##  Light Fert emmean   SE df lower.CL upper.CL\n##  L-    F-      356 23.1 60      310      402\n##  L+    F-      386 23.1 60      340      432\n##  L-    F+      449 23.1 60      403      496\n##  L+    F+      575 23.1 60      529      621\n## \n## Confidence level used: 0.95 \n## \n## $contrasts\n##  contrast          estimate   SE df lower.CL upper.CL\n##  (L- F-) - (L+ F-)    -30.1 32.7 60     -117    56.35\n##  (L- F-) - (L- F+)    -93.7 32.7 60     -180    -7.22\n##  (L- F-) - (L+ F+)   -219.2 32.7 60     -306  -132.75\n##  (L+ F-) - (L- F+)    -63.6 32.7 60     -150    22.90\n##  (L+ F-) - (L+ F+)   -189.1 32.7 60     -276  -102.63\n##  (L- F+) - (L+ F+)   -125.5 32.7 60     -212   -39.06\n## \n## Confidence level used: 0.95 \n## Conf-level adjustment: tukey method for comparing a family of 4 estimates"},{"path":"multivariate-linear-models.html","id":"ancova","chapter":"12 Multivariate linear models","heading":"12.6 ANCOVA","text":"previous section looked interaction two categorical variables, can also examine interactions factor continuous variable. Often referred ANCOVA.data experimental study effects low-level atmospheric pollutants drought agricultural yields. experiment aimed see yields soya bean (William variety), affected stress Ozone levels. task first determine whether evidence interaction effect, drop term model report estimates confidence intervals simplified model.","code":""},{"path":"multivariate-linear-models.html","id":"activity-2-build-your-own-analysis","chapter":"12 Multivariate linear models","heading":"12.7 Activity 2: Build your own analysis","text":"Try make much progress can without checking solutions. Click boxes need help/check working","code":"\n# check the structure of the data\nglimpse(pollution)\n\n# check data is in a tidy format\nhead(pollution)\n\n# check variable names\ncolnames(pollution)\n\n# check for duplication\npollution %>% \n  duplicated() %>% \n  sum()\n\n# check for typos - by looking at impossible values\n# quick summary\n\nsummary(biomass)\n\n# check for typos by looking at distinct characters/values\npollution %>% \n  distinct(Stress)\n\n\n# missing values\nbiomass %>% \n  is.na() %>% \n  sum()## Rows: 30\n## Columns: 4\n## $ Stress  <chr> \"Well-watered\", \"Well-watered\", \"Well-watered\", \"Well-watered\"…\n## $ SO2     <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.02, 0.02, 0.02, 0.02, 0.…\n## $ O3      <dbl> 0.02, 0.05, 0.07, 0.08, 0.10, 0.02, 0.05, 0.07, 0.08, 0.10, 0.…\n## $ William <dbl> 8.623533, 8.690642, 8.360071, 8.151910, 8.032685, 8.535426, 8.…## [1] \"Stress\"  \"SO2\"     \"O3\"      \"William\"\n## [1] 0\n##      Fert              Light                FL              Biomass.m2   \n##  Length:64          Length:64          Length:64          Min.   :152.3  \n##  Class :character   Class :character   Class :character   1st Qu.:370.1  \n##  Mode  :character   Mode  :character   Mode  :character   Median :425.9  \n##                                                           Mean   :441.6  \n##                                                           3rd Qu.:517.2  \n##                                                           Max.   :750.4## [1] 0\npollution %>% \n  ggplot(aes(x = O3, y = William))+\n  geom_point()+\n  geom_smooth(method = \"lm\")+\n  facet_wrap(~ Stress)+\n    labs(x = expression(paste(Ozone~mu~L~L^-1)),\n       y = expression(paste(Log~Yield~(kg~ha^-1))))\nWilliam_ls1 <- lm(William ~ O3 + Stress + O3:Stress, data = pollution)\n\nWilliam_ls1 %>% \n    broom::tidy(conf.int = T)"},{"path":"multivariate-linear-models.html","id":"check-the-model-fit","chapter":"12 Multivariate linear models","heading":"12.7.0.1 Check the model fit","text":"","code":"\nperformance::check_model(William_ls1)"},{"path":"multivariate-linear-models.html","id":"simplify-the-model","chapter":"12 Multivariate linear models","heading":"12.7.0.2 Simplify the model","text":"Testing dropping interaction term significantly reduce variance explainedGet F values simpler modelReport estimates confidence intervals new model","code":"\nWilliam_ls2 <- lm(William ~ O3 + Stress, data = pollution)\n\nanova(William_ls2, William_ls1, test = \"F\")\nWilliam_ls3a <- lm(William ~ Stress, data = pollution)\nWilliam_ls3b <- lm(William ~ O3, data = pollution)\n\nanova(William_ls3a, William_ls2, test = \"F\")\nanova(William_ls3b, William_ls2, test = \"F\")\nWilliam_ls2 %>% \n    broom::tidy(conf.int = T)"},{"path":"multivariate-linear-models.html","id":"section","chapter":"12 Multivariate linear models","heading":"12.7.0.3 ","text":"Get F values simpler model","code":"\nWilliam_ls3a <- lm(William ~ Stress, data = pollution)\nWilliam_ls3b <- lm(William ~ O3, data = pollution)\n\nanova(William_ls3a, William_ls2, test = \"F\")\nanova(William_ls3b, William_ls2, test = \"F\")"},{"path":"multivariate-linear-models.html","id":"section-1","chapter":"12 Multivariate linear models","heading":"12.7.0.4 ","text":"Report estimates confidence intervals new model","code":"\nWilliam_ls2 %>% \n    broom::tidy(conf.int = T)"},{"path":"multivariate-linear-models.html","id":"summary-7","chapter":"12 Multivariate linear models","heading":"12.8 Summary","text":"Always good hypothesis including interactionAlways good hypothesis including interactionWhen models significant interaction effects must always consider main terms even significant themselvesWhen models significant interaction effects must always consider main terms even significant themselvesReport F values interactions priorityReport F values interactions priorityIF interactions significant estimates come full model, F-values come reduced model (main effects). interaction terms significant can removed (model simplification).interactions significant estimates come full model, F-values come reduced model (main effects). interaction terms significant can removed (model simplification).Use nested models avoid mistakes using unbalanced experiment designUse nested models avoid mistakes using unbalanced experiment designAlways report estimates effect sizes - important bit - easy lose sight models get complicatedAlways report estimates effect sizes - important bit - easy lose sight models get complicated","code":""},{"path":"complex-models.html","id":"complex-models","chapter":"13 Complex models","heading":"13 Complex models","text":"","code":""},{"path":"complex-models.html","id":"designing-a-model","chapter":"13 Complex models","heading":"13.0.1 Designing a Model","text":"introduced fruitfly dataset Partridge Farquhar (1981)3. understanding sexual selection reproductive biology fruit flies, know well established 'cost' reproduction terms reduced longevity female fruitflies. data experiment designed test whether increased sexual activity affects lifespan male fruitflies.flies used outbred stock, sexual activity manipulated supplying males either new virgin females day, previously mated females ( Inseminated, remating rates lower), provide females (Control). groups otherwise treated identically.type: type female companion (virgin, inseminated, control(partners = 0))type: type female companion (virgin, inseminated, control(partners = 0))longevity: lifespan dayslongevity: lifespan daysthorax: length thorax micrometres (proxy body size)thorax: length thorax micrometres (proxy body size)sleep: percentage day spent sleepingsleep: percentage day spent sleeping","code":""},{"path":"complex-models.html","id":"hypothesis-1","chapter":"13 Complex models","heading":"13.0.2 Hypothesis","text":"start formal analysis think clearly sensible parameters test. example, interested effect sexual activity longevity. possible factors may also affect longevity include model well, think hard terms might reasonably expected interact sexual activity affect longevity.exercise just asked try think logically suitable predictors. formal investigation support evidence possibletype - definitely included.type - definitely included.thorax - size flies determine longevity. Carreira et al (2009)4thorax - size flies determine longevity. Carreira et al (2009)4sleep - sleep easily help determine longevity. Thompson et al (2020)5sleep - sleep easily help determine longevity. Thompson et al (2020)5type:sleep - amount sleep (rest) helps promote longevity change depending much activity fly engages awake. Chen et al (2017)6type:sleep - amount sleep (rest) helps promote longevity change depending much activity fly engages awake. Chen et al (2017)6","code":""},{"path":"complex-models.html","id":"checking-the-data","chapter":"13 Complex models","heading":"13.0.3 Checking the data","text":"now import, clean tidy data. Making sure tidy format, variables useful names, mistakes, missing data typos.Based variables decided test start simple visualisations, understand distribution data, investigate visually relationships wish test.full two--two plot entire dataset, try follow specific plots.","code":"\nGGally::ggpairs(fruitfly)"},{"path":"complex-models.html","id":"activity-1-building-a-model","chapter":"13 Complex models","heading":"13.1 Activity 1: Building a model","text":"Think carefully plots make investigate potential differences relationships wish investigate - try answer questions first checking examples hidden behind dropdowns.first figure - can investigate whether obvious difference longevities males across three treatments\nFigure 13.1: density distribution longevity across three sexual activity treatments\nQ like treatment affects longevity? YesNoIn first figure - can investigate whether obvious difference longevities males across three treatments\nFigure 13.2: scatterplot longevity body size (thorax (mm)). trend line added - often good idea look data points without lead conclusion line\nQ look like size affects longevity? YesNo\nFigure 13.3: scatterplot thorax longevity - colours indicate treatment types. time included line, help determine think slopes different group\nQ look like size affects longevity differently treatment groups? YesNoHere look though larger flies longer lifespan smaller flies. appears little difference angle slopes groups. mean test model, may decide worth including.also interested potential effect sleep activity, can construct scatter plot sleep longevity, including treatment covariate.\nFigure 6.4: scatter plot proportion time spent sleeping longevity linear model trendline. Points represent individual flies, colours represent treatments.\nplots - trendlines moving direction? YesNoHere look though sleep interacts treatment affect lifespan. slopes lines different group. order know strength association, significantly different might observe null hypothesis, build model.","code":"\ncolours <- c(\"cyan\", \"darkorange\", \"purple\")\n\nfruitfly %>% \n  ggplot(aes(x = longevity, y = type, fill = type))+\n  geom_density_ridges(alpha = 0.5)+\n  scale_fill_manual(values = colours)+\n  theme_minimal()+\n  theme(legend.position = \"none\")\nfruitfly %>% \n  ggplot(aes(x = thorax, y = longevity))+\n  geom_point()+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ncolours <- c(\"cyan\", \"darkorange\", \"purple\")\n\nfruitfly %>% \n  ggplot(aes(x=thorax, y = longevity, group = type, colour = type))+\n  geom_point( alpha = 0.6)+\n  geom_smooth(method = \"lm\",\n            se = FALSE)+\n  scale_colour_manual(values = colours)+\n  theme_minimal()\nfruitfly %>% \n  ggplot(aes(x=sleep, y = longevity, group = type, colour = type))+\n  geom_point( alpha = 0.6)+\n  geom_smooth(method = \"lm\",\n            se = FALSE)+\n  scale_colour_manual(values = colours)+\n  theme_minimal()"},{"path":"complex-models.html","id":"designing-a-model-1","chapter":"13 Complex models","heading":"13.1.0.1 Designing a model","text":"\ninclude interaction term, numbers produced \nmuch less mean\nestimate just combined main effects.\n\nincluded interaction effect number terms \nquite long takes consideration understand. can see \nindividual estimates appear interaction \nstrong effect (estimate) appear \ndifferent null hypothesis interaction effect. \nuse F test look overall effect \nsure.\n","code":"\n# a full model\nflyls1 <- lm(longevity ~ type + thorax + sleep + type:sleep, data = fruitfly)\n\nflyls1 %>% \n  broom::tidy()# intercept\ncoef(flyls1)[1] + \n  \n# 1*coefficient for virgin treatment  \ncoef(flyls1)[3] + \n  \n# 0.79 * coefficient for thorax size  \n(coef(flyls1)[4]*0.79) + \n  \n# 22 * coefficient for sleep  \n(coef(flyls1)[5]*22) + \n# 22 * 1 * coefficient for interaction\n(coef(flyls1)[7]*22*1)## typeVirgin:sleep \n##        -2.473406"},{"path":"complex-models.html","id":"activity-2-model-checking","chapter":"13 Complex models","heading":"13.2 Activity 2: Model checking","text":"","code":""},{"path":"complex-models.html","id":"model-checking-collinearity","chapter":"13 Complex models","heading":"13.2.0.1 Model checking & collinearity","text":"start playing terms model, check see even good way fitting measuring data. check assumptions model met.Question - assumption homogeneity variance met? YesNoMostly - reference line fairly flat (slight curve).Mostly - reference line fairly flat (slight curve).looks though might increasing heterogeneity larger values, though minor.looks though might increasing heterogeneity larger values, though minor.VERDICT, pretty much ok, fine making inferences.slight curvature indicate might get better fit transformation, perhaps missing variable included model improve residuals. instance overly concerned. See great explainer intepreting residuals7.Question - residuals normally distributed? YesNoYes - QQplot looks pretty good, minor indication right skew, nothing worry .Interpreting QQ plotsQuestion - issue Collinearity? YesNoThis graph clearly shows collinearity. unusual include interaction term, see evidence collinearity terms part interaction take another look8.can collinearity main effects? 1) Nothing 2) Transform 3) Drop one terms.check_performance() function produces visual summary Variance Inflation Factor produced vif() function. measure standard error estimated coefficient. larger (greater 5 10), indicates model problems estimating coefficient. affect model predictions, makes difficult determine estimate change predictor.","code":"\nperformance::check_model(flyls1)\ncar::vif(flyls1)##                 GVIF Df GVIF^(1/(2*Df))\n## type       12.478906  2        1.879508\n## thorax      1.052967  1        1.026142\n## sleep       8.750764  1        2.958169\n## type:sleep 38.749001  2        2.494969"},{"path":"complex-models.html","id":"data-transformations","chapter":"13 Complex models","heading":"13.3 Data transformations","text":"common issues trying fit simple linear regression models response variable normal violates modelling assumption. two things can case:Variable transformation e.g lm(sqrt(x) ~ y, data = data)\nCan sometimes fix linearity\nCan sometimes fix non-normality heteroscedasticity (.e non-constant variance)\nVariable transformation e.g lm(sqrt(x) ~ y, data = data)Can sometimes fix linearityCan sometimes fix linearityCan sometimes fix non-normality heteroscedasticity (.e non-constant variance)Can sometimes fix non-normality heteroscedasticity (.e non-constant variance)Generalized Linear Models (GLMs) change error structure (.e assumption residuals need normal - see next week.)Generalized Linear Models (GLMs) change error structure (.e assumption residuals need normal - see next week.)","code":""},{"path":"complex-models.html","id":"boxcox","chapter":"13 Complex models","heading":"13.3.1 BoxCox","text":"\nBoxCox gets name two inventors, George Box David\nCox. Implemented MASS package, applied linear model \nsytematically applies transformations raising y variable \npower (lambda).\n\nR output MASS::boxcox() function plots \nmaximum likelihood curve (95% confidence interval - drops \ndotted lines) best transformation fitting data \nmodel.\n\nTable 13.1: Common Box-Cox Transformations\n\nFigure 5.3: standard curve fitted maximum likelihood, dashed lines represent 95% confidence interval range picking 'best' transformation dependent variable\nQuestion - fit model improve square root transformation? YesNo","code":"\n# run this, pick a transformation and retest the model fit\nMASS::boxcox(flyls1)\nflyls_sqrt <- lm(sqrt(longevity) ~ type + thorax + sleep + type:sleep, data = fruitfly)\n\nperformance::check_model(flyls_sqrt)"},{"path":"complex-models.html","id":"model-selection","chapter":"13 Complex models","heading":"13.4 Model selection","text":"Based ANOVA table, appear strong rationale keeping interaction term model (AIC F-test). Therefore can confidently remove interaction, simplifying model making interpretation easier.Question - drop sleep model? YesNoThere good reason remove non-significant interaction terms model, complicate estimates make interpretations difficult. main effects things little ambiguous.main aim prediction, makes sense cautious retain non-significant terms, extra terms make difference R^2 model.focus hypothesis testing, removal non-significant terms can help produce 'true' model, optional. Generally speaking often simpler leave main effects model (carefully considered terms included first place).","code":"\n# Remove top-level interaction\n\nflyls2 <- lm(longevity ~ type + thorax + sleep, data = fruitfly)\n\nanova(flyls2, flyls1, test = \"F\")\nflyls3a <- lm(longevity ~ type + thorax, data = fruitfly)\nflyls3b <- lm(longevity ~ type + sleep, data = fruitfly)\nflyls3c <- lm(longevity ~ thorax + sleep, data = fruitfly)\n\nanova(flyls3a, flyls2)\nanova(flyls3b, flyls2)\nanova(flyls3c, flyls2)"},{"path":"complex-models.html","id":"posthoc","chapter":"13 Complex models","heading":"13.5 Posthoc","text":"Using emmeans package easy way produce estimate mean values (rather mean differences) different categories emmeans. term pairwise included also include post-hoc pairwise comparisons levels tukey test contrasts.\ncontinuous variables (sleep thorax) - emmeans\nset mean value within dataset, comparisons \nconstant categories average value continuous\nvariables.\n","code":"\nemmeans::emmeans(flyls2, specs = pairwise ~ type + thorax + sleep)## $emmeans\n##  type        thorax sleep emmean   SE  df lower.CL upper.CL\n##  Control      0.821  23.5   61.3 2.26 120     56.8     65.8\n##  Inseminated  0.821  23.5   64.9 1.59 120     61.8     68.1\n##  Virgin       0.821  23.5   48.0 1.59 120     44.9     51.2\n## \n## Confidence level used: 0.95 \n## \n## $contrasts\n##  contrast                                                                 \n##  Control thorax0.82096 sleep23.464 - Inseminated thorax0.82096 sleep23.464\n##  Control thorax0.82096 sleep23.464 - Virgin thorax0.82096 sleep23.464     \n##  Inseminated thorax0.82096 sleep23.464 - Virgin thorax0.82096 sleep23.464 \n##  estimate   SE  df t.ratio p.value\n##     -3.63 2.77 120  -1.309  0.3929\n##     13.25 2.76 120   4.796  <.0001\n##     16.87 2.25 120   7.508  <.0001\n## \n## P value adjustment: tukey method for comparing a family of 3 estimates"},{"path":"complex-models.html","id":"activity-3-write-up","chapter":"13 Complex models","heading":"13.6 Activity 3: Write-up","text":"tested hypothesis sexual activity costly male Drosophila melanogaster fruitflies. Previous research indicated sleep deprived males less attractive females, indicate levels sexual activity might affected sleep impact effect longevity, included interaction term full model. Body size also know affect lifespan, included covariate mode.small interaction effect decreased lifespan increasing sleep treatment groups compared control samples, significantly different effect (F2,118 = 0.512, P = 0.6), therefore dropped full model (Table 15.1).\nTable 13.2: Linear model coefficients\nsignificant overall effect treatment male longevity (Linear model: F2,120 = 30.1, P < 0.001), males paired virgin females lowest mean longevity (48 days, [95%CI: 44.9 - 51.2]) (holding body size sleep constant), compared control males (61.3 days [56.8 - 65.8]) males paired inseminated females (64.9 days [61.8 - 68.1 days]).Post hoc analysis showed differences statistically significant males paired control females compared inseminated (Tukey test: t120 = 4.8, P < 0.001) virgin groups (t120 = 7.5, P < 0.001), overall evidence difference inseminated virgin groups (t120 = -1.309 P < 0.3929) (Figure 19.4).Comparing treatment effects predictors longevity body size sleep, found sleep small effect longevity (mean change -0.05 days [-0.18 - 0.07]) significantly different effect (Linear model: F1,120 = 0.68, P = 0.41). Body size (taken thorax length) significant predictor longevity (F1,120 = 121, P < 0.001), 0.1 mm increase body size adding 14.4 days individual lifespan [11.8 - 17]. appears though body size stronger effect longevity treatment, indicating measurable cost sexual activity males, may less severe females (compared ), less severe measurable predictors.\nFigure 13.2:  scatter plot longevity body size across three treatments differening male sexual activity. Fitted model slopes reduced linear model (main effects thorax size, sleep treatment group), 95% confidence intervals, circles individual data points. Marginal plots density plot distributions thorax length longevity split treatments.\n","code":"\nlibrary(kableExtra)\nflyls2 %>% broom::tidy(conf.int = T) %>% \n select(-`std.error`) %>% \nmutate_if(is.numeric, round, 2) %>% \nkbl(col.names = c(\"Predictors\",\n                    \"Estimates\",\n                    \"t-value\",\n                    \"P\",\n                    \"Lower 95% CI\",\n                    \"Upper 95% CI\"),\n      caption = \"Linear model coefficients\", \n    booktabs = T) %>% \n   kable_styling(full_width = FALSE, font_size=16)"},{"path":"complex-models.html","id":"summary-8","chapter":"13 Complex models","heading":"13.7 Summary","text":"chapter worked scientific knowledge develop testable hypotheses built statistical models formally assess . now working pipeline tackling complex datasets, developing insights producing explaining robust linear models.","code":""},{"path":"complex-models.html","id":"checklist-2","chapter":"13 Complex models","heading":"13.7.0.1 Checklist","text":"Think carefully hypotheses test, use scientific knowledge background reading support thisThink carefully hypotheses test, use scientific knowledge background reading support thisImport, clean understand dataset: use data visuals investigate trends determine clear support hypothesesImport, clean understand dataset: use data visuals investigate trends determine clear support hypothesesFit linear model, including interaction terms cautionFit linear model, including interaction terms cautionInvestigate fit model, understand parameters may never perfect, classic patterns residuals may indicate poorly fitting model - sometimes can fixed careful consideration missing variables data transformationInvestigate fit model, understand parameters may never perfect, classic patterns residuals may indicate poorly fitting model - sometimes can fixed careful consideration missing variables data transformationTest removal interaction terms model, look AIC significance testsTest removal interaction terms model, look AIC significance testsMake sure understand output model summary, sense check graphs madeMake sure understand output model summary, sense check graphs madeThe direction size effects priority - produce estimates uncertainties. Make sure observations clear.direction size effects priority - produce estimates uncertainties. Make sure observations clear.Write-significance test results, taking care report just significance (required parts significance test). know report? Within complex model - reporting t indicate slope line single term intercept, F overall effect predictor across levels, post-hoc wish compare across levels.Write-significance test results, taking care report just significance (required parts significance test). know report? Within complex model - reporting t indicate slope line single term intercept, F overall effect predictor across levels, post-hoc wish compare across levels.Well described tables figures can enhance results sections - take time make sure informative attractive.Well described tables figures can enhance results sections - take time make sure informative attractive.","code":""},{"path":"complex-models.html","id":"supplementary-code","chapter":"13 Complex models","heading":"13.8 Supplementary code","text":"sjPlot really nice package helps produce model summaries automatically","code":"\nlibrary(sjPlot)\ntab_model(flyls2)\nlibrary(gtsummary)\ntbl_regression(flyls2)"},{"path":"generalized-linear-models.html","id":"generalized-linear-models","chapter":"14 Generalized Linear Models","heading":"14 Generalized Linear Models","text":"","code":""},{"path":"generalized-linear-models.html","id":"motivation","chapter":"14 Generalized Linear Models","heading":"14.1 Motivation","text":"previous workshop seen linear models powerful modelling tool.\nHowever, satisfy following assumptions:linear relationship predictors mean response value.Variances equal across predicted values response (homoscedatic)Errors normally distributed.Samples collected random.omitted variables importanceIf assumptions 1-3 violated can often transform response variable\ntry fix (Box-Cox & transformation).\nHowever, lot cases either possible (e.g binary output)\nwant explicitly model underlying distribution (e.g count data).\nInstead, can use Generalized Linear Models (GLMs) let us change error structure (assumption 3) something normal distribution.","code":""},{"path":"generalized-linear-models.html","id":"generalised-linear-models-glms","chapter":"14 Generalized Linear Models","heading":"14.2 Generalised Linear Models (GLMs)","text":"Generalised Linear Models (GLMs) :linear predictor.linear predictor.error/variance structure.error/variance structure.link function (like 'internal' transformation).link function (like 'internal' transformation).first (1) familiar, everything comes ~ linear model formula. equation \\(\\beta_0 + \\beta_1\\). second (2) also familiar, variance measures error structure model \\(\\epsilon\\). ordinary least squares model uses normal distribution, GLMs able use wider range distributions including poisson, binomial Gamma. third component (3) less familiar, link function equivalent transformation ordinary least squares model. However, rather transforming data, transform predictions made linear predictors. Common link functions log square root.\nMaximum Likelihood - Generalised Linear Models fit \nregression line finding parameter values best fit model\ndata. similar way ordinary least squares\nfinds line best fit reducing sum squared errors. \nfact data normally distributed residuals, particular form\nmaximum likelihood least squares.\n\nHowever normal (gaussian) distribution good model\nlots types data, binary data, good example one\ninvestigate workshop.\n\nMaximum likelihood provides generalized approach model\nfitting includes, broader , least squares.\n\nadvantage least squares method using \ncan generate precise equations fit line. contrast\ncalculations GLMs (beyond scope course)\napproximate, essentially multiple potential best fit lines made\ncompared .\n\nsee two main differences GLM output:\n\nmodel one mean variance calculated\nseparately (e.g. normal distributions), uncertainty\nestimates use t distribution; compare complex\nsimplified models (using anova() \ndrop1()) use F-test.\n\nHowever, provide distributions mean variance\nexpected change together (Poisson Binomial), \ncalculate uncertainty estimates using z distribution, \ncompare models chi-square distribution.\nsimple linear regression model used far special cases GLM:equivalent toCompared lm(), glm() function takes additional argument called family, \nspecifies error structure link function.default link function normal (Gaussian) distribution identity, transformation neededi.e. mean \\(\\mu\\) :\\[\n\\mu = \\beta_0 + \\beta_1 X\n\\]Defaults usually good choices (shown bold ):exactly . surprising, maximum likelihood fitted ordinary least squares model.","code":"lm(height ~ weight)glm(height ~ weight, family=gaussian(link=identity))\nflyls <- lm(longevity ~ type + thorax + sleep, data = fruitfly)\nsummary(flyls)## \n## Call:\n## lm(formula = longevity ~ type + thorax + sleep, data = fruitfly)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -28.153  -6.836  -2.191   7.196  29.046 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     -56.04502   11.17882  -5.013 1.87e-06 ***\n## typeInseminated   3.62796    2.77122   1.309    0.193    \n## typeVirgin      -13.24603    2.76198  -4.796 4.70e-06 ***\n## thorax          144.43008   13.11616  11.012  < 2e-16 ***\n## sleep            -0.05281    0.06383  -0.827    0.410    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.23 on 120 degrees of freedom\n## Multiple R-squared:  0.6046, Adjusted R-squared:  0.5914 \n## F-statistic: 45.88 on 4 and 120 DF,  p-value: < 2.2e-16\nflyglm <- glm(longevity ~ type + thorax + sleep, \n             family = gaussian(link = \"identity\"),\n             data = fruitfly)\nsummary(flyglm)## \n## Call:\n## glm(formula = longevity ~ type + thorax + sleep, family = gaussian(link = \"identity\"), \n##     data = fruitfly)\n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     -56.04502   11.17882  -5.013 1.87e-06 ***\n## typeInseminated   3.62796    2.77122   1.309    0.193    \n## typeVirgin      -13.24603    2.76198  -4.796 4.70e-06 ***\n## thorax          144.43008   13.11616  11.012  < 2e-16 ***\n## sleep            -0.05281    0.06383  -0.827    0.410    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 126.0381)\n## \n##     Null deviance: 38253  on 124  degrees of freedom\n## Residual deviance: 15125  on 120  degrees of freedom\n## AIC: 966.2\n## \n## Number of Fisher Scoring iterations: 2"},{"path":"generalized-linear-models.html","id":"section-2","chapter":"14 Generalized Linear Models","heading":"14.2.0.1 ","text":"exactly . surprising, maximum likelihood fitted ordinary least squares model.","code":"\nflyglm <- glm(longevity ~ type + thorax + sleep, \n             family = gaussian(link = \"identity\"),\n             data = fruitfly)\nsummary(flyglm)## \n## Call:\n## glm(formula = longevity ~ type + thorax + sleep, family = gaussian(link = \"identity\"), \n##     data = fruitfly)\n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     -56.04502   11.17882  -5.013 1.87e-06 ***\n## typeInseminated   3.62796    2.77122   1.309    0.193    \n## typeVirgin      -13.24603    2.76198  -4.796 4.70e-06 ***\n## thorax          144.43008   13.11616  11.012  < 2e-16 ***\n## sleep            -0.05281    0.06383  -0.827    0.410    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 126.0381)\n## \n##     Null deviance: 38253  on 124  degrees of freedom\n## Residual deviance: 15125  on 120  degrees of freedom\n## AIC: 966.2\n## \n## Number of Fisher Scoring iterations: 2"},{"path":"generalized-linear-models.html","id":"workflow-for-fitting-a-glm","chapter":"14 Generalized Linear Models","heading":"14.3 Workflow for fitting a GLM","text":"Exploratory data analysisChoose suitable error termChoose suitable mean function (link function)Fit model\nResidual checks model fit diagnostics\nRevise model (transformations etc.)\nResidual checks model fit diagnosticsRevise model (transformations etc.)Model simplification requiredCheck final model \ntransform data e.g. log sqrt, changes\nmean variance time (everything gets squished ).\nmight fine, can lead difficult model fits need \nreduce unequal variance leads change (often curvature) \nway predictors fit response variable.\n\nGLMs model mean variability independently. link function\nproduces transformation predictors mean, \nrelationship mean data points modeled separately.\n","code":""},{"path":"poisson-regression-for-count-data-or-rate-data.html","id":"poisson-regression-for-count-data-or-rate-data","chapter":"15 Poisson regression (for count data or rate data)","heading":"15 Poisson regression (for count data or rate data)","text":"Count rate data ubiquitous life sciences (e.g number parasites per microlitre blood, number species counted particular area). type data discrete non-negative.\ncases assuming response variable normally distributed typically sensible.\nPoisson distribution lets us model count data explicitly.Recall simple linear regression case (.e GLM Gaussian error structure identity link). sake clarity consider single explanatory variable \\(\\mu\\) mean Y:\\[\n\\begin{aligned}\n\\mu & = \\beta_0 + \\beta_1X\n\\end{aligned}\n\\]mean function unconstrained, .e value \\(\\beta_0 + \\beta_1X\\) can range \\(-\\infty\\) \\(+\\infty\\). want model count data therefore want constrain mean positive . Mathematically can taking logarithm mean (log default link Poisson distribution). assume count data variance Poisson distributed (discrete, non-negative distribution), obtain Poisson regression model (consistent statistics literature rename \\(\\mu\\) \\(\\lambda\\)):\\[\n\\begin{aligned}\nY & \\sim \\mathcal{Pois}(\\lambda) \\\\\n\\log{\\lambda} & = \\beta_0 + \\beta_1X\n\\end{aligned}\n\\]Note - relationship mean data modelled Poisson variance. relationship predictors mean modelled log transformation.Poisson distribution following characteristics:Discrete variable, defined range \\(0, 1, \\dots, \\infty\\).single rate parameter \\(\\lambda\\), \\(\\lambda > 0\\).Mean = \\(\\lambda\\)Variance = \\(\\lambda\\)model variance equal mean - mean increases variance.Poisson regression case assume mean variance .\nHence, mean increases, variance increases also (heteroscedascity).\nmay may sensible assumption watch ! Just Poisson distribution usually fits well count data, mean Gaussian distribution always work.Recall link function predictors mean rules logarithms (\\(\\log{\\lambda} = k\\)(value predictors), \\(\\lambda = e^k\\)):\\[\n\\begin{aligned}\n\\log{\\lambda} & = \\beta_0 + \\beta_1X \\\\\n\\lambda & = e^{\\beta_0 + \\beta_1X }\n\\end{aligned}\n\\]\nThus effectively modelling observed counts (original scale) using exponential mean function.","code":""},{"path":"poisson-regression-for-count-data-or-rate-data.html","id":"example-cuckoos","chapter":"15 Poisson regression (for count data or rate data)","heading":"15.1 Example: Cuckoos","text":"study Kilner et al. (1999), authors\nstudied begging rate nestlings relation total mass brood reed warbler chicks cuckoo chicks.\nquestion interest :nestling mass affect begging rates different species?data columns :Mass: nestling mass chick gramsBeg: begging calls per 6 secsSpecies: Warbler CuckooThere seem relationship mass begging calls \ndifferent species. tempting fit linear model data.\nfact, authors\noriginal paper ; reed warbler chicks (solid circles, dashed fitted line) \ncuckoo chick (open circles, solid fitted line):model inadequate. predicting negative begging calls within \nrange observed data, clearly make sense.Let us display model diagnostics plots linear model.residuals plot depicts strong \"funnelling\" effect, highlighting model assumptions violated.\ntherefore try different model structure.response variable case classic count data: discrete bounded zero (.e negative counts). therefore try Poisson model using canonical log link function mean:\\[\n    \\log{\\lambda} = \\beta_0 + \\beta_1 M_i + \\beta_2 S_i + \\beta_3 M_i S_i\n\\]\\(M_i\\) nestling mass \\(S_i\\) dummy variable\\[\nS_i = \\left\\{\\begin{array}{ll}\n        1 & \\mbox{$$ warbler},\\\\\n        0 & \\mbox{otherwise}.\n        \\end{array}\n        \\right.\n\\]term \\(M_iS_i\\) interaction term. Think additional explanatory variable model.\nEffectively lets us different slopes different species (without interaction term assume \nspecies slope relationship begging rate mass, intercept differ).mean regression lines two species look like :Cuckoo (\\(S_i=0\\))\\[\n\\begin{aligned}\n    \\log{\\lambda} & = \\beta_0 + \\beta_1 M_i + (\\beta_2 \\times 0)  + (\\beta_3 \\times M_i \\times 0)\\\\\n    \\log{\\lambda} & = \\beta_0 + \\beta_1 M_i\n\\end{aligned}\n\\]Intercept = \\(\\beta_0\\), Gradient = \\(\\beta_1\\)Intercept = \\(\\beta_0\\), Gradient = \\(\\beta_1\\)Warbler (\\(S_i=1\\))Warbler (\\(S_i=1\\))\\[\n\\begin{aligned}\n    \\log{\\lambda} & = \\beta_0 + \\beta_1 M_i + (\\beta_2 \\times 1)  + (\\beta_3 \\times M_i \\times 1)\\\\\n    \\log{\\lambda} & = \\beta_0 + \\beta_1 M_i + \\beta_2 + \\beta_3M_i\\\\\n\\end{aligned}\n\\]Fit model interaction term R:Note appears negative interaction effect Species:Mass, indicating Begging calls increase mass much expect Warblers compared Cuckoos.##Activity 1: Plot mean regression line species:Hint: Use broom::augment. fit model log scale, get fitted generalized linear response (Y log scale). lines straight. get exponential curve scale original data, specify type.predict = response straight line log-scaled version data. Try :","code":"\nhead(cuckoo)\nggplot(cuckoo, aes(x=Mass, y=Beg, colour=Species)) + geom_point()\n## Fit model\n## There is an interaction term here, it is reasonable to think that how calling rates change with size might be different between the two species.\ncuckoo_ls1 <- lm(Beg ~ Mass+Species+Mass:Species, data=cuckoo) \nperformance::check_model(cuckoo_ls1, \n                         check = c(\"homogeneity\",\n                                   \"qq\"))\ncuckoo_glm1 <- glm(Beg ~ Mass + Species + Mass:Species, data=cuckoo, family=poisson(link=\"log\"))\n\nsummary(cuckoo_glm1)## \n## Call:\n## glm(formula = Beg ~ Mass + Species + Mass:Species, family = poisson(link = \"log\"), \n##     data = cuckoo)\n## \n## Coefficients:\n##                      Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)          0.334475   0.227143   1.473  0.14088    \n## Mass                 0.094847   0.007261  13.062  < 2e-16 ***\n## SpeciesWarbler       0.674820   0.259217   2.603  0.00923 ** \n## Mass:SpeciesWarbler -0.021673   0.008389  -2.584  0.00978 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 970.08  on 50  degrees of freedom\n## Residual deviance: 436.05  on 47  degrees of freedom\n## AIC: 615.83\n## \n## Number of Fisher Scoring iterations: 6"},{"path":"poisson-regression-for-count-data-or-rate-data.html","id":"variable-scale","chapter":"15 Poisson regression (for count data or rate data)","heading":"15.1.1 Variable scale","text":"","code":"\n# using augment allows you to generate fitted outcomes from the regression, make sure to set the predictions onto the response scale in order to 'back transform` the data onto the original scale\n\nbroom::augment(cuckoo_glm1, type.predict = \"response\") %>% \nggplot(aes(x=Mass, y=.fitted, colour=Species)) + \n  geom_point() +\n  geom_line()+\n  scale_colour_manual(values=c(\"green3\",\"turquoise3\"))+\n  theme_minimal()"},{"path":"poisson-regression-for-count-data-or-rate-data.html","id":"log-scale","chapter":"15 Poisson regression (for count data or rate data)","heading":"15.1.2 Log scale","text":"Compare new Poisson model fits ordinary least squares model. can see although homogeneity variance far perfect, curvature model drastically reduced (makes sense now model fitted exponential data), qqplot within acceptable confidence intervals.reminder interpret regression coefficients model interaction termIntercept = \\(\\beta 0\\) (intercept reference baseline log mean number begging calls cuckoos mass 0)Intercept = \\(\\beta 0\\) (intercept reference baseline log mean number begging calls cuckoos mass 0)Mass = \\(\\beta1\\) (slope: change log mean count begging calls every gram bodyweight cuckoos)Mass = \\(\\beta1\\) (slope: change log mean count begging calls every gram bodyweight cuckoos)SpeciesWarbler = \\(\\beta2\\) (log mean increase/decrease begging call rate warblers relative cuckoos)SpeciesWarbler = \\(\\beta2\\) (log mean increase/decrease begging call rate warblers relative cuckoos)Mass:SpeciesWarbler =\\(\\beta3\\) (log mean increase/decrease slope warblers relative cuckoos)Mass:SpeciesWarbler =\\(\\beta3\\) (log mean increase/decrease slope warblers relative cuckoos)\nPoisson distribution variance fixed \nmean using z scores. Estimates log scale \nlink function - means S.E. confidence\nintervals\n","code":"\nbroom::augment(cuckoo_glm1) %>% \nggplot(aes(x=Mass, y=.fitted, colour=Species)) + \n  geom_point() +\n  geom_line()+\n  scale_colour_manual(values=c(\"green3\",\"turquoise3\"))+\n  theme_minimal()\nperformance::check_model(cuckoo_glm1, \n                         check = c(\"homogeneity\",\n                                   \"qq\"))\nsummary(cuckoo_glm1)## \n## Call:\n## glm(formula = Beg ~ Mass + Species + Mass:Species, family = poisson(link = \"log\"), \n##     data = cuckoo)\n## \n## Coefficients:\n##                      Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)          0.334475   0.227143   1.473  0.14088    \n## Mass                 0.094847   0.007261  13.062  < 2e-16 ***\n## SpeciesWarbler       0.674820   0.259217   2.603  0.00923 ** \n## Mass:SpeciesWarbler -0.021673   0.008389  -2.584  0.00978 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 970.08  on 50  degrees of freedom\n## Residual deviance: 436.05  on 47  degrees of freedom\n## AIC: 615.83\n## \n## Number of Fisher Scoring iterations: 6"},{"path":"poisson-regression-for-count-data-or-rate-data.html","id":"estimates-and-intervals","chapter":"15 Poisson regression (for count data or rate data)","heading":"15.2 Estimates and Intervals","text":"Remember Poisson model fitting variance using Poisson Normal distribution. also relating predictors response variable \"log-link\" means need exponentiate estimates get scale response (y) variable. model estimates logn(y).Luckily tidy models broom can specify want put model predictions response variable scale specifying exponentiate = T remove log transformation, allow easy calculation confidence intervals.","code":"\nexp(coef(cuckoo_glm1)[1]) ### Intercept - Incidence rate at Mass=0, and Species = Cuckoo\n\nexp(coef(cuckoo_glm1)[2]) ### Change in the average incidence rate with Mass \n\nexp(coef(cuckoo_glm1)[3]) ### Change in the incidence rate intercept when Species = Warbler and Mass = 0\n \nexp(coef(cuckoo_glm1)[4]) ### The extra change in incidence rate for each unit increase in Mass when Species = Warbler (the interaction effect)## (Intercept) \n##    1.397207 \n##    Mass \n## 1.09949 \n## SpeciesWarbler \n##       1.963679 \n## Mass:SpeciesWarbler \n##           0.9785598\nbroom::tidy(cuckoo_glm1, \n            exponentiate = T, \n            conf.int = T)"},{"path":"poisson-regression-for-count-data-or-rate-data.html","id":"interpretation","chapter":"15 Poisson regression (for count data or rate data)","heading":"15.2.1 Interpretation","text":"important remember whether describing results log-link scale original scale. usually make sense provide answers original scale, means must first exponentiate relationship response predictors described writing results.\ndefault coefficients model summary log scale,\nadditive can added subtracted \n(just like ordinary least squares regression) work \nlog-estimates. exponentiate terms model get\nvalues ‘response’ scale, now changes ‘rate’ \nmultiplicative.\n\nAlso remember Poisson models ‘fixed variance’ z- \nChisquare distributions used instead t- F.\nexample wished infer relationship begging rates mass two species.hypothesised rate begging chicks increase body size increased. Interestingly found significant interaction effect mass species, Warbler chicks increased calling rate mass rate 0.98 [95%CI: 0.96-0.99] Cuckoo chicks (Poisson GLM: \\(\\chi^2\\)1,47 = 6.77, P = 0.009). meant hatching Warbler chicks start mean call rate higher parasitic brood mates, quickly reverses grow.","code":"\n# For a fixed  mean-variance model we use a Chisquare distribution\ndrop1(cuckoo_glm1, test = \"Chisq\")\n\n# emmeans can be another handy function - if you specify response then here it provideds the average call rate for each species, at the average value for any continuous measures - so here the average call rate for both species at an average body mass of 20.3\n\nemmeans::emmeans(cuckoo_glm1, specs = ~ Species:Mass, type = \"response\")##  Species Mass  rate    SE  df asymp.LCL asymp.UCL\n##  Cuckoo  20.3  9.61 0.884 Inf      8.03      11.5\n##  Warbler 20.3 12.15 0.658 Inf     10.93      13.5\n## \n## Confidence level used: 0.95 \n## Intervals are back-transformed from the log scale"},{"path":"poisson-regression-for-count-data-or-rate-data.html","id":"overdispersion","chapter":"15 Poisson regression (for count data or rate data)","heading":"15.3 Overdispersion","text":"one extra check need apply Poisson model overdispersionPoisson (binomial models) assume variance equal mean.However, residual deviance bigger residual degrees freedom variance expect prediction mean model.Overdispersion Poisson models can diagnosed \\(\\frac{residual~deviance}{residual~degrees~~freedom}\\) example 'summary()' \\(\\frac{436}{47} = 9.3\\)Overdispersion statistic values > 1 = OverdispersedOverdispersion result larger expected variance mean Poisson distribution, clearly issue model!Luckily simple fix fit quasi-likelihood model accounts , think \"quasi\" \"sort completely\" Poisson.can see, none estimates changed, standard errors (therefore confidence intervals) , accounts greater expected uncertainty saw deviance, applies cautious estimate uncertainty. interaction effect appears longer significant \\(\\alpha\\) = 0.05, now wider standard errors.\nnow estimating variance , test statistics\nreverted t distributions anova drop1 functions\nspecify F-test .\n","code":"\ncuckoo_glm2 <- glm(Beg ~ Mass+Species+Mass:Species, data=cuckoo, family=quasipoisson(link=\"log\"))\n\nsummary(cuckoo_glm2)## \n## Call:\n## glm(formula = Beg ~ Mass + Species + Mass:Species, family = quasipoisson(link = \"log\"), \n##     data = cuckoo)\n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)          0.33448    0.63129   0.530    0.599    \n## Mass                 0.09485    0.02018   4.700  2.3e-05 ***\n## SpeciesWarbler       0.67482    0.72043   0.937    0.354    \n## Mass:SpeciesWarbler -0.02167    0.02332  -0.930    0.357    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasipoisson family taken to be 7.7242)\n## \n##     Null deviance: 970.08  on 50  degrees of freedom\n## Residual deviance: 436.05  on 47  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 6"},{"path":"poisson-regression-for-count-data-or-rate-data.html","id":"activity-2-write-up","chapter":"15 Poisson regression (for count data or rate data)","heading":"15.4 Activity 2: Write-up","text":"used Poisson log-link Generalized Linear Model quasi-likelihoods account overdispersion analyse begging call rates Warbler Cuckoo chicks. species chick included categorical predictor mass included continuous predictor.initial model also included interaction term species mass, removed final model removal term significantly alter fit model (ANOVA).","code":""},{"path":"logistic-regression-for-binary-data.html","id":"logistic-regression-for-binary-data","chapter":"16 Logistic regression (for binary data)","heading":"16 Logistic regression (for binary data)","text":"response variable binary, can use glm binomial error distributionSo far considered continuous discrete data response variables. response categorical variable (e.g passing failing exam, voting yes referendum, whether egg successfully fledged predated, infected/uninfected, alive/dead)?can model probability \\(p\\) particular class function \nexplanatory variables.type binary data assumed follow Bernoulli distribution (special case Binomial) following characteristics:\\[\nY \\sim \\mathcal{Bern}(p)\n\\]Binary variable, taking values 0 1 (yes/, pass/fail).probability parameter \\(p\\), \\(0 < p < 1\\).Mean = \\(p\\)Variance = \\(p(1 - p)\\)Let us now place Gaussian (simple linear regression), Poisson logistic models next :\\[\n\\begin{aligned}\nY & \\sim \\mathcal{N}(\\mu, \\sigma^2) &&& Y  \\sim \\mathcal{Pois}(\\lambda) &&& Y  \\sim \\mathcal{Bern}(p)\\\\\n\\mu & = \\beta_0 + \\beta_1X &&& \\log{\\lambda} = \\beta_0 + \\beta_1X &&& ?? = \\beta_0 + \\beta_1X\n\\end{aligned}\n\\]Now need fill ?? appropriate term. Similar Poisson regression case,\nsimply model probabiliy \\(p = \\beta_0 + \\beta_1X\\), \\(p\\) negative.\n\\(\\log{p} = \\beta_0 + \\beta_1X\\) work either, \\(p\\) greater 1. Instead \nmodel log odds \\(\\log\\left(\\frac{p}{1 - p}\\right)\\) linear function. logistic regression model looks\nlike :\\[\n\\begin{aligned}\nY  & \\sim \\mathcal{Bern}(p)\\\\\n\\log\\left(\\frac{p}{1 - p}\\right) &  = \\beta_0 + \\beta_1 X\n\\end{aligned}\n\\], note still \"\" fitting straight lines data, time log odds space.\nshorthand notation write \\(\\log\\left(\\frac{p}{1 - p}\\right) = \\text{logit}(p) = \\beta_0 + \\beta_1 X\\).can also re-arrange equation get expression \\(p\\)\\[\np = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]Note \\(p\\) can vary 0 1.implement logistic regression model R, choose family=binomial(link=logit) (Bernoulli distribution special case Binomial distribution).Challenger DisasterIn 1985, NASA made decision send first civilian space.decision brought huge amount public attention STS-51-L mission, Challenger’s 25th trip space school teacher Christa McAuliffe’s 1st. afternoon January 28th, 1986 students around America tuned watch McAuliffe six astronauts launch Cape Canaveral, Florida. 73 seconds flight, shuttle experienced critical failure broke apart mid air, resulting deaths seven crewmembers: Christa McAuliffe, Dick Scobee, Judy Resnik, Ellison Onizuka, Ronald McNair, Gregory Jarvis, Michael Smith.investigation incident, discovered failure caused O-ring solid rocket booster. Additionally, revealed incident foreseeable.half worksheet discuss right statistical model predicted critical failure O-ring day.data columns :oring_tot: Total number orings flightoring_tot: Total number orings flightoring_dt : number orings failed flightoring_dt : number orings failed flighttemp: Outside temperature date flighttemp: Outside temperature date flightflight order flightsflight order flightsIt frequently discussed issue temperature might play role critical safety o-rings shuttles.\nOne biggest mistakes made assessing flight risk Challenger look flights failure occurredFrom concluded temperature appear affect o-ring risk failure, o-ring failures detected range different temperatures.However compare full data available different picture emerges.include flights without incident incident, can see clear relationship temperature risk o-ring failure. argued clear presentation data allowed even casual observer determine high risk disaster.However want understand actual relationship temperature risk several issues fitting linear model - data integer, bounded zero (model predicts negative failure rates within observed data range).consider suitable Poisson model - really interested determining binary risk flight o-ring failure vs. failure. implement GLM Binomial distribution.can use dplyr generate binary column incident '0' fail '1' anything >0.Fitting binary GLMSo now fitting following model\\[\nY \\sim Bern(p)\n\\]\n\\[\n\\log\\left[ \\frac { P( \\operatorname{oring\\_binary} = fail) }{ 1 - P( \\operatorname{oring\\_binary} = fail) } \\right] = \\beta_{0} + \\beta_{1}(\\operatorname{temp})\n\\]R look like :Intercept = \\(\\beta_{0}\\) = 23.77When temperature 0°F mean log-odds 23.77 [95%CI: 7.24- 58.19] failure incident O-ringsTemp = \\(\\beta_{1}\\) = -0.37 [95%CI: -0.87 - -0.12]every rise temperature 1°F, log-odds critical incident fall 0.37.","code":"glm(response ~ explanatory, family=binomial(link=logit))\nhead(challenger)\nchallenger %>% \n  filter(oring_dt > 0) %>% \nggplot(aes(y=oring_dt, x=temp))+geom_point()+\n  ggtitle(\"Temperature on flight launches where an O-ring incident occurred\")\nchallenger %>% \nggplot(aes(y=oring_dt, \n           x=temp))+\n  geom_point()+\n  geom_smooth(method=\"lm\")+\n  ggtitle(\"All launch data\")\nchallenger <- challenger %>% \n  mutate(oring_binary = ifelse(oring_dt =='0', 0, 1))\nbinary_model <- glm(oring_binary~temp, family=binomial, data=challenger)\nbinary_model %>% \n  broom::tidy(conf.int=T)\nbinary_model <- glm(oring_binary~temp, family=binomial, data=challenger)"},{"path":"logistic-regression-for-binary-data.html","id":"interpreting-model-coefficients","chapter":"16 Logistic regression (for binary data)","heading":"16.0.1 Interpreting model coefficients","text":"first consider dealing odds \\(\\frac{p}{1-p}\\) instead just \\(p\\). contain information, choice somewhat arbitrary, however ’ve using probabilities long feels unnatural switch odds. good reasons , however. Odds \\(\\frac{p}{1-p}\\) can take value 0 ∞ part translation \\(p\\) unrestricted domain already done (\\(P\\) restricted 0-1). Take look examples :use binomial model, produce 'log-odds', produces fully unconstrained linear regression anything less 1, can now occupy infinite negative value -∞ ∞.\\[\n\\log\\left(\\frac{p}{1-p}\\right)  =   \\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}\n\\]\\[\n\\frac{p}{1-p}   =   e^{\\beta_{0}}e^{\\beta_{1}x_{1}}e^{\\beta_{2}x_{2}}\n\\]can interpret \\(\\beta_{1}\\) \\(\\beta_{2}\\) increase log odds every unit increase \\(x_{1}\\) \\(x_{2}\\). alternatively interpret \\(\\beta_{1}\\) \\(\\beta_{2}\\) using notion one unit change \\(x_{1}\\) percent change \\(e^{\\beta_{1}}\\) odds. say, \\(e^{\\beta_{1}}\\) odds ratio change.want find Odds log-odds head ","code":"          Odds Verbiage"},{"path":"logistic-regression-for-binary-data.html","id":"probability","chapter":"16 Logistic regression (for binary data)","heading":"16.1 Probability","text":"powerful use logisitic regression prediction. Can predict probability event occuring using data?","code":""},{"path":"logistic-regression-for-binary-data.html","id":"activity-3-make-predictions","chapter":"16 Logistic regression (for binary data)","heading":"16.1.1 Activity 3: Make predictions","text":"Can use prediction functions predict() broom::augment() look models predictions O-ring failure data? Bonus points can convert log-odds probability?","code":"\npredict(binary_model, type = \"response\")##           1           2           3           4           5           6 \n## 0.394769696 0.130776531 0.178373050 0.238538593 0.311308842 0.067388639 \n##           7           8           9          10          11          12 \n## 0.047687994 0.130776531 0.946495574 0.662129014 0.130776531 0.007941202 \n##          13          14          15          16          17          18 \n## 0.311308842 0.987128767 0.311308842 0.023485304 0.130776531 0.002657203 \n##          19          20          21          22          23 \n## 0.016393905 0.005516836 0.023485304 0.016393905 0.924582363\nbroom::augment(binary_model, \n               type.predict = \"response\")"},{"path":"logistic-regression-for-binary-data.html","id":"emmeans-1","chapter":"16 Logistic regression (for binary data)","heading":"16.1.2 Emmeans","text":"use emmeans() function convert log-odds estimate probability o-ring failure mean value x (temperature). add confidence intervals!log-odds probability actually relate?equation work probability using exponent linear regression equation:\\[\nP(\\operatorname{risk failure }  X=69)\\left[ \\frac{e^{23.77+(-0.37 \\times 69.6)}}{1+e^{23.77+(-0.37 \\times 69.6)}} \\right]\n\\]produces following result can confirm risk o-ring failure average day 0.15","code":"\nemmeans::emmeans(binary_model, specs=~temp, type=\"response\")##  temp prob     SE  df asymp.LCL asymp.UCL\n##  69.6 0.15 0.0985 Inf    0.0373     0.445\n## \n## Confidence level used: 0.95 \n## Intervals are back-transformed from the logit scale\nodds_at_69.6 <- exp(coef(binary_model)[1]+coef(binary_model)[2]*69.6)\n# To convert from odds to a probability, divide the odds by one plus the odds\n\nprobability <-  odds_at_69.6/(1+odds_at_69.6)\nprobability## (Intercept) \n##   0.1483717"},{"path":"logistic-regression-for-binary-data.html","id":"changes-in-probability","chapter":"16 Logistic regression (for binary data)","heading":"16.1.3 Changes in probability","text":"useful rule--thumb can divide--four rule.can described maximum difference probability unit change \\(X\\) \\(\\beta/4\\).example maximum difference probability one degree change Temp \\(-0.37/4 = -0.09\\)maximum difference probability failure corresponding one degree change 9%Remember want augment data model, can use augment() function, remembering specify type.predict = \"response get probabilities o-ring failure (log odds).Annoyingly limitation augment function produce 95% CI predictions glms.Alternatively can also calculation via emmeans","code":"\nbroom::augment(binary_model, \n               type.predict=\"response\", \n               se_fit = T) %>% \n  head()\nemmeans::emmeans(binary_model, \n                 specs = ~ temp, \n                 at=list(temp=c(66:27)), \n                 type='response') ##  temp     prob         SE  df asymp.LCL asymp.UCL\n##    66 0.394770 0.17018097 Inf  0.139034  0.724865\n##    65 0.484853 0.19701707 Inf  0.167060  0.815387\n##    64 0.575932 0.21823406 Inf  0.190739  0.886694\n##    63 0.662129 0.22770706 Inf  0.210461  0.935096\n##    62 0.738753 0.22299102 Inf  0.227046  0.964568\n##    61 0.803166 0.20584013 Inf  0.241266  0.981259\n##    60 0.854818 0.18057075 Inf  0.253727  0.990288\n##    59 0.894693 0.15192302 Inf  0.264874  0.995033\n##    58 0.924582 0.12364996 Inf  0.275030  0.997482\n##    57 0.946496 0.09807106 Inf  0.284428  0.998731\n##    56 0.962301 0.07624709 Inf  0.293240  0.999364\n##    55 0.973568 0.05837488 Inf  0.301592  0.999682\n##    54 0.981533 0.04416262 Inf  0.309579  0.999841\n##    53 0.987129 0.03310075 Inf  0.317275  0.999921\n##    52 0.991045 0.02462725 Inf  0.324734  0.999961\n##    51 0.993777 0.01821445 Inf  0.332003  0.999981\n##    50 0.995679 0.01340624 Inf  0.339114  0.999990\n##    49 0.997001 0.00982751 Inf  0.346096  0.999995\n##    48 0.997920 0.00717951 Inf  0.352970  0.999998\n##    47 0.998558 0.00522962 Inf  0.359755  0.999999\n##    46 0.999000 0.00379955 Inf  0.366464  0.999999\n##    45 0.999307 0.00275431 Inf  0.373111  1.000000\n##    44 0.999520 0.00199257 Inf  0.379705  1.000000\n##    43 0.999667 0.00143888 Inf  0.386253  1.000000\n##    42 0.999769 0.00103733 Inf  0.392763  1.000000\n##    41 0.999840 0.00074671 Inf  0.399239  1.000000\n##    40 0.999889 0.00053676 Inf  0.405687  1.000000\n##    39 0.999923 0.00038534 Inf  0.412110  1.000000\n##    38 0.999947 0.00027631 Inf  0.418510  1.000000\n##    37 0.999963 0.00019791 Inf  0.424891  1.000000\n##    36 0.999974 0.00014160 Inf  0.431253  1.000000\n##    35 0.999982 0.00010122 Inf  0.437600  1.000000\n##    34 0.999988 0.00007228 Inf  0.443931  1.000000\n##    33 0.999992 0.00005158 Inf  0.450247  1.000000\n##    32 0.999994 0.00003677 Inf  0.456549  1.000000\n##    31 0.999996 0.00002619 Inf  0.462837  1.000000\n##    30 0.999997 0.00001865 Inf  0.469112  1.000000\n##    29 0.999998 0.00001327 Inf  0.475372  1.000000\n##    28 0.999999 0.00000943 Inf  0.481619  1.000000\n##    27 0.999999 0.00000670 Inf  0.487850  1.000000\n## \n## Confidence level used: 0.95 \n## Intervals are back-transformed from the logit scale\nemmeans::emmeans(binary_model, \n                 specs = ~ temp, \n                 at=list(temp=c(27:80)), \n                 type='response') %>% \n  as_tibble() %>% \n  ggplot(aes(x=temp, y=prob))+geom_line(aes(x=temp, y=prob))+\n  geom_ribbon(aes(ymin=asymp.LCL, ymax=asymp.UCL), alpha=0.2)"},{"path":"logistic-regression-for-binary-data.html","id":"predictions","chapter":"16 Logistic regression (for binary data)","heading":"16.2 Predictions","text":"day Challenger launched outside air temperature 36°FWe can use augment add model new data - make predictions risk o-ring failure","code":""},{"path":"logistic-regression-for-binary-data.html","id":"activity-4-more-predictions","chapter":"16 Logistic regression (for binary data)","heading":"16.2.1 Activity 4: More predictions","text":"First - make new dataset temperature day Challenger Launch 36°FWe can see fitted model, O-ring failure day Challenger launch predicted probability >0.999 [95%CI: 0.43-1]","code":"\n emmeans::emmeans(binary_model, \n                 specs = ~ temp, \n                 at = list(temp = 36),                  \n                 type='response')##  temp    prob        SE  df asymp.LCL asymp.UCL\n##    36 0.99997 0.0001416 Inf   0.43125         1\n## \n## Confidence level used: 0.95 \n## Intervals are back-transformed from the logit scale"},{"path":"logistic-regression-for-binary-data.html","id":"assumptions-1","chapter":"16 Logistic regression (for binary data)","heading":"16.3 Assumptions","text":"standard model checks plot() used binomial glms, usually mess!Luckily performance package detects alters checks. Pay particular attention binned residuals plot - best estimate possible overdispersion (requiring quasi-likelihood check). case likely enough data robust checks","code":"\nlibrary(arm)\n\nx <- predict(binary_model)\ny <- resid(binary_model)\narm::binnedplot(x,y)\nperformance::check_model(binary_model)"},{"path":"logistic-regression-for-binary-data.html","id":"activity-5-write-up","chapter":"16 Logistic regression (for binary data)","heading":"16.3.1 Activity 5: Write-up","text":"write findings analysis?Analysis: used Binomial logit-link Generalized Linear Model analyse effect temperature likelihood O-ring failure.analyses carried R (ver 4.1.3) (R Core Team 2021) following packages; tidyverse (Wickham et al 2019).Results: found significant negative relationship temperature probability o-ring failure (logit-odds = -0.37 [95%CI: -0.88 - -0.12], z = -2.1, d.f = 21, P = 0.036). average temperature 69.6°F probability o-ring failure estimated 0.15[0.03-0.45], rose near certainty failure 0.99[0.43-1] 36°F.Note use anova( test = \"Chisq\") , one, continuous variable, can also report directly summary table.","code":""},{"path":"logistic-regression-for-binary-data.html","id":"summary-9","chapter":"16 Logistic regression (for binary data)","heading":"16.4 Summary","text":"GLMs powerful flexible.can used fit wide variety data types.Model checking becomes trickier.Extensions include:mixed models;survival models;generalized additive models (GAMs).information Poisson Binomial GLMs check :https://bookdown.org/dereksonderegger/571/11-binomial-regression.html#confidence-intervals-1https://bookdown.org/ronsarafian/IntrotoDS/glm.html#poisson-regression","code":""},{"path":"logistic-regression-for-binary-data.html","id":"final-checklist","chapter":"16 Logistic regression (for binary data)","heading":"16.5 Final checklist","text":"Think carefully hypotheses test, use scientific knowledge background reading support thisThink carefully hypotheses test, use scientific knowledge background reading support thisImport, clean understand dataset: use data visuals investigate trends determine clear support hypothesesImport, clean understand dataset: use data visuals investigate trends determine clear support hypothesesDecide appropriate error structure model (Gaussian = continuous, Poisson = count, Binomial = binary)Decide appropriate error structure model (Gaussian = continuous, Poisson = count, Binomial = binary)Start canonical link functions, can altered neededStart canonical link functions, can altered neededFit (generalized) linear model, including interaction terms cautionFit (generalized) linear model, including interaction terms cautionInvestigate fit model, understand parameters may never perfect, classic patterns residuals may indicate poorly fitting model - sometimes can fixed careful consideration missing variables data transformationInvestigate fit model, understand parameters may never perfect, classic patterns residuals may indicate poorly fitting model - sometimes can fixed careful consideration missing variables data transformationCheck overdispersion models variance estimated independently mean (Poisson Binomial) - may use quasi-likelihood fittingCheck overdispersion models variance estimated independently mean (Poisson Binomial) - may use quasi-likelihood fittingTest removal interaction terms model, look AIC significance tests (Remember test = \"F\" Gaussian Quasilikelihood models, \"Chisq\" Poisson Binomial models)Test removal interaction terms model, look AIC significance tests (Remember test = \"F\" Gaussian Quasilikelihood models, \"Chisq\" Poisson Binomial models)Make sure understand output model summary, sense check graphs made - estimates need converting back original response scale?Make sure understand output model summary, sense check graphs made - estimates need converting back original response scale?direction size effects priority - produce estimates uncertainties. Make sure observations clear.direction size effects priority - produce estimates uncertainties. Make sure observations clear.Write-significance test results, taking care report just significance (required parts significance test). know report? Within complex model - reporting t/z indicate slope line single term intercept, F/Chi overall effect predictor across levels, post-hoc wish compare across levels.Write-significance test results, taking care report just significance (required parts significance test). know report? Within complex model - reporting t/z indicate slope line single term intercept, F/Chi overall effect predictor across levels, post-hoc wish compare across levels.Well described tables figures can enhance results sections - take time make sure informative attractive.Well described tables figures can enhance results sections - take time make sure informative attractive.","code":""},{"path":"foundations-of-mixed-modelling.html","id":"foundations-of-mixed-modelling","chapter":"17 Foundations of Mixed Modelling","heading":"17 Foundations of Mixed Modelling","text":"","code":""},{"path":"foundations-of-mixed-modelling.html","id":"what-is-a-mixed-model","chapter":"17 Foundations of Mixed Modelling","heading":"17.1 What is a mixed model?","text":"Mixed models (also known linear mixed models hierarchical linear models) statistical tests build simpler tests regression, t-tests ANOVA. tests special cases general linear model; fit straight line data explain variance systematic way.key difference linear mixed-effects model inclusion random effects - variables observations grouped subcategories systematically affect outcome - account important structure data.mixed-effects model can used many situations instead one straightforward tests structure may important. main advantages approach :mixed-effects models account variancemixed-effects models incorporate group even individual-level differencesmixed-effects models cope well missing data, unequal group sizes repeated measurements","code":""},{"path":"foundations-of-mixed-modelling.html","id":"fixed-vs-random-effects","chapter":"17 Foundations of Mixed Modelling","heading":"17.2 Fixed vs Random effects","text":"Fixed effects random effects terms commonly used mixed modeling, statistical framework combines order analyze data.mixed modeling, fixed effects used estimate overall relationship predictors response variable, random effects account within-group variability allow modeling individual differences group-specific effects.hierarchical structure data refers data organization observations nested within higher-level groups clusters. example, students nested within classrooms, patients nested within hospitals, employees nested within companies. hierarchical structure introduces dependencies correlations within data, observations within group tend similar observations different groups.need mixed models arises want account dependencies properly model variability different levels hierarchy. Traditional regression models, ordinary least squares (OLS), assume observations independent . However, working hierarchical data, assumption violated, ignoring hierarchical structure can lead biased inefficient estimates, incorrect standard errors, misleading inference.including random effects, mixed models allow estimation within-group -group variability. provide flexible framework modeling individual group-specific effects can capture heterogeneity within groups. Additionally, mixed models can handle unbalanced incomplete data, groups may different numbers observations.","code":""},{"path":"foundations-of-mixed-modelling.html","id":"fixed-effects","chapter":"17 Foundations of Mixed Modelling","heading":"17.2.1 Fixed effects","text":"broad terms, fixed effects variables expect affect dependent/response variable: ’re call explanatory variables standard linear regression.Fixed effects common random effects, least use. Fixed effects estimate different levels relationship assumed levels. example, model dependent variable body length fixed effect fish sex, get estimate mean body length males estimate females separately.can consider terms simple linear model, estimated intercept expected value outcome \\(y\\) predictor \\(x\\) value 0. estimated slope expected change \\(y\\) single unit change \\(x\\). parameters \"fixed\", meaning individual population expected value intercept slope.difference expected value true value called \"residual error\".\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]","code":""},{"path":"foundations-of-mixed-modelling.html","id":"examples","chapter":"17 Foundations of Mixed Modelling","heading":"17.2.1.1 Examples:","text":"Medical Research: clinical trial studying effectiveness different medications treating specific condition, fixed effects include categorical variables treatment group (e.g., medication , medication B, placebo) dosage level (e.g., low, medium, high). fixed effects capture systematic differences response variable (e.g., symptom improvement) due specific treatment received.Medical Research: clinical trial studying effectiveness different medications treating specific condition, fixed effects include categorical variables treatment group (e.g., medication , medication B, placebo) dosage level (e.g., low, medium, high). fixed effects capture systematic differences response variable (e.g., symptom improvement) due specific treatment received.Education Research: Suppose study examines impact teaching methods student performance different schools. fixed effects case might include variables school type (e.g., public, private), curriculum approach (e.g., traditional, progressive), classroom size. fixed effects help explain differences student achievement across schools, accounting systematic effects factors.Education Research: Suppose study examines impact teaching methods student performance different schools. fixed effects case might include variables school type (e.g., public, private), curriculum approach (e.g., traditional, progressive), classroom size. fixed effects help explain differences student achievement across schools, accounting systematic effects factors.Environmental Science: Imagine study investigating factors influencing bird species richness across different habitats. fixed effects context include variables habitat type (e.g., forest, grassland, wetland), habitat disturbance level (e.g., low, medium, high), geographical region. fixed effects capture systematic variations bird species richness associated specific habitat characteristics.Environmental Science: Imagine study investigating factors influencing bird species richness across different habitats. fixed effects context include variables habitat type (e.g., forest, grassland, wetland), habitat disturbance level (e.g., low, medium, high), geographical region. fixed effects capture systematic variations bird species richness associated specific habitat characteristics.Fixed effects default effects learn begin understand statistical concepts, fixed effects default effects functions like lm() aov().","code":""},{"path":"foundations-of-mixed-modelling.html","id":"random-effects","chapter":"17 Foundations of Mixed Modelling","heading":"17.2.2 Random effects","text":"Random effects less commonly used perhaps widely encountered nature. level can considered random variable underlying process distribution random effect.random effect parameter allowed vary across groups individuals. Random effects take single fixed value, rather follow distribution (usually normal distribution). Random effects can added model account variation around intercept slope. individual group gets estimated random effect, representing adjustment mean.random effects usually grouping factors trying control. always categorical, can’t force R treat continuous variable random effect. lot time specifically interested impact response variable, know might influencing patterns see.","code":""},{"path":"foundations-of-mixed-modelling.html","id":"examples-1","chapter":"17 Foundations of Mixed Modelling","heading":"17.2.2.1 Examples:","text":"Longitudinal Health Study: Consider study tracking blood pressure individuals multiple time points. case, random effect can included account individual-specific variation blood pressure. individual's blood pressure measurements time treated repeated measures within individual, random effect capture variability individuals explained fixed effects. random effect allows modeling inherent individual differences blood pressure levels.Longitudinal Health Study: Consider study tracking blood pressure individuals multiple time points. case, random effect can included account individual-specific variation blood pressure. individual's blood pressure measurements time treated repeated measures within individual, random effect capture variability individuals explained fixed effects. random effect allows modeling inherent individual differences blood pressure levels.Social Network Analysis: Suppose study examines influence peer groups adolescent behavior. study may collect data individual behaviors within schools, students nested within classrooms. scenario, random effect can incorporated classroom level account shared social environment within classroom. random effect captures variability behavior among classrooms accounted fixed effects, enabling study analyze effects individual-level classroom-level factors simultaneously.Social Network Analysis: Suppose study examines influence peer groups adolescent behavior. study may collect data individual behaviors within schools, students nested within classrooms. scenario, random effect can incorporated classroom level account shared social environment within classroom. random effect captures variability behavior among classrooms accounted fixed effects, enabling study analyze effects individual-level classroom-level factors simultaneously.Ecological Study: Imagine research project investigating effect environmental factors species abundance different study sites. study sites may geographically dispersed, random effect can included account variation study sites. random effect captures unexplained heterogeneity species abundance across different sites, allowing examination effects environmental variables accounting site-specific differences.Ecological Study: Imagine research project investigating effect environmental factors species abundance different study sites. study sites may geographically dispersed, random effect can included account variation study sites. random effect captures unexplained heterogeneity species abundance across different sites, allowing examination effects environmental variables accounting site-specific differences.random effect \\(U_i\\) often assumed follow normal distribution mean zero variance estimated model fitting process.\\[Y_i = β_0 + U_j + ε_i\\]\nbook “Data analysis using regression \nmultilevel/hierarchical models” (Gelman & Hill (2006)). authors examined five\ndefinitions fixed random effects found consistent\nagreement.\n\n\nFixed effects constant across individuals, random effects\nvary. example, growth study, model random intercepts a_i\nfixed slope b corresponds parallel lines different\nindividuals , model y_it = a_i + b t thus distinguish \nfixed random coefficients.\n\n\nFixed effects constant across individuals, random effects\nvary. example, growth study, model random intercepts a_i\nfixed slope b corresponds parallel lines different\nindividuals , model y_it = a_i + b t thus distinguish \nfixed random coefficients.\n\n\nEffects fixed interesting random\ninterest underlying population.\n\n\nEffects fixed interesting random\ninterest underlying population.\n\n\n“sample exhausts population, corresponding\nvariable fixed; sample small (.e., negligible) part \npopulation corresponding variable random.”\n\n\n“sample exhausts population, corresponding\nvariable fixed; sample small (.e., negligible) part \npopulation corresponding variable random.”\n\n\n“effect assumed realized value random\nvariable, called random effect.”\n\n\n“effect assumed realized value random\nvariable, called random effect.”\n\n\nFixed effects estimated using least squares (, \ngenerally, maximum likelihood) random effects estimated \nshrinkage.\n\n\nFixed effects estimated using least squares (, \ngenerally, maximum likelihood) random effects estimated \nshrinkage.\n\nThus turns fixed random effects born made. \nmust make decision treat variable fixed random \nparticular analysis.\ndetermining fixed random effect study, consider trying ? trying make predictions ? just variation (.k.“noise”) need control ?\nrandom effects:\n\nNote golden rule generally want random\neffect least five levels. , instance, wanted \ncontrol effects fish sex body length, fit sex (\ntwo level factor: male female) fixed, random, effect.\n\n, put simply, estimating variance data points\nimprecise. Mathematically , wouldn’t lot\nconfidence . two three levels, model\nstruggle partition variance - give output,\nnecessarily one can trust.\n\nFinally, keep mind name random doesn’t much \nmathematical randomness. Yes, ’s confusing. Just think \ngrouping variables now. Strictly speaking ’s \nmaking models representative questions getting better\nestimates. Hopefully, next examples help make sense \n’re used.\n\n’s firm rule minimum number factor levels\nrandom effect really can use factor \ntwo levels. However commonly reported may want\nfive factor levels random effect order really\nbenefit random effect can (though argue even\n, 10 levels). Another case may want random\neffect don’t want factor levels inform \ndon’t assume factor levels come common\ndistribution. noted , male female factor \ntwo levels oftentimes want male female information\nestimated separately ’re necessarily assuming males \nfemales come population sexes infinite\nnumber ’re interested average.\n","code":""},{"path":"foundations-of-mixed-modelling.html","id":"why-use-mixed-models","chapter":"17 Foundations of Mixed Modelling","heading":"17.3 Why use mixed models?","text":"provided code generates dataset suitable testing mixed models. break code annotate step:section creates data frame called rand_eff containing random effects. consists five levels grouping variable (group), level, generates random effects (b0 b1) using rnorm function.section creates main dataset (data) testing mixed models. uses expand.grid create combination levels grouping variable (group) observation variable (obs). performs left join rand_eff data frame, matching group variable incorporate random effects group.code continues mutate dataset adding additional variables:x random predictor variable generated using runif values 0 10.x random predictor variable generated using runif values 0 10.B0 B1 represent fixed effects intercept slope predetermined values 20 2, respectively.B0 B1 represent fixed effects intercept slope predetermined values 20 2, respectively.E represents error term, generated using rnorm mean 0 standard deviation 10.E represents error term, generated using rnorm mean 0 standard deviation 10.Finally, y created response variable using linear model equation includes fixed effects (B0 B1), random effects (b0 b1), predictor variable (x), error term (E).Finally, y created response variable using linear model equation includes fixed effects (B0 B1), random effects (b0 b1), predictor variable (x), error term (E).section creates additional dataset (data.1) specific group (group = 5) smaller number observations (obs = 30) testing purposes. appended original dataset, see effect smaller group within random effects discuss partial pooling shrinkage later .Now three variables consider models: x, y group (five levels).Now suitable simulated dataset, start modelling!","code":"\n# Generating a fake dataset with different means for each group\nset.seed(123)  # Setting seed for reproducibility\n\n\nrand_eff <- data.frame(group = as.factor(seq(1:5)),\n            b0 = rnorm(5, mean = 0, sd = 20),\n            b1 = rnorm(5, 0, 0.5))\ndata <- expand.grid(group = as.factor(seq(1:10)), \n                    obs = as.factor(seq(1:100))) %>%\n  left_join(rand_eff,\n            by = \"group\") %>%\n  mutate(x = runif(n = nrow(.), 0, 10),\n         B0 = 20,\n         B1 = 2,\n         E = rnorm(n = nrow(.), 0, 10)) %>%\n  mutate(y = B0 + b0 + x * (B1 + b1) + E)\n\ndata <- expand.grid(group = as.factor(seq(1:4)), \n                    obs = as.factor(seq(1:100)))\ndata.1 <- expand.grid(group = as.factor(5),\n          obs = as.factor(seq(1:30)))\n\ndata <- bind_rows(data, data.1) %>% \n  left_join(rand_eff,\n            by = \"group\") %>%\n  mutate(x = runif(n = nrow(.), 0, 10),\n         B0 = 20,\n         B1 = 2,\n         E = rnorm(n = nrow(.), 0, 10)) %>%\n  mutate(y = B0 + b0 + x * (B1 + b1) + E)\ndata %>% \n  select(x, y, group, obs) %>% \n  head()"},{"path":"foundations-of-mixed-modelling.html","id":"all-in-one-model","chapter":"17 Foundations of Mixed Modelling","heading":"17.3.1 All in one model","text":"begin highlighting importance considering data structure hierarchy building linear models. illustrate , delve example showcases consequences ignoring underlying data structure. might naively construct single linear model ignores group-level variation treats observations independent. oversimplified approach fails account fact observations within groups similar due shared characteristics.\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]can see basic linear model produced statistically significant regression analysis (t428 = 9.034, p <0.001) \\(R^2\\) 0.16. medium effect positive relationship changes x y (Estimate = 2.58, S.E. = 0.29).can see clearly produce simple plot x y:\nFigure 1.2: Simple scatter plot x y\nuse function geom_smooth() scatter plot, plot also includes fitted regression line obtained using \"lm\" method. allows us examine overall trend potential linear association variables.\nFigure 17.1: Scatter plot displaying relationship independent variable dependent variable. points represent observed data, fitted regression line represents linear relationship variables. plot helps visualize trend potential association variables.\ncheck_model() function performance package (Lüdecke et al. (2023)) used evaluate performance diagnostic measures statistical model. provides comprehensive assessment model's fit, assumptions, predictive capabilities. calling function, can obtain summary various evaluation metrics diagnostic plots specified model.enables identify potential issues, violations assumptions, influential data points, lack fit, can affect interpretation reliability model's resultsLooking fit model tempted conclude accurate robust model.However, data hierarchically structured, individuals nested within groups, typically correlation similarity among observations within group. accounting clustering effect, estimates derived single model can biased inefficient. assumption independence among observations violated, leading incorrect standard errors inflated significance levels.figure can see difference median range x values within groups:\nFigure 17.2: Linear model conducted data\nfigure, colour tag data points group, can useful determining mixed model appropriate.:colour-coding data points based grouping variable, plot allows visually assess within-group -group variability. noticeable differences data patterns dispersion among groups, suggests data may hierarchical structure, observations within group similar observations groups.plots, confirms observations within ranges aren’t independent. can’t ignore : ’re starting see, lead completely erroneous conclusion.","code":"\nbasic_model <- lm(y ~ x, data = data)\nsummary(basic_model)## \n## Call:\n## lm(formula = y ~ x, data = data)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -37.23 -12.11  -2.36  11.00  44.53 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  21.0546     1.6490  12.768   <2e-16 ***\n## x             2.5782     0.2854   9.034   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 16.96 on 428 degrees of freedom\n## Multiple R-squared:  0.1602, Adjusted R-squared:  0.1582 \n## F-statistic: 81.62 on 1 and 428 DF,  p-value: < 2.2e-16\nplot(data$x, data$y)\nggplot(data, aes(x = x, \n                 y = y)) +\n  geom_point() +\n  labs(x = \"Independent Variable\", \n       y = \"Dependent Variable\")+\n  geom_smooth(method = \"lm\")\nperformance::check_model(basic_model)\nggplot(data, aes(x = group, \n                 y = y)) +\n  geom_boxplot() +\n  labs(x = \"Groups\", \n       y = \"Dependent Variable\")\n# Color tagged by group\nplot_group <- ggplot(data, aes(x = x, \n                               y = y, \n                               color = group,\n                               group = group)) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"Data Coloured by Group\", \n       x = \"Independent Variable\", \n       y = \"Dependent Variable\")+\n  theme(legend.position=\"none\")\n\nplot_group"},{"path":"foundations-of-mixed-modelling.html","id":"multiple-analyses-approach","chapter":"17 Foundations of Mixed Modelling","heading":"17.3.2 Multiple analyses approach","text":"Running separate linear models per group, also known stratified analysis, can feasible approach certain situations. However, several drawbacks includingIncreased complexityIncreased complexityInability draw direct conclusions overall variabilityInability draw direct conclusions overall variabilityReduced statistical powerReduced statistical powerInflated Type 1 error riskInflated Type 1 error riskInconsistent estimatesInconsistent estimatesLimited ability handle unbalanced/missing dataLimited ability handle unbalanced/missing data\nFigure 17.3: Scatter plot showing relationship independent variable (x) dependent variable (y) colored group. subplot represents different group. line represents group-level linear regression smoothing.\ncode , dataset data first grouped variable 'group' using group_by function, data within group nested using nest function. results new dataset nested_data group's data stored nested tibble.Next, linear regression model (lm) fit nested data group using map function. broom::tidy function applied model using map extract model summary statistics, coefficients, p-values, standard errors. resulting models stored models object.bind_rows function used combine model results single data frame called combined_models. data frame filtered include rows predictor 'x' using filter function, resulting filtered_models data frame.add column group index, rowid_to_column function applied filtered_models data frame, creating group_indexed_models data frame additional column named 'group'.Finally, p-values group_indexed_models data frame modified using custom function report_p","code":"\n# Plotting the relationship between x and y with group-level smoothing\nggplot(data, aes(x = x, y = y, color = group, group = group)) +\n  geom_point(alpha = 0.6) +  # Scatter plot of x and y with transparency\n  labs(title = \"Data Colored by Group\", x = \"Independent Variable\", y = \"Dependent Variable\") +\n  theme(legend.position = \"none\") +\n  geom_smooth(method = \"lm\") +  # Group-level linear regression smoothing\n  facet_wrap(~group)  # Faceting the plot by group\n# Creating nested data by grouping the data by 'group'\nnested_data <- data %>%\n  group_by(group) %>%\n  nest()\n\n# Fitting linear regression models to each nested data group\nmodels <- map(nested_data$data, ~ lm(y ~ x, data = .)) %>% \n          map(broom::tidy)\n\n# Combining the model results into a single data frame\ncombined_models <- bind_rows(models)\n\n# Filtering the rows to include only the 'x' predictor\nfiltered_models <- combined_models %>%\n                   filter(term == \"x\")\n\n# Adding a column for the group index using rowid_to_column function\ngroup_indexed_models <- filtered_models %>%\n                        rowid_to_column(\"group\")\n\n# Modifying the p-values using a custom function report_p\nfinal_models <- group_indexed_models %>%\n                mutate(p.value = report_p(p.value))\n\nfinal_models\n report_p <- function(p, digits = 3) {\n     reported <- if_else(p < 0.001,\n             \"p < 0.001\",\n             paste(\"p=\", round(p, digits)))\n     \n     return(reported)\n }"},{"path":"foundations-of-mixed-modelling.html","id":"complex-model","chapter":"17 Foundations of Mixed Modelling","heading":"17.3.3 Complex model","text":"Using group level term interaction x fixed effect means explicitly including interaction term x group predictor model equation. approach assumes relationship x outcome variable differs across groups differences constant fixed. implies group unique intercept (baseline level) slope (effect size) relationship x outcome variable. treating group level term fixed effect, model estimates specific parameter values group.explicitly interested outcomes differences individual group (wish account ) - may best option can lead overfitting uses lot degrees freedom - impacting estimates widening confidence intervals. running multiple models , limited ability make inferences outside observed groups, handle missing data unbalanced designs well.\\[Y_i = \\beta_0 + \\beta_1X_i + \\beta_2.group_i+\\beta_3(X_i.group_i)+\\epsilon_i\\]","code":"\nadditive_model <- lm(y ~ x*group, data = data)\n\nsummary(additive_model)## \n## Call:\n## lm(formula = y ~ x * group, data = data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -24.8614  -6.2579   0.2044   6.9342  28.5474 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   6.8125     1.8881   3.608 0.000346 ***\n## x             3.0644     0.3379   9.070  < 2e-16 ***\n## group2        6.4526     2.7289   2.365 0.018505 *  \n## group3       44.8356     2.8539  15.710  < 2e-16 ***\n## group4       15.8607     2.7184   5.835 1.08e-08 ***\n## group5       22.9090     4.0772   5.619 3.51e-08 ***\n## x:group2     -0.5481     0.4875  -1.124 0.261560    \n## x:group3     -1.6739     0.4781  -3.501 0.000513 ***\n## x:group4     -1.3787     0.4830  -2.855 0.004522 ** \n## x:group5     -3.1400     0.7479  -4.198 3.28e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.801 on 420 degrees of freedom\n## Multiple R-squared:  0.7246, Adjusted R-squared:  0.7187 \n## F-statistic: 122.8 on 9 and 420 DF,  p-value: < 2.2e-16"},{"path":"foundations-of-mixed-modelling.html","id":"our-first-mixed-model","chapter":"17 Foundations of Mixed Modelling","heading":"17.4 Our first mixed model","text":"mixed model good choice : allow us use data (higher sample size) account correlations data coming groups. also estimate fewer parameters avoid problems multiple comparisons encounter using separate regressions.can now join random effect \\(U_j\\) full dataset define y values \\[Y_{ij} = β_0 + β_1*X_{ij} + U_j + ε_{ij}\\].response variable, attempting explain part variation test score fitting independent variable fixed effect. response variable residual variation (.e. unexplained variation) associated group. using random effects, modeling unexplained variation variance.now want know association y ~ x exists controlling variation group.","code":""},{"path":"foundations-of-mixed-modelling.html","id":"running-mixed-effects-models-with-lmertest","chapter":"17 Foundations of Mixed Modelling","heading":"17.4.1 Running mixed effects models with lmerTest","text":"section detail run mixed models lmer function R package lmerTest (Kuznetsova et al. (2020)). builds older lme4 (Bates et al. (2023)) package, particular add p-values previously included. R packages can used run mixed-effects models including nlme package (Pinheiro et al. (2023)) glmmTMB package (Brooks et al. (2023)). Outside R also packages software capable running mixed-effects models, though arguably none better supported R software.groups clearly explain lot varianceSo differences groups explain ~67% variance ’s “left ” variance explained fixed effects.","code":"\nplot_function2 <- function(model, title = \"Data Coloured by Group\"){\n  \ndata <- data %>% \n  mutate(fit.m = predict(model, re.form = NA),\n         fit.c = predict(model, re.form = NULL))\n\ndata %>%\n  ggplot(aes(x = x, y = y, col = group)) +\n  geom_point(pch = 16, alpha = 0.6) +\n  geom_line(aes(y = fit.c, col = group), linewidth = 2)  +\n  coord_cartesian(ylim = c(-40, 100))+\n  labs(title = title, \n       x = \"Independent Variable\", \n       y = \"Dependent Variable\") \n}\n\nmixed_model <- lmer(y ~ x + (1|group), data = data)\n\nplot_function2(mixed_model, \"Random intercept\")\n# random intercept model\nmixed_model <- lmer(y ~ x + (1|group), data = data)\n\nsummary(mixed_model)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: y ~ x + (1 | group)\n##    Data: data\n## \n## REML criterion at convergence: 3224.4\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.61968 -0.63654 -0.03584  0.66113  3.13597 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  group    (Intercept) 205      14.32   \n##  Residual             101      10.05   \n## Number of obs: 430, groups:  group, 5\n## \n## Fixed effects:\n##             Estimate Std. Error       df t value Pr(>|t|)    \n## (Intercept)  23.2692     6.4818   4.1570    3.59   0.0215 *  \n## x             2.0271     0.1703 424.0815   11.90   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##   (Intr)\n## x -0.131\n205/(205 + 101) = 0.669 / 66.9%\n"},{"path":"foundations-of-mixed-modelling.html","id":"partial-pooling","chapter":"17 Foundations of Mixed Modelling","heading":"17.4.2 Partial pooling","text":"worth noting random effect estimates function group-level information overall (grand) mean random effect. Group levels low sample size poor information (.e., strong relationship) strongly influenced grand mean, adds information otherwise poorly-estimated group. However, group large sample size strong information (.e., strong relationship) little influence grand mean largely reflect information contained entirely within group. process called partial pooling (opposed pooling, effect considered, total pooling, separate models run different groups). Partial pooling results phenomenon known shrinkage, refers group-level estimates shrunk toward mean. mean? use random effect, prepared factor levels influence overall mean levels. good, clear signal group, won’t see much impact overall mean, small groups without much signal.can take look estimates standard errors three previously constructed models:Pooling helps improve precision estimates borrowing strength entire dataset. However, can also lead differences estimates standard errors compared models without pooling.Pooled: pooled model averaged estimates may accurately reflect true values within group. result, estimates pooled models can biased towards average behavior across groups. can see small standard error intercept, underestimating variance dataset. time substantial variability relationships groups, pooled estimates can less precise. increased variability across groups can contribute larger standard errors difference (SED) fixed effects pooled models.Pooled: pooled model averaged estimates may accurately reflect true values within group. result, estimates pooled models can biased towards average behavior across groups. can see small standard error intercept, underestimating variance dataset. time substantial variability relationships groups, pooled estimates can less precise. increased variability across groups can contribute larger standard errors difference (SED) fixed effects pooled models.pooling: model extremely precise smallest errors, however estimates reflect conditions first group modelNo pooling: model extremely precise smallest errors, however estimates reflect conditions first group modelPartial pooling/Mixed models: model reflects greater uncertainty Mean SE intercept. However, SED partial pooling model accounts variability within groups uncertainty groups. Compared pooling approach, SED partial pooling model tends smaller incorporates pooled information, reduces overall uncertainty. adjusted SED provides accurate measure uncertainty associated estimated differences groups fixed effects.Partial pooling/Mixed models: model reflects greater uncertainty Mean SE intercept. However, SED partial pooling model accounts variability within groups uncertainty groups. Compared pooling approach, SED partial pooling model tends smaller incorporates pooled information, reduces overall uncertainty. adjusted SED provides accurate measure uncertainty associated estimated differences groups fixed effects.","code":"\npooled <- basic_model %>% \n  broom::tidy() %>% \n  mutate(Approach = \"Pooled\", .before = 1) %>% \n  select(term, estimate, std.error, Approach)\n\nno_pool <- additive_model %>% \n  broom::tidy() %>% \n  mutate(Approach = \"No Pooling\", .before = 1) %>% \n  select(term, estimate, std.error, Approach)\n\npartial_pool <- mixed_model %>% \n  broom.mixed::tidy() %>% \n  mutate(Approach = \"Mixed Model/Partial Pool\", .before = 1) %>% \n  select(Approach, term, estimate, std.error)\n\nbind_rows(pooled, no_pool, partial_pool) %>% \n  filter(term %in% c(\"x\" , \"(Intercept)\") )"},{"path":"foundations-of-mixed-modelling.html","id":"predictions-1","chapter":"17 Foundations of Mixed Modelling","heading":"17.5 Predictions","text":"One misconception mixed-effects models produce estimates relationships group.?can use coef() function extract estimates (strictly predictions) random effects. output several components.function produces 'best linear unbiased predictions' (BLUPs) intercept slope regression site. predictions given different get ran individual models site, BLUPs product compromise complete-pooling -pooling models. Now predicted intercept influenced sites leading process called 'shrinkage'.called predictions estimates? estimated variance site essentially borrowed information across sites improve accuracy, combine fixed effects. strictest sense predicting relationships rather direct observation.generous ability make predictions one main advantages mixed-model.summary() function already provided estimates fixed effects, can also extracted fixef() function.can also apply anova() single model get F-test fixed effect","code":"\ncoef(mixed_model)## $group\n##   (Intercept)        x\n## 1    11.82356 2.027066\n## 2    15.68146 2.027066\n## 3    47.94678 2.027066\n## 4    21.01028 2.027066\n## 5    19.88385 2.027066\n## \n## attr(,\"class\")\n## [1] \"coef.mer\"\nfixef(mixed_model)## (Intercept)           x \n##   23.269187    2.027066\nanova(mixed_model)"},{"path":"foundations-of-mixed-modelling.html","id":"shrinkage-in-mixed-models","chapter":"17 Foundations of Mixed Modelling","heading":"17.5.1 Shrinkage in mixed models","text":"graph demonstrates compromise complete pooling pooling. plots overall regression line/mean (fixed effects lmer model), predicted slopes site mixed-effects model, compares estimates site (nested lm).can see groups show shrinkage, deviate less overall mean, obviously group 5, sample size deliberately reduced. can see predicted line much closer overall regression line, reflecting greater uncertainty. slope drawn towards overall mean shrinkage.\nFigure 5.3: Regression relationships fixed-effects mixed effects models, note shrinkage group 5\nre.form = NA: re.form set NA, indicates random effects ignored prediction. means prediction based solely fixed effects model, ignoring variation introduced random effects. useful interested estimating overall trend relationship described fixed effects, without considering specific random effects individual groups levels.re.form = NULL: Conversely, re.form set NULL, indicates random effects included prediction. means prediction take account fixed effects random effects associated levels random effect variable. model use estimated random effects generate predictions account variation introduced random effects. useful want visualize analyze variation response variable explained different levels random effect.always easy/straightforward make prediciton confidence intervals complex general generalized linear mixed models, luckily excellent R packages us.","code":"\n# Nesting the data by group\nnested_data <- data %>% \n    group_by(group) %>% \n    nest()\n\n# Fitting linear regression models and obtaining predictions for each group\nnested_models <- map(nested_data$data, ~ lm(y ~ x, data = .)) %>% \n    map(predict)\n# Creating a new dataframe and adding predictions from different models\ndata1 <- data %>% \n  mutate(fit.m = predict(mixed_model, re.form = NA),\n         fit.c = predict(mixed_model, re.form = NULL)) %>% \n  arrange(group,obs) %>% \n  mutate(fit.l = unlist(nested_models)) \n\n# Creating a plot to visualize the predictions\ndata1 %>% \n  ggplot(aes(x = x, y = y, colour = group)) +\n    geom_point(pch = 16) + \n  geom_line(aes(y = fit.l, linetype = \"lm\"), colour = \"black\")+\n  geom_line(aes(y = fit.c, linetype = \"lmer\"))+ \n  geom_line(aes(y = fit.m, linetype = \"Mean\"), colour = \"grey\")+\n   scale_linetype_manual(name = \"Model Type\", \n                        labels = c(\"Mean\", \"lmer\", \"lm\"),\n                        values = c(\"dotdash\", \"solid\", \"dashed\"))+\n  facet_wrap( ~ group)+\n  guides(colour = \"none\")"},{"path":"foundations-of-mixed-modelling.html","id":"ggeffects","chapter":"17 Foundations of Mixed Modelling","heading":"17.5.2 ggeffects","text":"ggeffects (Lüdecke (2023a)) light-weight package aims easily calculating marginal effects adjusted predictions","code":""},{"path":"foundations-of-mixed-modelling.html","id":"ggpredict","chapter":"17 Foundations of Mixed Modelling","heading":"17.5.2.1 ggpredict","text":"argument type = random ggpredict function (ggeffects package Lüdecke (2023a)) used specify type predictions generated context mixed effects models. main difference using ggpredict without type = random lies type predictions produced:Without type = random: ggpredict generate fixed-effects predictions. estimates based solely fixed effects model, ignoring variability associated random effects. resulting estimate represent average expected values response variable specific combinations predictor values, considering fixed components model.Estimated mean fixed effects provide comprehensive view average effect dependent variables response variable. plotting estimated mean fixed effects using ggpredict, can visualize response variable changes across different levels values predictors considering effects variables model.type = random: ggpredict generate predictions incorporate fixed random effects. predictions take account variability introduced random effects model. resulting predictions reflect average trend captured fixed effects also additional variability associated random effects different levels grouping factor(s).default figure now produces prediciton intervals\nConfidence Intervals: confidence interval used estimate \nrange plausible values population parameter, mean\nregression coefficient, based sample population.\nprovides range within true population parameter likely\nfall certain level confidence. example, 95%\nconfidence interval implies repeat sampling\nprocess many times, 95% resulting intervals contain \ntrue population parameter.\n\nPrediction Intervals: hand, prediction interval \nused estimate range plausible values individual\nobservation future observation population. takes \naccount uncertainty estimated model parameters \ninherent variability within data. 95% prediction interval provides\nrange within specific observed predicted value likely \nfall certain level confidence. wider prediction\ninterval, greater uncertainty around specific value \npredicted.\nFurthermore, ggpredict() enables explore group-level predictions, provide deeper understanding response variable varies across different levels grouping variable. specifying desired grouping variable ggpredict type = random, can generate plots depict predicted values group separately, allowing comparative analysis group-level effects.","code":"\nlibrary(ggeffects)\n\nggpredict(mixed_model, terms = \"x\") %>% \nplot(., add.data = TRUE)\nggpredict(mixed_model, terms = \"x\", type = \"random\") %>% \nplot(., add.data = TRUE)\nggpredict(mixed_model, terms = c(\"x\", \"group\"), type = \"random\") %>% \nplot(., add.data = TRUE) + \n  facet_wrap(~group)+\n  theme(legend.position = \"none\")"},{"path":"foundations-of-mixed-modelling.html","id":"sjplot","chapter":"17 Foundations of Mixed Modelling","heading":"17.5.3 sjPlot","text":"Another way visualise mixed model results package sjPlot(Lüdecke (2023b)), interested showing variation among levels random effects, can plot departure overall model estimate intercepts - slopes, random slope model:\nvalues see actual values, rather difference\ngeneral intercept slope value found model summary\nestimate specific level random/fixed effect.\nsjPlot also capable producing prediction plots way ggeffects:","code":"\nlibrary(sjPlot)\n\nplot_model(mixed_model, terms = c(\"x\", \"group\"), type = \"re\")/\n  (plot_model(mixed_model, terms = c(\"x\", \"group\"), type = \"est\")+ggtitle(\"Fixed Effects\"))\nplot_model(mixed_model,type=\"pred\",\n           terms=c(\"x\", \"group\"),\n           pred.type=\"re\",\n           show.data = T)+\n  facet_wrap( ~ group)"},{"path":"foundations-of-mixed-modelling.html","id":"checking-model-assumptions","chapter":"17 Foundations of Mixed Modelling","heading":"17.6 Checking model assumptions","text":"need just conscious testing assumptions mixed effects models . assumptions :Within-group errors independent normally distributed mean zero variance \\(\\sigma^2\\)Within-group errors independent normally distributed mean zero variance \\(\\sigma^2\\)Within-group errors independent random effects.Within-group errors independent random effects.Random effects normally distributed mean zero.Random effects normally distributed mean zero.Random effects independent different groups, except specified nesting (later)Random effects independent different groups, except specified nesting (later)Several model check plots help us confirm/deny assumptions, note QQ-plots may relevant model structure. Two commonly-used plots :can often useful check distribution residuals groups (e.g. blocks) check assumptions 1 2. can plotting residuals fitted values, separately level random effect, using coplot() function:sub-figure plot, refers individual group, doesn’t contain much data. can hard judge whether spread residuals around fitted values group observations low.can check assumption 3 histogram (levels , can assured):Overlaying random distribution intercept regression line produces plot like :\nFigure 6.2: Marginal fit, heavy black line random effect model histogram distribution conditional intercepts\n","code":"\nplot(mixed_model) \nqqnorm(resid(mixed_model))\nqqline(resid(mixed_model)) \ncoplot(resid(mixed_model) ~ fitted(mixed_model) | data$group)\nrand_dist <- as.data.frame(ranef(mixed_model)) %>% \n  mutate(group = grp,\n         b0_hat = condval,\n         intercept_cond = b0_hat + summary(mixed_model)$coef[1,1],\n         .keep = \"none\")\n\nhist(rand_dist$b0_hat)\ndata1 %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(pch = 16, col = \"grey\") +\n  geom_violinhalf(data = rand_dist, \n                  aes(x = 0, y = intercept_cond), \n                  trim = FALSE, \n                  width = 4, \n                  adjust = 2, \n                  fill = NA)+\n  geom_line(aes(y = fit.m), linewidth = 2)  +\n  coord_cartesian(ylim = c(-40, 100))+\n  labs(x = \"Independent Variable\", \n       y = \"Dependent Variable\")"},{"path":"foundations-of-mixed-modelling.html","id":"performance","chapter":"17 Foundations of Mixed Modelling","heading":"17.6.1 performance","text":"check_model() function performance package R useful tool evaluating performance assumptions statistical model, particularly context mixed models. provides comprehensive set diagnostics visualizations assess model's fit, identify potential issues, verify assumptions underlying model, can difficult complex models","code":"\nlibrary(performance)\ncheck_model(mixed_model)"},{"path":"foundations-of-mixed-modelling.html","id":"dharma","chapter":"17 Foundations of Mixed Modelling","heading":"17.6.2 DHARma","text":"Simulation-based methods, like available DHARMa (Hartig (2022)), often preferred model validation assumption checking offer flexibility rely specific assumptions. Simulation particularly useful evaluating complex models multiple levels variability, non-linearity, complex hierarchical structures. models may adequately evaluated solely examining residuals, simulation provides robust approach assess assumptions performance.Read authors summary \nlot data, even minimal deviations \nexpected distribution become significant (discussed \nhelp/vignette DHARMa package). need assess \ndistribution decide important .\n","code":"\nlibrary(DHARMa)\n\nresid.mm <- simulateResiduals(mixed_model)\n\nplot(resid.mm)"},{"path":"foundations-of-mixed-modelling.html","id":"practice-questions","chapter":"17 Foundations of Mixed Modelling","heading":"17.7 Practice Questions","text":"mixed-effects models, independent variables also called:Random EffectsFixed EffectsMediatorsVarianceA random effect best described ?following advantage mixed-effects models?Mixed-effects models cope well missing data ?","code":""},{"path":"worked-example-1---dolphins.html","id":"worked-example-1---dolphins","chapter":"18 Worked Example 1 - Dolphins","heading":"18 Worked Example 1 - Dolphins","text":"","code":""},{"path":"worked-example-1---dolphins.html","id":"how-do-i-decide-if-it-is-a-fixed-or-random-effect","chapter":"18 Worked Example 1 - Dolphins","heading":"18.1 How do I decide if it is a fixed or random effect?","text":"One common questions mixed-effects modelling decide effect considered fixed random. can quite complicated question, touched upon briefly definition random effect universal. considered process can include hypothesis question asking.1 ) directly interested effect question. answer yes fixed effect.2 ) variable continuous? answer yes fixed effect.3 ) variable less five levels? ther answer yes fixed effect.","code":""},{"path":"worked-example-1---dolphins.html","id":"dolphins","chapter":"18 Worked Example 1 - Dolphins","heading":"18.2 Dolphins","text":"dataset collected measure resting lung function 32 bottlenose dolphins. main dependent variable tidal volume (\\(V_T\\)) measured litres, index lung capacity.interested relationship (\\(V_T\\)) body mass (kg), measurements taken breath breath , dolphin observed one four times.need determine fixed effects, random effects model structure:Body Mass Random EffectFixed EffectBody Mass Random EffectFixed EffectDirection Random EffectFixed EffectDirection Random EffectFixed EffectAnimal 1) Body Mass Random EffectFixed EffectAnimal 1) Body Mass Random EffectFixed EffectWith basic structure y ~ x + z + (1|group) think model ?:basic structure y ~ x + z + (1|group) think model ?:clearly interested effect body mass (\\(V_T\\)) fixed effect.clearly interested effect body mass (\\(V_T\\)) fixed effect.may think relationship (\\(V_T\\)) body mass may different breath. may directly interested , fewer fivel levels fixed effect. (outbreath coded 1, inbreath coded 2).may think relationship (\\(V_T\\)) body mass may different breath. may directly interested , fewer fivel levels fixed effect. (outbreath coded 1, inbreath coded 2).Individual dolphins - averaged across measurements dolphin, measurement precision different animal. include data point, double-counting animals observations independent. account multiple observations treat animal random effect.Individual dolphins - averaged across measurements dolphin, measurement precision different animal. include data point, double-counting animals observations independent. account multiple observations treat animal random effect.basic linear model place - carry model fit checks DHARMa performance::check_model(), assuming good fit can look interpretation:\nFigure 18.1: Scatter plot VT function body mass dolphins. Different directions breath represented different colors. solid lines indicate marginal fitted values model.\nfitted linear mixed model (estimated using REML nloptwrap optimizer) predict (\\(V_T\\)) bodymass(kg) direction (/breath). 95% Confidence Intervals (CIs) p-values computed using Wald t-distribution approximation. included random intercept effect animal account repeated measurements (1 4 observations) across total 32 bottlenosed dolphins.found every 1kg increase bodymass, (\\(V_T\\)) increased 0.02 litres (95% CI [0.01 - 0.02]), . inbreath average higher volume outbreath (1.11 litre difference [0.71 - 1.52]..Q. perfect write-, else consider including?","code":"\ndolphins <- read_csv(\"files/dolphins.csv\") \n\ndolphins$direction <- factor(dolphins$direction)\n\ndolphins <- drop_na(dolphins)\ndolphmod <- lmer(vt ~ bodymass + direction + (1|animal), data=dolphins)\nsummary(dolphmod)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: vt ~ bodymass + direction + (1 | animal)\n##    Data: dolphins\n## \n## REML criterion at convergence: 387.4\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.30795 -0.51983  0.04156  0.62404  2.26396 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  animal   (Intercept) 1.039    1.019   \n##  Residual             1.158    1.076   \n## Number of obs: 112, groups:  animal, 31\n## \n## Fixed effects:\n##              Estimate Std. Error        df t value Pr(>|t|)    \n## (Intercept)  2.226398   0.627115 28.758081   3.550  0.00135 ** \n## bodymass     0.016782   0.003259 26.720390   5.150  2.1e-05 ***\n## direction2   1.114821   0.203389 77.414421   5.481  5.1e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##            (Intr) bdymss\n## bodymass   -0.927       \n## direction2 -0.162  0.000\ndolphins.1 <- dolphins %>% \n    mutate(fit.m = predict(dolphmod, re.form = NA),\n           fit.c = predict(dolphmod, re.form = NULL))\ndolphins.1 %>%\n  ggplot(aes(x = bodymass, y = vt, group = direction)) +\n  geom_point(pch = 16, aes(colour = direction)) +\n  geom_line(aes(y = fit.m, \n                linetype = direction), \n            linewidth = 1)  +\n  labs(x = \"Body Mass\", \n       y = \"VT\") \nplot_model(dolphmod,type=\"pred\",\n           terms=c(\"bodymass\", \"direction\"),\n           pred.type=\"fe\",\n           show.data = T)"},{"path":"multiple-random-effects.html","id":"multiple-random-effects","chapter":"19 Multiple random effects","heading":"19 Multiple random effects","text":"Previously used (1|group) fit random effect. Whatever right side | operator factor referred “grouping factor” term.","code":""},{"path":"multiple-random-effects.html","id":"crossed-or-nested","chapter":"19 Multiple random effects","heading":"19.0.1 Crossed or Nested","text":"one possible random effect can crossed nested - depends relationship variables. Let’s look.common issue causes confusion issue specifying random effects either ‘crossed’ ‘nested’. reality, way specify random effects determined experimental sampling design. simple example can illustrate difference.","code":""},{"path":"multiple-random-effects.html","id":"crossed-random-effects","chapter":"19 Multiple random effects","heading":"19.0.1.1 1. Crossed Random Effects:","text":"Crossed random effects occur levels two grouping variables crossed independent . case, grouping variables unrelated, combination levels represented data.\nFigure 19.1: Fully Crossed\nExample 1: consider study examining academic performance students different schools different cities. grouping variables \"School\" \"City\". school can located multiple cities, city can multiple schools. random effects \"School\" \"City\" crossed since levels variables independent .lmer(y ~ x + (1 | School) + (1 | City), data = dataset)Example 2: Imagine long-term study breeding success passerine birds across multiple woodlands, researcher returns every year five years carry measurements. \"Year\" crossed random effect \"Woodland\" Woodland can appear multiple years study.\nImagine researcher interested understanding factors affecting clutch mass passerine bird. study population spread across five separate woodlands, containing 30 nest boxes. Every week breeding measure foraging rate females feeders, measure subsequent clutch mass. females multiple clutches season contribute multiple data points.lmer(y ~ x + (1 | Year) + (1 | Woodland), data = dataset)","code":""},{"path":"multiple-random-effects.html","id":"nested-random-effects","chapter":"19 Multiple random effects","heading":"19.0.1.2 2. Nested Random Effects:","text":"Nested random effects occur levels one grouping variable completely nested within levels another grouping variable. words, levels one variable uniquely exclusively associated specific levels another variable.\nFigure 19.2: Fully Nested\nExample 1. Consider study job performance employees within different departments organization. grouping variables \"Employee\" \"Department\". employee belongs one specific department, employee can part multiple departments. random effects \"Employee\" nested within random effects \"Department\" since employee uniquely associated specific department.lmer(y ~ x + (1 | Department/Employee), data = dataset)Example 2: bird study female ID said nested within woodland : woodland contains multiple females unique woodland (never move among woodlands). nested random effect controls fact () clutches female independent, (ii) females woodland may clutch masses similar one another females woodlandslmer(y ~ x + (1 | Woodland/Female ID), data = dataset)remember year model crossed nested random effectslmer(y ~ x + (1 | Woodland/Female ID) + (1|Year), data = dataset)designing models around crossed nested designs check amazing nature paper Krzywinski et al. (2014)","code":""},{"path":"worked-example-2---nested-design.html","id":"worked-example-2---nested-design","chapter":"20 Worked Example 2 - Nested design","heading":"20 Worked Example 2 - Nested design","text":"experiment involved simple one-way anova 3 treatments given 6 rats, researchers measured glycogen levels liver. analysis complicated fact three preparations taken liver rat, two readings glycogen content taken preparation. generated 6 pseudoreplicates per rat give total 36 readings .Clearly, mistake analyse data straightforward one-way anova, give us 33 degrees freedom error. fact, since two rats treatment, one degree freedom per treatment, giving total 3 d.f. error.variance likely different level nested analysis :readings differ variation glycogen detection method within liver sample (measurement error);\npieces liver may differ heterogeneity distribution glycogen within liver single rat;\nrats differ one another glycogen levels sex, age, size, genotype, etc.;\nrats allocated different experimental treatments may differ result fixed effects treatment.\nwant test whether experimental treatments affected glycogen levels, interested liver bits within rat’s livers, preparations within liver bits. combine pseudoreplicates together, analyse 6 averages. virtue showing tiny experiment really . latter approach also ignores nested sources uncertainties. Instead use linear mixed model.Q. model wrongIt wrong add two random effect terms, one rat 1 one rat 2. fact 6 rats altogether. way data coded allows kinds mistakes happen. true Liver, coded 1, 2 3. means write thinking including correct random effects Rat Liver. fact, assumes data come crossed design, 2 rats 3 parts liver, Liver = 1 corresponds type measurement rats 1 2 . Sometimes appropriate, !nature way many data sets coded makes kinds mistakes easy make!better design one :can see effects figures :","code":"\nrats <- readRDS(\"rats.rds\")\nrats %>% \n  aggregate(Glycogen ~ Rat + Treatment + Liver, data = ., mean)   Glycogen Rat Treatment Liver\n1       131   1         1     1\n2       130   1         1     1\n3       131   1         1     2\n4       125   1         1     2\n5       136   1         1     3\n6       142   1         1     3\n7       150   2         1     1\n8       148   2         1     1\n9       140   2         1     2\n10      143   2         1     2\n11      160   2         1     3\n12      150   2         1     3\n13      157   3         2     1\n14      145   3         2     1\n15      154   3         2     2\n16      142   3         2     2\n17      147   3         2     3\n18      153   3         2     3\n19      151   4         2     1\n20      155   4         2     1\n21      147   4         2     2\n22      147   4         2     2\n23      162   4         2     3\n24      152   4         2     3\n25      134   5         3     1\n26      125   5         3     1\n27      138   5         3     2\n28      138   5         3     2\n29      135   5         3     3\n30      136   5         3     3\n31      138   6         3     1\n32      140   6         3     1\n33      139   6         3     2\n34      138   6         3     2\n35      134   6         3     3\n36      127   6         3     3\n\nrats_lmer.1 <- lmer(Glycogen ~ Treatment + (1 | Rat) + (1 | Liver), data = rats)\nsummary(rats_lmer.1)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: Glycogen ~ Treatment + (1 | Rat) + (1 | Liver)\n##    Data: rats\n## \n## REML criterion at convergence: 221.9\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -1.79334 -0.66536  0.01792  0.59281  2.05206 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  Rat      (Intercept) 39.187   6.260   \n##  Liver    (Intercept)  2.168   1.472   \n##  Residual             30.766   5.547   \n## Number of obs: 36, groups:  Rat, 6; Liver, 3\n## \n## Fixed effects:\n##             Estimate Std. Error      df t value Pr(>|t|)    \n## (Intercept)  140.500      4.783   3.174  29.373 5.65e-05 ***\n## Treatment2    10.500      6.657   3.000   1.577    0.213    \n## Treatment3    -5.333      6.657   3.000  -0.801    0.482    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##            (Intr) Trtmn2\n## Treatment2 -0.696       \n## Treatment3 -0.696  0.500\nrats_lmer.2 <- lmer(Glycogen ~ Treatment + (1 | Rat / Liver), data = rats)\nsummary(rats_lmer.2)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: Glycogen ~ Treatment + (1 | Rat/Liver)\n##    Data: rats\n## \n## REML criterion at convergence: 219.6\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -1.48212 -0.47263  0.03062  0.42934  1.82935 \n## \n## Random effects:\n##  Groups    Name        Variance Std.Dev.\n##  Liver:Rat (Intercept) 14.17    3.764   \n##  Rat       (Intercept) 36.06    6.005   \n##  Residual              21.17    4.601   \n## Number of obs: 36, groups:  Liver:Rat, 18; Rat, 6\n## \n## Fixed effects:\n##             Estimate Std. Error      df t value Pr(>|t|)    \n## (Intercept)  140.500      4.707   3.000  29.848 8.26e-05 ***\n## Treatment2    10.500      6.657   3.000   1.577    0.213    \n## Treatment3    -5.333      6.657   3.000  -0.801    0.482    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##            (Intr) Trtmn2\n## Treatment2 -0.707       \n## Treatment3 -0.707  0.500\nplot(ggpredict(rats_lmer.2, terms = c(\"Treatment\")))\nplot(ggpredict(rats_lmer.2, \n               terms = c(\"Treatment\", \"Rat\"), \n               type = \"random\"), \n     add.data = TRUE)"},{"path":"types-of-random-effects.html","id":"types-of-random-effects","chapter":"21 Types of Random Effects","heading":"21 Types of Random Effects","text":"","code":""},{"path":"types-of-random-effects.html","id":"random-slopes","chapter":"21 Types of Random Effects","heading":"21.1 Random slopes","text":"far looked random effects group intercept. means fit regression line across five groups, constant slope allowed shift group. represented panel B.alternative random intercepts model random slopes model, shown panel C. model slopes permitted vary, intercept fixed across groups. current data random slopes model fair job well.Finally can allow intercepts slope vary groups, shown panel D. captures shallower slope groups vertical offsets present. inspect original data set generated, can see random effect group calculated product random distribution intercept slope, surprising model fits data best.model use degrees freedom two, must used calculate variance intercept slope.Mixed-effects models enormously flexible. decision whether include random intercepts, random slopes, depend heavily hypotheses. generally quite rare see random slopes models , commonly question whether random intercepts random intercepts random slopes necessary.combine random slopes intercept model requries degrees freedom, may case using likelihood ratio tests (LRT) advisable decide model describes data best (later).random intercepts models require fewer degrees freedome, may easier fit data sets fewer observations.\nrandom effects model, random effects assumed \nrandom sample population possible random effects. random\neffects capture variation different groups clusters \ndata. number random effects typically smaller \ntotal number observations, random effects represent \ndistinct groups clusters dataset.\n\ndegrees freedom associated random effects linear\nmixed model reflect number independent groups clusters \ndata, rather number individual observations. example, \ndata 100 individuals, belong 10 distinct\ngroups, random effects 10 degrees freedom, \n100.\n\nusing fewer degrees freedom random effects, model\naccounts fact group-level variation estimated based\nsmaller number parameters. approach helps prevent\noverfitting provides appropriate estimation variance\ncomponents associated random effects.\n","code":"\n# random intercept model\nlmer1 <- lmer(y ~ x + (1|group), data = data)\n\nplot_function(lmer1, \"Random intercept\")\n\n# Random slope model\n\nlmer2 <- lmer(y ~ x + (0 + x | group), data = data)\n\nplot_function(lmer2, \"Random slope\")\n\n# Random slope and intercept model\n\nlmer3 <- lmer(y ~ x + (x | group), data = data)"},{"path":"types-of-random-effects.html","id":"model-refining-likelihood-ratio-tests","chapter":"21 Types of Random Effects","heading":"21.2 Model refining / Likelihood Ratio Tests","text":"produce initial model random effects structure, may wish perform model selection comparing different nested models varying random effects structures. allows us assess whether inclusion additional random effects changes random effects structure significantly improve model fit. comparing likelihood values different nested models, can determine model provides better fit data.Previously looked random intercepts vs. random intercepts slopes model concluded latter looked like fit data best.can use likelihood ratio test anova() function want determine fit significantly different simpler randome intercepts model.\nliterature idea model selection, , \nautomated (sometimes manual) way testing many versions model\ndifferent subset predictors attempt find \nmodel fits best. sometimes called “stepwise”\nprocedures.\n\nOthers argue method number flaws, including:\n\n\nbasically “p-value mining”, , running lot\ntests till find p-value like.\n\n\nbasically “p-value mining”, , running lot\ntests till find p-value like.\n\n\nlikelihood making false positive high.\n\n\nlikelihood making false positive high.\n\n\nAdding/removing new variable can effect \npredictors.\n\n\nAdding/removing new variable can effect \npredictors.\n\nInstead model selection, use knowledge \ndata select subset variables either ) \nimportance , b) theoretically influential outcome. \ncan fit single model including .\n\nHowever, argue inclusion random effects \nstructure clear rationale implementation,\nadjustments better understand best type random effects\nstructure perfectly reasonable.\nFrequentist fit used LMM lme4 / lmer based Maximum Likelihood principle, maximize likelihood \\(L(y)\\) observing data \\(y\\), equivalent minimizing residuals model, Ordinary Least Squares (OLS) approach. measures probability observing data given specific set parameter values.attempting optimise model can use likelihood ratio test (LRT). Given two nested models, denoted Model 1 Model 2, LRT compares likelihood values models assess whether complex Model 2 provides significantly better fit data compared simpler Model 1. LRT statistic, denoted \\(D\\), calculated difference log-likelihood values Model 1 Model 2, multiplied 2:\\[D = -2~*~(ln(L_1)-ln(L_2))\\]\\(L_1\\) represents likelihood value Model 1, \\(L_2\\) represents likelihood value Model 2. LRT statistic follows chi-square (\\(\\chi^2\\)) distribution degrees freedom equal difference number parameters two models.determine statistical significance LRT statistic, one can compare critical value chi-square distribution appropriate degrees freedom. LRT statistic exceeds critical value, indicates complex Model 2 provides significantly better fit data compared simpler Model 1.ML estimation often used perform hypothesis tests, including chi-square test. chi-square test compares observed data expected data predicted statistical model. assesses goodness--fit observed data model's predictions.REML (Restricted Maximum Likelihood) estimation variant ML estimation addresses issue bias estimation random effects mixed effects models. mixed effects models, random effects account variation group individual level explained fixed effects. However, inclusion random effects introduces bias ML estimates, influenced variability random effects.REML estimation addresses bias optimizing likelihood function conditional fixed effects , effectively removing influence random effects estimation. approach provides unbiased estimates fixed effects especially useful primary interest lies fixed effects rather random effects.Maximum Likelihood (ML) estimation preferable comparing nested models allows direct comparison likelihood values different models. ML estimation provides quantitative measure well given model fits observed data, based likelihood function.context, ML estimation preferable allows formal statistical comparison nested models. provides rigorous objective way assess whether inclusion additional parameters complex model leads significantly better fit data compared simpler model. approach ensures model comparisons based sound statistical principles helps determining appropriate model given data.REML can preferable producing unbiased estimates fixed effects. Older versions model fitting packages like lmer used require manual switch REML ML fitting models order switch objectives assessing goodness--fit interpreting estimates. notice perform LRT anova() function informs switch made automatically.","code":"\nanova(lmer1, lmer3)"},{"path":"types-of-random-effects.html","id":"maximum-likelihood","chapter":"21 Types of Random Effects","heading":"21.2.1 Maximum Likelihood","text":"Frequentist fit used LMM lme4 / lmer based Maximum Likelihood principle, maximize likelihood \\(L(y)\\) observing data \\(y\\), equivalent minimizing residuals model, Ordinary Least Squares (OLS) approach. measures probability observing data given specific set parameter values.attempting optimise model can use likelihood ratio test (LRT). Given two nested models, denoted Model 1 Model 2, LRT compares likelihood values models assess whether complex Model 2 provides significantly better fit data compared simpler Model 1. LRT statistic, denoted \\(D\\), calculated difference log-likelihood values Model 1 Model 2, multiplied 2:\\[D = -2~*~(ln(L_1)-ln(L_2))\\]\\(L_1\\) represents likelihood value Model 1, \\(L_2\\) represents likelihood value Model 2. LRT statistic follows chi-square (\\(\\chi^2\\)) distribution degrees freedom equal difference number parameters two models.determine statistical significance LRT statistic, one can compare critical value chi-square distribution appropriate degrees freedom. LRT statistic exceeds critical value, indicates complex Model 2 provides significantly better fit data compared simpler Model 1.ML estimation often used perform hypothesis tests, including chi-square test. chi-square test compares observed data expected data predicted statistical model. assesses goodness--fit observed data model's predictions.","code":""},{"path":"types-of-random-effects.html","id":"reml","chapter":"21 Types of Random Effects","heading":"21.2.2 REML","text":"REML (Restricted Maximum Likelihood) estimation variant ML estimation addresses issue bias estimation random effects mixed effects models. mixed effects models, random effects account variation group individual level explained fixed effects. However, inclusion random effects introduces bias ML estimates, influenced variability random effects.REML estimation addresses bias optimizing likelihood function conditional fixed effects , effectively removing influence random effects estimation. approach provides unbiased estimates fixed effects especially useful primary interest lies fixed effects rather random effects.","code":""},{"path":"types-of-random-effects.html","id":"ml-vs.-reml-fitting","chapter":"21 Types of Random Effects","heading":"21.2.3 ML vs. REML fitting","text":"Maximum Likelihood (ML) estimation preferable comparing nested models allows direct comparison likelihood values different models. ML estimation provides quantitative measure well given model fits observed data, based likelihood function.context, ML estimation preferable allows formal statistical comparison nested models. provides rigorous objective way assess whether inclusion additional parameters complex model leads significantly better fit data compared simpler model. approach ensures model comparisons based sound statistical principles helps determining appropriate model given data.REML can preferable producing unbiased estimates fixed effects. Older versions model fitting packages like lmer used require manual switch REML ML fitting models order switch objectives assessing goodness--fit interpreting estimates. notice perform LRT anova() function informs switch made automatically.","code":""},{"path":"types-of-random-effects.html","id":"practice-questions-1","chapter":"21 Types of Random Effects","heading":"21.3 Practice Questions","text":"formula y ~ x + (x|group) fit model \n\nRandom SlopesRandom InterceptsRandom Slopes InterceptsAn intercept fixed 0\nformula fit random intercepts model?\n\ny ~ x + (x|group)y ~ x + (1|group)y ~ x + (y | group)\n","code":""},{"path":"worked-example-3---sleep-study.html","id":"worked-example-3---sleep-study","chapter":"22 Worked Example 3 - Sleep study","heading":"22 Worked Example 3 - Sleep study","text":"chapter, working real data study looking effects sleep deprivation psychomotor performance Belenky et al. (2003). dataset sleepstudy built-lme4 package.Access documentation typing ??sleepstudy consoleA good way start every analysis plot data.model data appropriately, first need know design - according study went follows:first 3 days (T1, T2 B) adaptation training (T1 T2) baseline (B) subjects required bed 23:00 07:00 h [8 h required time bed (TIB)]. third day (B), baseline measures taken. Beginning fourth day continuing total 7 days (E1–E7) subjects one four sleep conditions [9 h required TIB (22:00–07:00 h), 7 h required TIB (24:00–07:00 h), 5 h required TIB (02:00–07:00 h), 3 h required TIB (04:00–07:00 h)], effectively one sleep augmentation condition, three sleep restriction conditions.seven nights sleep restriction, first night restriction occurring third day. first two days, coded 0, 1, adaptation training. day coded 2, baseline measurement taken, place start analysis. include days 0 1 analysis, might bias results, since changes performance first two days training, sleep restriction.Take moment think might model relationship days_deprived Reaction. reaction time increase decrease increasing sleep deprivation? relationship roughly stable change time?reasonable model Model 2 - different patients may different initial reaction times & may respond sleep deprivation differently.can compare simpler intercept model check fitQ. LRT indicate effect random slopeon model?remove model? YesNoThe LRT indicates removing random slope design model explains significantly less variance, left model.","code":"\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9) +\n  facet_wrap(~Subject)\nsleep2 <- sleepstudy %>%\n  filter(Days >= 2L) %>%\n  mutate(days_deprived = Days - 2L)\n\nhead(sleep2)\nsleep_model <- lmer(Reaction ~ days_deprived + (days_deprived | Subject), data = sleep2)\nsummary(sleep_model)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: Reaction ~ days_deprived + (days_deprived | Subject)\n##    Data: sleep2\n## \n## REML criterion at convergence: 1404.1\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -4.0157 -0.3541  0.0069  0.4681  5.0732 \n## \n## Random effects:\n##  Groups   Name          Variance Std.Dev. Corr\n##  Subject  (Intercept)   958.35   30.957       \n##           days_deprived  45.78    6.766   0.18\n##  Residual               651.60   25.526       \n## Number of obs: 144, groups:  Subject, 18\n## \n## Fixed effects:\n##               Estimate Std. Error      df t value Pr(>|t|)    \n## (Intercept)    267.967      8.266  17.000  32.418  < 2e-16 ***\n## days_deprived   11.435      1.845  16.999   6.197 9.75e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr)\n## days_deprvd -0.062\nsleep_model_intercept <- lmer(Reaction ~ days_deprived + (1| Subject), data = sleep2)\n\nanova(sleep_model_intercept, sleep_model)"},{"path":"reporting-mixed-model-results.html","id":"reporting-mixed-model-results","chapter":"23 Reporting Mixed Model Results","heading":"23 Reporting Mixed Model Results","text":"get model, present accurate, clear attractive form.summary() function provides us useful numbers amount variance left fitting fixed effects can assigned random effects. also provides information correlated random effects , may interest understanding structure data well.can use check modelled random effect structure matches data.includes coefficient estimates, t-statistics p-values fixed effects\nNote t values appear df p value using\nlmerTest version lmer(). degrees freedom\nmixed-effects model well-defined. Often people treat\nWald z- values, .e., observations standard normal\ndistribution. t-distribution asymptotes standard normal\ndistribution number observations goes infinity, \n“t--z” practice legitimate large enough set \nobservations.\n\nlmerTest function produces estimated degrees freedom \nsatterthwaite approximation (another common method “kenward\napproximation”), effect widening confidence\nintervals. important know method using, \nconsistent across presentation/write-\ncan also produce anova() type summary fixed effectsWe may also wish report \\(R^2\\) values model fit. requires MuMIn package.calculates two values first \\(R^2_{m}\\) marginal \\(R^2\\) value, representing proportion variance explained fixed effects. second \\(R^2_{c}\\) conditional \\(R^2\\), proportion variance explained full model, fixed random effects.","code":"\nsummary(sleep_model)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: Reaction ~ days_deprived + (days_deprived | Subject)\n##    Data: sleep2\n## \n## REML criterion at convergence: 1404.1\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -4.0157 -0.3541  0.0069  0.4681  5.0732 \n## \n## Random effects:\n##  Groups   Name          Variance Std.Dev. Corr\n##  Subject  (Intercept)   958.35   30.957       \n##           days_deprived  45.78    6.766   0.18\n##  Residual               651.60   25.526       \n## Number of obs: 144, groups:  Subject, 18\n## \n## Fixed effects:\n##               Estimate Std. Error      df t value Pr(>|t|)    \n## (Intercept)    267.967      8.266  17.000  32.418  < 2e-16 ***\n## days_deprived   11.435      1.845  16.999   6.197 9.75e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr)\n## days_deprvd -0.062\nanova(sleep_model)\nlibrary(MuMIn)\nr.squaredGLMM(sleep_model)##            R2m       R2c\n## [1,] 0.2055824 0.8062353"},{"path":"reporting-mixed-model-results.html","id":"tables","chapter":"23 Reporting Mixed Model Results","heading":"23.1 Tables","text":"packages producing beautiful summary tables regression models, can handle mixed-effects models, sjPlot Lüdecke (2023b) package one robust produces simple HTML table detailing fixed random effects, produces 95% confidence intervals fixed effects calculates \\(R^2\\) values :","code":"\nsjPlot::tab_model(sleep_model, \n                  df.method = \"satterthwaite\")"},{"path":"reporting-mixed-model-results.html","id":"figures","chapter":"23 Reporting Mixed Model Results","heading":"23.2 Figures","text":"covered packages allow us easily produce marginal conditional fits, along 95% confidence prediction intervals: output ggplot2 figures, allowing plenty customisation presentation","code":"\n# Custom colors\ncustom.col <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\ncols <- colorRampPalette(custom.col)(18)\n\n\nggpredict(sleep_model, terms = c(\"days_deprived\", \"Subject\" ), \n          type = \"random\") %>% \nplot(., add.data = TRUE) + \n  scale_color_manual(values = cols)+\n  facet_wrap(~group)+theme(legend.position = \"none\")"},{"path":"reporting-mixed-model-results.html","id":"write-ups","chapter":"23 Reporting Mixed Model Results","heading":"23.3 Write-ups","text":"fitted linear mixed model (estimated using REML) predict reaction time (ms) days sleep deprivation. \nmodel included random slope intercept design allow repeated measure design measuring patients multiple days effect o sleep deprivation vary patient. model's total explanatory power substantial (conditional R^2 = 0.81), part related fixed effects alone (marginal R2) 0.21.\nmean reaction time Day 0 267.97ms (95% CI[250.53, 285.41], t(17) = 32.42, p < .001). effect sleep deprivation steady increase reaction time 11.44ms per day sleep loss ([7.54 - 15.33], t(17) = 6.2, p < 0.001).\n95% Confidence Intervals (CIs) p-values computed using satterthwaite approximation degrees freedom t-distribution approximation.","code":""},{"path":"worked-example-4.html","id":"worked-example-4","chapter":"24 Worked Example 4","heading":"24 Worked Example 4","text":"microbiologist wishes know four growth media best rearing large populations anthrax, quickly. However, poorly funded scientist large enough incubator grow lots replicate populations. Instead requests space five different incubators owned , better-funded researchers. incubator just space four bottles medium. scientist allocates growth medium one bottle per incubator random, inoculates anthrax monitors population growth rate.data available :\nCan produce suitable linear mixed model analysis data,\nanswer question “four growth media best rearing\nlarge populations anthrax?”\n","code":"\nbacteria <- readRDS(\"bacCabinets.rds\")"},{"path":"summary-10.html","id":"summary-10","chapter":"25 Summary","heading":"25 Summary","text":"suggested workflow:Start model containing fixed effects: Begin fitting model fixed effects relevant research question. Include main predictors potential interactions hypothesize might exist. example, predictors B, might start model like response ~ + B + :B.Start model containing fixed effects: Begin fitting model fixed effects relevant research question. Include main predictors potential interactions hypothesize might exist. example, predictors B, might start model like response ~ + B + :B.Assess fixed effects interactions: Evaluate significance, direction, magnitude fixed effects coefficients. Look interactions show significant effects consider interpretation context research question. step allows identify key variables interactions important explaining variation response variable.Assess fixed effects interactions: Evaluate significance, direction, magnitude fixed effects coefficients. Look interactions show significant effects consider interpretation context research question. step allows identify key variables interactions important explaining variation response variable.Model evaluation refinement: Assess goodness fit fixed effects model using appropriate measures like AIC, BIC, model deviance. Consider conducting model comparison evaluate different models alternative fixed effects structures. process helps refine model select appropriate combination variables interactions. However - substitution carefully considered hypotheses experimental design.Model evaluation refinement: Assess goodness fit fixed effects model using appropriate measures like AIC, BIC, model deviance. Consider conducting model comparison evaluate different models alternative fixed effects structures. process helps refine model select appropriate combination variables interactions. However - substitution carefully considered hypotheses experimental design.Incorporate random effects: identified significant fixed effects relevant interactions, can consider inclusion random effects. Random effects capture variation different levels can account individual differences clustering within groups. Evaluate need random intercepts, random slopes, crossed random effects based research design nature data.Incorporate random effects: identified significant fixed effects relevant interactions, can consider inclusion random effects. Random effects capture variation different levels can account individual differences clustering within groups. Evaluate need random intercepts, random slopes, crossed random effects based research design nature data.Assess compare models random effects: Fit models random effects compare fit fixed effects model. Consider appropriate measures likelihood ratio tests, AIC, BIC model comparison. Evaluate contribution random effects model.However - substitution carefully considered hypotheses experimental design. seen examples leave random effects place despite LRT tests.Assess compare models random effects: Fit models random effects compare fit fixed effects model. Consider appropriate measures likelihood ratio tests, AIC, BIC model comparison. Evaluate contribution random effects model.However - substitution carefully considered hypotheses experimental design. seen examples leave random effects place despite LRT tests.Validate interpret final model: Validate final model assessing assumptions, checking influential observations, performing sensitivity analysis. Interpret estimated coefficients, including fixed effects random effects, context research question. Report results figures, summary tables carefully considered text summarising analysis.Validate interpret final model: Validate final model assessing assumptions, checking influential observations, performing sensitivity analysis. Interpret estimated coefficients, including fixed effects random effects, context research question. Report results figures, summary tables carefully considered text summarising analysis.initially focusing fixed effects, can establish foundation model identify significant predictors interactions. step allows better understand relationships data guide subsequent inclusion random effects appropriate.","code":""},{"path":"summary-10.html","id":"mixed-model-extensions","chapter":"25 Summary","heading":"25.1 Mixed Model extensions","text":"","code":""},{"path":"summary-10.html","id":"practical-problems","chapter":"25 Summary","heading":"25.2 Practical problems","text":"two common issues warnings likely encounter fitting linear models:boundary (singular) fit: see help('isSingular'): model fit, generated warning random effects small, common complex mixed-effect models. can read help pageboundary (singular) fit: see help('isSingular'): model fit, generated warning random effects small, common complex mixed-effect models. can read help pageConvergence warnings: Values mixed-effects models determined using optimisation algorithms. Sometimes algorithms fail converge best parameter estimate, produce error. several possible solutions:\nNormalise rescale: Rescaling variables can mitigate issues caused differences scales magnitudes predictors, can affect optimization process. can rescale fixed effects predictors subtracting mean dividing standard deviation. centers variables around zero scales standard deviation 1. can done using scale() function R.\nTry alternative optimisation algorithms: e.g. lmer3 <- lmer(y ~ x + (x | group), data = data, control = lmerControl(optimizer =\"Nelder_Mead\"))\nFinally, although pains admit , try running model using different package - unique slightly different optimisation protocols.\nConvergence warnings: Values mixed-effects models determined using optimisation algorithms. Sometimes algorithms fail converge best parameter estimate, produce error. several possible solutions:Normalise rescale: Rescaling variables can mitigate issues caused differences scales magnitudes predictors, can affect optimization process. can rescale fixed effects predictors subtracting mean dividing standard deviation. centers variables around zero scales standard deviation 1. can done using scale() function R.Try alternative optimisation algorithms: e.g. lmer3 <- lmer(y ~ x + (x | group), data = data, control = lmerControl(optimizer =\"Nelder_Mead\"))Finally, although pains admit , try running model using different package - unique slightly different optimisation protocols.","code":""},{"path":"summary-10.html","id":"further-reading","chapter":"25 Summary","heading":"25.3 Further Reading","text":"essential reading continue mixed-model journey!brief introduction mixed effects modelling multi-model inference ecology Harrison et al. (2018)brief introduction mixed effects modelling multi-model inference ecology Harrison et al. (2018)Mixed Effects Models Extensions Ecology R Zuur et al. (2009)Mixed Effects Models Extensions Ecology R Zuur et al. (2009)Perils pitfalls mixed-effects regression models biology Silk et al. (2020)Perils pitfalls mixed-effects regression models biology Silk et al. (2020)","code":""},{"path":"summary-10.html","id":"references","chapter":"25 Summary","heading":"25.4 References","text":"","code":""}]
